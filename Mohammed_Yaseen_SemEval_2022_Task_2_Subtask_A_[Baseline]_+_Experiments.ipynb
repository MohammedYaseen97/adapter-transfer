{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mohammed Yaseen - SemEval 2022 Task 2 : Subtask A [Baseline] + Experiments",
      "provenance": [],
      "collapsed_sections": [
        "do-TXGBemGgH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedYaseen97/idiomaticity-detection/blob/subtaskA_basic_exps/Mohammed_Yaseen_SemEval_2022_Task_2_Subtask_A_%5BBaseline%5D_%2B_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9b_p85gxaGc"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook provides a baseline for each setting in [Subtask A of SemEval 2022 Task 2](https://sites.google.com/view/semeval2022task2-idiomaticity#h.qq7eefmehqf9). In addition this provides some helpful pre-processing scripts that you are free to use with your experiments. \n",
        "\n",
        "Please start by stepping through this notebook so you have a clear idea as to what is expected of the task and what you need to submit. \n",
        "\n",
        "These baselines are based on the results described in the paper “[AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models](https://arxiv.org/abs/2109.04413)”. \n",
        "\n",
        "## Zero-shot setting: Methodology \n",
        "\n",
        "Note that in the zero-shot setting you are NOT allowed to train the model using the one-shot data. \n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”). This is based on the results presented in the dataset paper. \n",
        "\n",
        "We use Multilingual BERT for this setting.\n",
        "\n",
        "## One-shot setting: Methodology\n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”. Again, this is based on the results presented in the dataset paper. \n",
        "\n",
        "We also use Multilingual BERT for this setting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-TXGBemGgH"
      },
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WsITUAnzvFl"
      },
      "source": [
        "Download the Task data and evaluation scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3qhQdpl-1-",
        "outputId": "32508c92-ab01-4d56-d8cf-211fb229ca86"
      },
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval_2022_Task2-idiomaticity'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 123 (delta 48), reused 61 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 2.50 MiB | 8.78 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-0POB9tzfNx"
      },
      "source": [
        "Download the “AStitchInLanguageModels” code which we make use of. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affNQCRktdx4",
        "outputId": "bfbb04fc-3971-41f9-e81a-95854948c86a"
      },
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AStitchInLanguageModels'...\n",
            "remote: Enumerating objects: 1030, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 1030 (delta 11), reused 4 (delta 4), pack-reused 1013\u001b[K\n",
            "Receiving objects: 100% (1030/1030), 79.59 MiB | 14.31 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60w-An2vzikk"
      },
      "source": [
        "Download and install an editable version of huggingfaces transformers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8BhcLYcmVvd",
        "outputId": "5cc5d3a2-c594-4df8-a901-e78a41f8a680"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!pip install --editable .\n",
        "%cd /content/ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 101286, done.\u001b[K\n",
            "remote: Counting objects: 100% (268/268), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 101286 (delta 114), reused 126 (delta 52), pack-reused 101018\u001b[K\n",
            "Receiving objects: 100% (101286/101286), 94.91 MiB | 15.30 MiB/s, done.\n",
            "Resolving deltas: 100% (74663/74663), done.\n",
            "/content/transformers\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVMnwTSzmjJ"
      },
      "source": [
        "Required for run_glue ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tsWits5tw1t",
        "outputId": "a363c93b-50cf-4acd-a87b-3c3717c41e8a"
      },
      "source": [
        "## run_glue needs this. \n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 18.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.3 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.4.0 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-igYdTTgzp9e"
      },
      "source": [
        "Editable install requires runtime restart unless we do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOuKplBmmbeB"
      },
      "source": [
        "import site\n",
        "site.main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvC8kAGNnKk_"
      },
      "source": [
        "# Imports and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOw3MaG7nN77"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzDtW9eXnOhG"
      },
      "source": [
        "def load_csv( path, delimiter=',' ) : \n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile, delimiter=delimiter ) \n",
        "    for row in reader : \n",
        "      if header is None : \n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row ) \n",
        "  return header, data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwtDsdtAnSZu"
      },
      "source": [
        "def write_csv( data, location ) : \n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile ) \n",
        "    writer.writerows( data ) \n",
        "  print( \"Wrote {}\".format( location ) ) \n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44LyZ-OXmgQW"
      },
      "source": [
        "# Pre-process: Create train and dev and evaluation data in required format\n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”). \n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-3ymBcEmxaV"
      },
      "source": [
        "## Functions for pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MthVK7EQm6m_"
      },
      "source": [
        "### _get_train_data\n",
        "\n",
        "This function generates training data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPGq-Y1Jmvv5"
      },
      "source": [
        "def _get_train_data( data_location, file_name, include_context, include_idiom ) :\n",
        "    \n",
        "    file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "        \n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem in data :\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem[ header.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ header.index( 'Previous' ) ], elem[ header.index( 'Target' ) ], elem[ header.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ header.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        out_data.append( this_row )\n",
        "        assert len( out_header ) == len( this_row )\n",
        "    return [ out_header ] + out_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytociCB3WZM"
      },
      "source": [
        "### _get_dev_eval_data\n",
        "\n",
        "This function generates training dev and eval data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters. \n",
        "\n",
        "Additionally, if there is no gold label provides (as in the case of eval) it will generate a file that can be used to generate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe4YQJ9Sm-B2"
      },
      "source": [
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None : \n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "    \n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None : \n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "            \n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "            \n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        assert len( out_header ) == len( this_row ) \n",
        "        out_data.append( this_row )\n",
        "        \n",
        "\n",
        "    return [ out_header ] + out_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIbyTnn3fHP"
      },
      "source": [
        "### create_data\n",
        "\n",
        "This function generates the training, development and evaluation data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1tr-zNvnBCV"
      },
      "source": [
        "\"\"\"\n",
        "Based on the results presented in `AStitchInLanguageModels' we work with not including the idiom for the zero shot setting and including it in the one shot setting.\n",
        "\"\"\"\n",
        "def create_data( input_location, output_location, lang=None) :\n",
        "\n",
        "    \n",
        "    ## Zero shot data\n",
        "    train_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv' if lang is None else 'train_zero_shot_'+lang.lower()+'.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False\n",
        "    )\n",
        "    write_csv( train_data, os.path.join( output_location, 'ZeroShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv' if lang is None else 'dev_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = 'dev_gold.csv' if lang is None else 'dev_gold_'+lang.lower()+'.csv', \n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv' if lang is None else 'eval_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "\n",
        "    ## OneShot Data (combine both for training)\n",
        "    train_zero_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv' if lang is None else 'train_zero_shot_'+lang.lower()+'.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "    train_one_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot.csv' if lang is None else 'train_one_shot_'+lang.lower()+'.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "\n",
        "    assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "    train_data = train_one_data + train_zero_data[1:]\n",
        "    write_csv( train_data, os.path.join( output_location, 'OneShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv' if lang is None else 'dev_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = 'dev_gold.csv' if lang is None else 'dev_gold_'+lang.lower()+'.csv', \n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'OneShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv' if lang is None else 'eval_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'OneShot', 'eval.csv' ) )\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmQfvym8ndKH"
      },
      "source": [
        "## Setup and Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxCgaHlKnpMR",
        "outputId": "32d49214-ca65-46c0-bf53-5f835a532cbb"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AStitchInLanguageModels  SemEval_2022_Task2-idiomaticity\n",
            "sample_data\t\t transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_lang( data_location, file_name, lang ) :\n",
        "  # Filter csv files by language\n",
        "    \n",
        "  file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "  header, data = load_csv( file_name )\n",
        "\n",
        "  # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "  out_data = list()\n",
        "  for elem in data :\n",
        "      if elem[ header.index( 'Language'  ) ] == lang:\n",
        "        out_data.append( elem )\n",
        "  # write_csv( eval_data, os.path.join( data_location, file_name+\"_\"+lang.tolower()+\".csv\" ) )\n",
        "  return [header] + out_data"
      ],
      "metadata": {
        "id": "OYbvrA_98voB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lang_input(file_location, lang):\n",
        "  train0_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'train_zero_shot.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( train0_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"train_zero_shot_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  train1_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'train_one_shot.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( train1_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"train_one_shot_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  dev_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'dev.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( dev_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"dev_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  dev_gold_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'dev_gold.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( dev_gold_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"dev_gold_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  eval_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'eval.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( eval_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"eval_\"+lang.lower()+\".csv\" ) )"
      ],
      "metadata": {
        "id": "e3sgi86tO5Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create_lang_input('SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', 'EN')"
      ],
      "metadata": {
        "id": "Ve6NW5FdTsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkeKLg-Hngs4",
        "outputId": "8252d68f-1927-402a-e974-a0dbbd0bf88b"
      },
      "source": [
        "outpath = 'Data'\n",
        "    \n",
        "Path( os.path.join( outpath, 'ZeroShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "Path( os.path.join( outpath, 'OneShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', outpath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/train.csv\n",
            "Wrote Data/ZeroShot/dev.csv\n",
            "Wrote Data/ZeroShot/eval.csv\n",
            "Wrote Data/OneShot/train.csv\n",
            "Wrote Data/OneShot/dev.csv\n",
            "Wrote Data/OneShot/eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-Ol7hfoC8a"
      },
      "source": [
        "# Zero Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 1 : Monolingual BERT for zero-shot english idiomaticity detection "
      ],
      "metadata": {
        "id": "LfqCJteZxeXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPRUmDjokHx_",
        "outputId": "445b240f-b5f3-4697-83e7-f538513a68d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k0BwA0uoKAu",
        "outputId": "6ead7166-da27-4652-b272-e3b13210559e"
      },
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-uncased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/25/2022 22:57:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/25/2022 22:57:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Jul25_22-57-00_5d96eb7a1ae8,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/25/2022 22:57:00 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "07/25/2022 22:57:00 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "07/25/2022 22:57:01 - WARNING - datasets.builder -   Using custom data configuration default-7da02ea52db35938\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7da02ea52db35938/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11290.19it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1482.09it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7da02ea52db35938/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1079.34it/s]\n",
            "[INFO|hub.py:592] 2022-07-25 22:57:02,354 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphgq44j1d\n",
            "Downloading: 100% 570/570 [00:00<00:00, 567kB/s]\n",
            "[INFO|hub.py:596] 2022-07-25 22:57:03,239 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|hub.py:604] 2022-07-25 22:57:03,239 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:659] 2022-07-25 22:57:03,239 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-07-25 22:57:03,242 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-25 22:57:04,122 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxnidlnvl\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 19.9kB/s]\n",
            "[INFO|hub.py:596] 2022-07-25 22:57:05,016 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|hub.py:604] 2022-07-25 22:57:05,016 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-07-25 22:57:05,908 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-07-25 22:57:05,909 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-25 22:57:07,682 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqt_w5hus\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 266kB/s]\n",
            "[INFO|hub.py:596] 2022-07-25 22:57:09,453 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:604] 2022-07-25 22:57:09,453 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:592] 2022-07-25 22:57:10,331 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmxg5zdk_\n",
            "Downloading: 100% 455k/455k [00:01<00:00, 432kB/s]\n",
            "[INFO|hub.py:596] 2022-07-25 22:57:12,302 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|hub.py:604] 2022-07-25 22:57:12,302 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-25 22:57:14,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-25 22:57:14,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-25 22:57:14,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-25 22:57:14,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-25 22:57:14,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-07-25 22:57:15,826 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-07-25 22:57:15,827 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-25 22:57:16,752 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp93pyc82n\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 72.5MB/s]\n",
            "[INFO|hub.py:596] 2022-07-25 22:57:22,855 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|hub.py:604] 2022-07-25 22:57:22,855 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:2107] 2022-07-25 22:57:22,856 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:2474] 2022-07-25 22:57:24,241 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2486] 2022-07-25 22:57:24,241 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  5.07ba/s]\n",
            "100% 1/1 [00:00<00:00,  9.49ba/s]\n",
            "07/25/2022 22:57:25 - INFO - __main__ -   Sample 1577 of the training set: {'label': 1, 'sentence1': 'Where do I stream Stag Night online? Stag Night is available to watch and stream, download, buy on demand at Amazon Prime, Amazon, Vudu, Google Play, iTunes, YouTube VOD online. Some platforms allow you to rent Stag Night for a limited time or purchase the movie and download it to your device.', 'input_ids': [101, 2073, 2079, 1045, 5460, 2358, 8490, 2305, 3784, 1029, 2358, 8490, 2305, 2003, 2800, 2000, 3422, 1998, 5460, 1010, 8816, 1010, 4965, 2006, 5157, 2012, 9733, 3539, 1010, 9733, 1010, 24728, 8566, 1010, 8224, 2377, 1010, 11943, 1010, 7858, 29536, 2094, 3784, 1012, 2070, 7248, 3499, 2017, 2000, 9278, 2358, 8490, 2305, 2005, 1037, 3132, 2051, 2030, 5309, 1996, 3185, 1998, 8816, 2009, 2000, 2115, 5080, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/25/2022 22:57:25 - INFO - __main__ -   Sample 3104 of the training set: {'label': 1, 'sentence1': 'Did Biden think for himself before he reversed direction on the Keystone Pipeline or did he do it just because the last Democrat president, under whom he served, was against it? I applaud Biden’s decisions to rejoin the World Health Organization and to call a world conference on climate change because, for whatever reason, the climate is changing. The atmosphere is getting warmer and the storms are getting bigger, as evidenced by the one in California earlier this week.', 'input_ids': [101, 2106, 7226, 2368, 2228, 2005, 2370, 2077, 2002, 11674, 3257, 2006, 1996, 22271, 13117, 2030, 2106, 2002, 2079, 2009, 2074, 2138, 1996, 2197, 7672, 2343, 1010, 2104, 3183, 2002, 2366, 1010, 2001, 2114, 2009, 1029, 1045, 10439, 17298, 2094, 7226, 2368, 1521, 1055, 6567, 2000, 25261, 1996, 2088, 2740, 3029, 1998, 2000, 2655, 1037, 2088, 3034, 2006, 4785, 2689, 2138, 1010, 2005, 3649, 3114, 1010, 1996, 4785, 2003, 5278, 1012, 1996, 7224, 2003, 2893, 16676, 1998, 1996, 12642, 2024, 2893, 7046, 1010, 2004, 21328, 2011, 1996, 2028, 1999, 2662, 3041, 2023, 2733, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/25/2022 22:57:25 - INFO - __main__ -   Sample 1722 of the training set: {'label': 1, 'sentence1': 'Mank leads the Critics Choice Awards 2021 nominations Coronavirus latest: Covid national research project will study effects of emerging mutations Jared Kushner and Ivanka Trump made up to $640 million while working in White House, report finds', 'input_ids': [101, 2158, 2243, 5260, 1996, 4401, 3601, 2982, 25682, 9930, 21887, 23350, 6745, 1024, 2522, 17258, 2120, 2470, 2622, 2097, 2817, 3896, 1997, 8361, 14494, 8334, 13970, 4095, 3678, 1998, 7332, 2912, 8398, 2081, 2039, 2000, 1002, 19714, 2454, 2096, 2551, 1999, 2317, 2160, 1010, 3189, 4858, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:662] 2022-07-25 22:57:29,017 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-25 22:57:29,027 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-25 22:57:29,027 >>   Num examples = 3327\n",
            "[INFO|trainer.py:1518] 2022-07-25 22:57:29,027 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1519] 2022-07-25 22:57:29,028 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1520] 2022-07-25 22:57:29,028 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-25 22:57:29,028 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-25 22:57:29,028 >>   Total optimization steps = 936\n",
            " 11% 104/936 [01:09<09:20,  1.48it/s][INFO|trainer.py:662] 2022-07-25 22:58:38,499 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 22:58:38,501 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 22:58:38,501 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 22:58:38,501 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 24.31it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 17.90it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 17.00it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:02, 16.34it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 16.03it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.68it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.58it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.73it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.73it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.57it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.26it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.36it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.44it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.35it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.18it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.18it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.36it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.35it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.6083008050918579, 'eval_accuracy': 0.6824034452438354, 'eval_f1': 0.6722551703163018, 'eval_runtime': 3.7972, 'eval_samples_per_second': 122.721, 'eval_steps_per_second': 15.538, 'epoch': 1.0}\n",
            " 11% 104/936 [01:13<09:20,  1.48it/s]\n",
            "100% 59/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 22:58:42,299 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-104\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 22:58:42,300 >> Configuration saved in models/ZeroShot/0/checkpoint-104/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 22:58:43,684 >> Model weights saved in models/ZeroShot/0/checkpoint-104/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 22:58:43,685 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-104/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 22:58:43,685 >> Special tokens file saved in models/ZeroShot/0/checkpoint-104/special_tokens_map.json\n",
            " 22% 208/936 [02:34<09:01,  1.34it/s][INFO|trainer.py:662] 2022-07-25 23:00:03,125 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:00:03,127 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:00:03,127 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:00:03,127 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.15it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.09it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.72it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.37it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.18it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.96it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.04it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.90it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 13.86it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.71it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.81it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.80it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.76it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.65it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.66it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.72it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.80it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.81it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.80it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.86it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.74it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.84it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.79it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.78it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.85it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.82it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.75it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8266248106956482, 'eval_accuracy': 0.6008583903312683, 'eval_f1': 0.5996119733924612, 'eval_runtime': 4.244, 'eval_samples_per_second': 109.802, 'eval_steps_per_second': 13.902, 'epoch': 2.0}\n",
            " 22% 208/936 [02:38<09:01,  1.34it/s]\n",
            "100% 59/59 [00:04<00:00, 13.67it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:00:07,373 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-208\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:00:07,374 >> Configuration saved in models/ZeroShot/0/checkpoint-208/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:00:08,316 >> Model weights saved in models/ZeroShot/0/checkpoint-208/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:00:08,317 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-208/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:00:08,317 >> Special tokens file saved in models/ZeroShot/0/checkpoint-208/special_tokens_map.json\n",
            " 33% 312/936 [03:59<07:38,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:01:28,803 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:01:28,804 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:01:28,804 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:01:28,804 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.44it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.57it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.32it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.79it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.51it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 14.14it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.19it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:02, 14.05it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 14.06it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.92it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.93it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.95it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.86it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.94it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.80it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.82it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.81it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.94it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.92it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.92it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.90it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.96it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.91it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.78it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.81it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.92it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.87it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8108971118927002, 'eval_accuracy': 0.6738197207450867, 'eval_f1': 0.6690402766096626, 'eval_runtime': 4.1888, 'eval_samples_per_second': 111.248, 'eval_steps_per_second': 14.085, 'epoch': 3.0}\n",
            " 33% 312/936 [04:03<07:38,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 14.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:01:32,995 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-312\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:01:32,995 >> Configuration saved in models/ZeroShot/0/checkpoint-312/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:01:34,027 >> Model weights saved in models/ZeroShot/0/checkpoint-312/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:01:34,028 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:01:34,028 >> Special tokens file saved in models/ZeroShot/0/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:01:37,611 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-208] due to args.save_total_limit\n",
            " 44% 416/936 [05:25<06:21,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:02:54,927 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:02:54,929 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:02:54,929 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:02:54,929 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.48it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.40it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.16it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.77it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.51it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 14.22it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.28it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.97it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 13.85it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.82it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.85it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.89it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.82it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.87it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.83it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.86it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.87it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.94it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.86it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.81it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.82it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.74it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.71it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.83it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.87it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.86it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.83it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.010894775390625, 'eval_accuracy': 0.716738224029541, 'eval_f1': 0.6895089646260701, 'eval_runtime': 4.211, 'eval_samples_per_second': 110.661, 'eval_steps_per_second': 14.011, 'epoch': 4.0}\n",
            " 44% 416/936 [05:30<06:21,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 13.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:02:59,141 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-416\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:02:59,142 >> Configuration saved in models/ZeroShot/0/checkpoint-416/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:03:00,076 >> Model weights saved in models/ZeroShot/0/checkpoint-416/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:03:00,077 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-416/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:03:00,077 >> Special tokens file saved in models/ZeroShot/0/checkpoint-416/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:03:03,757 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-104] due to args.save_total_limit\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:03:03,886 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-312] due to args.save_total_limit\n",
            "{'loss': 0.2833, 'learning_rate': 9.316239316239318e-06, 'epoch': 4.81}\n",
            " 56% 520/936 [06:52<05:05,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:04:21,242 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:04:21,244 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:04:21,244 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:04:21,244 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.02it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.21it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.07it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.71it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.42it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 14.24it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.30it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.93it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 13.91it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.91it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.90it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.88it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.87it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.98it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.91it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.96it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.85it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.79it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.84it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.82it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.87it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.91it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.86it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.90it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.79it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.74it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.81it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1952780485153198, 'eval_accuracy': 0.7231759428977966, 'eval_f1': 0.7126962506272851, 'eval_runtime': 4.2063, 'eval_samples_per_second': 110.787, 'eval_steps_per_second': 14.027, 'epoch': 5.0}\n",
            " 56% 520/936 [06:56<05:05,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 13.84it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:04:25,451 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-520\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:04:25,452 >> Configuration saved in models/ZeroShot/0/checkpoint-520/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:04:26,552 >> Model weights saved in models/ZeroShot/0/checkpoint-520/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:04:26,552 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:04:26,552 >> Special tokens file saved in models/ZeroShot/0/checkpoint-520/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:04:30,091 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-416] due to args.save_total_limit\n",
            " 67% 624/936 [08:18<03:48,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:05:47,426 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:05:47,428 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:05:47,428 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:05:47,428 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.48it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.53it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.20it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.65it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.35it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.92it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.09it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.92it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 13.98it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.90it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.96it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 14.00it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.99it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 14.04it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.99it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.97it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.95it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.87it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.89it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.93it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.85it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.76it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.84it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.93it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.89it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.92it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.3817256689071655, 'eval_accuracy': 0.7231759428977966, 'eval_f1': 0.6978148202140441, 'eval_runtime': 4.1937, 'eval_samples_per_second': 111.12, 'eval_steps_per_second': 14.069, 'epoch': 6.0}\n",
            " 67% 624/936 [08:22<03:48,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 13.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:05:51,623 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-624\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:05:51,624 >> Configuration saved in models/ZeroShot/0/checkpoint-624/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:05:52,626 >> Model weights saved in models/ZeroShot/0/checkpoint-624/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:05:52,627 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:05:52,627 >> Special tokens file saved in models/ZeroShot/0/checkpoint-624/special_tokens_map.json\n",
            " 78% 728/936 [09:44<02:32,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:07:13,367 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:07:13,369 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:07:13,369 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:07:13,369 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 20.02it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.02it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 15.14it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 14.77it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 14.51it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:03, 14.13it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:03, 14.04it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 14.04it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 13.97it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 13.88it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 13.93it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 13.90it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 13.92it/s]\u001b[A\n",
            " 51% 30/59 [00:02<00:02, 14.00it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 13.80it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 13.76it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 13.77it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 13.75it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 13.83it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 13.86it/s]\u001b[A\n",
            " 75% 44/59 [00:03<00:01, 13.95it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 13.94it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 13.87it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 13.93it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 13.82it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 13.87it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 13.96it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.4416186809539795, 'eval_accuracy': 0.7124463319778442, 'eval_f1': 0.7041578229229837, 'eval_runtime': 4.1946, 'eval_samples_per_second': 111.096, 'eval_steps_per_second': 14.066, 'epoch': 7.0}\n",
            " 78% 728/936 [09:48<02:32,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 14.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:07:17,565 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-728\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:07:17,566 >> Configuration saved in models/ZeroShot/0/checkpoint-728/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:07:18,721 >> Model weights saved in models/ZeroShot/0/checkpoint-728/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:07:18,722 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-728/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:07:18,722 >> Special tokens file saved in models/ZeroShot/0/checkpoint-728/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:07:22,237 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-624] due to args.save_total_limit\n",
            " 89% 832/936 [11:10<01:16,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:08:39,583 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:08:39,585 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:08:39,585 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:08:39,586 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.11it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.34it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.04it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.60it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.47it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 14.21it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.32it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:02, 14.12it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 14.15it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 14.00it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 14.03it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.83it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.85it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.89it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.88it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.87it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.82it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.97it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.70it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.84it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.83it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.89it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.91it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.87it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.82it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.68it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.70it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.496701717376709, 'eval_accuracy': 0.725321888923645, 'eval_f1': 0.7065029129270981, 'eval_runtime': 4.2066, 'eval_samples_per_second': 110.777, 'eval_steps_per_second': 14.025, 'epoch': 8.0}\n",
            " 89% 832/936 [11:14<01:16,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 13.77it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:08:43,794 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-832\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:08:43,795 >> Configuration saved in models/ZeroShot/0/checkpoint-832/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:08:44,835 >> Model weights saved in models/ZeroShot/0/checkpoint-832/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:08:44,835 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-832/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:08:44,835 >> Special tokens file saved in models/ZeroShot/0/checkpoint-832/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:08:48,584 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-728] due to args.save_total_limit\n",
            "100% 936/936 [12:37<00:00,  1.36it/s][INFO|trainer.py:662] 2022-07-25 23:10:06,065 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:10:06,067 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:10:06,067 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:10:06,067 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.29it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.36it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.12it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.48it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.42it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 14.02it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 14.08it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:02, 14.06it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 14.02it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.97it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.90it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.92it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.68it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.83it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.81it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.89it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.85it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.90it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.92it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.82it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.83it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.74it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.85it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.83it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.81it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.81it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 13.87it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5210881233215332, 'eval_accuracy': 0.7231759428977966, 'eval_f1': 0.7024457127017676, 'eval_runtime': 4.2106, 'eval_samples_per_second': 110.673, 'eval_steps_per_second': 14.012, 'epoch': 9.0}\n",
            "100% 936/936 [12:41<00:00,  1.36it/s]\n",
            "100% 59/59 [00:04<00:00, 13.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-25 23:10:10,279 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-936\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:10:10,280 >> Configuration saved in models/ZeroShot/0/checkpoint-936/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:10:11,324 >> Model weights saved in models/ZeroShot/0/checkpoint-936/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:10:11,324 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:10:11,324 >> Special tokens file saved in models/ZeroShot/0/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-25 23:10:14,911 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-832] due to args.save_total_limit\n",
            "[INFO|trainer.py:1761] 2022-07-25 23:10:15,094 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1834] 2022-07-25 23:10:15,095 >> Loading best model from models/ZeroShot/0/checkpoint-520 (score: 0.7126962506272851).\n",
            "{'train_runtime': 767.8968, 'train_samples_per_second': 38.994, 'train_steps_per_second': 1.219, 'train_loss': 0.1558915310435825, 'epoch': 9.0}\n",
            "100% 936/936 [12:47<00:00,  1.22it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-25 23:10:16,927 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:446] 2022-07-25 23:10:16,928 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-25 23:10:17,926 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-25 23:10:17,927 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-25 23:10:17,927 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.1559\n",
            "  train_runtime            = 0:12:47.89\n",
            "  train_samples            =       3327\n",
            "  train_samples_per_second =     38.994\n",
            "  train_steps_per_second   =      1.219\n",
            "07/25/2022 23:10:17 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:662] 2022-07-25 23:10:17,964 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-25 23:10:17,966 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-25 23:10:17,966 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-25 23:10:17,966 >>   Batch size = 8\n",
            "100% 59/59 [00:04<00:00, 14.59it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.7232\n",
            "  eval_f1                 =     0.7127\n",
            "  eval_loss               =     1.1953\n",
            "  eval_runtime            = 0:00:04.12\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    112.856\n",
            "  eval_steps_per_second   =     14.289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 2 : Multiingual BERT for zero-shot multilingual idiomaticity detection"
      ],
      "metadata": {
        "id": "VErwYC4h0kQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "id": "RDxc4AfhYgO3",
        "outputId": "2b033f1c-2216-486c-fd57-920c673f97d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 07:27:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 07:27:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Jul26_07-27-16_db65fb7f0caf,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 07:27:16 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "07/26/2022 07:27:16 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "07/26/2022 07:27:16 - WARNING - datasets.builder -   Using custom data configuration default-9ec5e5cf05739c5c\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-9ec5e5cf05739c5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 6512.89it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 851.46it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9ec5e5cf05739c5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 705.87it/s]\n",
            "[INFO|hub.py:592] 2022-07-26 07:27:17,571 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgqelca8a\n",
            "Downloading: 100% 625/625 [00:00<00:00, 728kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 07:27:17,936 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|hub.py:604] 2022-07-26 07:27:17,937 >> creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 07:27:17,937 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 07:27:17,940 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 07:27:18,299 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7y4vsrtu\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 28.2kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 07:27:18,665 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|hub.py:604] 2022-07-26 07:27:18,665 >> creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 07:27:19,026 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 07:27:19,027 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 07:27:19,750 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpa0cydkey\n",
            "Downloading: 100% 972k/972k [00:00<00:00, 1.95MB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 07:27:20,668 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:604] 2022-07-26 07:27:20,668 >> creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:592] 2022-07-26 07:27:21,028 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7zfi36c0\n",
            "Downloading: 100% 1.87M/1.87M [00:00<00:00, 3.27MB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 07:27:22,036 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|hub.py:604] 2022-07-26 07:27:22,036 >> creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:27:23,131 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:27:23,131 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:27:23,131 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:27:23,131 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:27:23,131 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 07:27:23,491 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 07:27:23,492 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 07:27:23,992 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4f1ksfdy\n",
            "Downloading: 100% 681M/681M [00:10<00:00, 67.9MB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 07:27:34,558 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|hub.py:604] 2022-07-26 07:27:34,558 >> creating metadata file for /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|modeling_utils.py:2107] 2022-07-26 07:27:34,558 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2474] 2022-07-26 07:27:36,691 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2486] 2022-07-26 07:27:36,691 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 5/5 [00:01<00:00,  4.97ba/s]\n",
            "100% 1/1 [00:00<00:00,  6.21ba/s]\n",
            "07/26/2022 07:27:37 - INFO - __main__ -   Sample 3155 of the training set: {'label': 0, 'sentence1': 'According to history.com, the leap day was originally discovered by Egyptian astronomers, but this discovery did not reach the western world until Julius Caesar’s reign in 45 BC. Caesar then created the leap year calendar to fix the problem, which was later adapted in accordance with new knowledge about the earth’s orbit by Pope Gregory into the Gregorian calendar that we observe today. Feb. 29 happens every four years, because the earth technically requires 365.25 days to complete its orbit around the sun.', 'input_ids': [101, 14156, 10114, 11486, 119, 10212, 117, 10105, 20169, 10410, 11940, 10134, 15556, 21756, 10155, 34624, 50575, 10901, 117, 10473, 10531, 30419, 12172, 10472, 24278, 10105, 16672, 11356, 11444, 18703, 30159, 100, 187, 38587, 10106, 10827, 19376, 119, 30159, 11059, 13745, 10105, 20169, 10410, 10924, 61637, 10114, 14045, 10686, 10105, 18077, 117, 10319, 10134, 10873, 40851, 10106, 88651, 10169, 10751, 22975, 10978, 10105, 39189, 100, 187, 17090, 10155, 23874, 22392, 10708, 10105, 47723, 11630, 61637, 10189, 11951, 78275, 18745, 119, 21194, 119, 10386, 105315, 14234, 11598, 10855, 117, 12373, 10105, 39189, 29914, 10454, 39575, 25385, 119, 10258, 13990, 10114, 17876, 10474, 17090, 12166, 10105, 42230, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 07:27:37 - INFO - __main__ -   Sample 3445 of the training set: {'label': 1, 'sentence1': 'A saída da crise, segundo Bachelet, depende de ações que garantam renda para os mais pobres e vacina para todos São Paulo – Alta comissária para os Direitos Humanos da Organização das Nações Unidas (ONU), a ex-presidenta do Chile Michelle Bachelet criticou governantes de países pobres e ricos que optaram pela economia em vez de promover a saúde da população. O resultado, segundo ela, foi o aprofundamento das desigualdades sociais causadas pela histórica falta de investimento em áreas sociais, entre elas a saúde.', 'input_ids': [101, 138, 70747, 10143, 34862, 117, 12943, 18965, 41583, 117, 59216, 10104, 77302, 10121, 24457, 14732, 10147, 21367, 10220, 10427, 10614, 64440, 173, 10321, 22849, 10220, 12656, 12114, 13360, 100, 22798, 10212, 47097, 19075, 10220, 10427, 66130, 10107, 87537, 10143, 72154, 27187, 10242, 95322, 49323, 113, 46743, 114, 117, 169, 11419, 118, 107041, 10149, 13218, 27062, 18965, 41583, 82389, 10138, 25574, 25819, 10104, 15395, 64440, 173, 99702, 10121, 10303, 35474, 10147, 11793, 28223, 10266, 11675, 10104, 57833, 169, 54728, 10143, 17857, 119, 152, 20229, 117, 12943, 12593, 117, 10448, 183, 26219, 10567, 55227, 16686, 10242, 10139, 104915, 110285, 55167, 102453, 10107, 11793, 36818, 23821, 10104, 10106, 63996, 11498, 10266, 23571, 55167, 117, 10402, 34338, 169, 54728, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 07:27:37 - INFO - __main__ -   Sample 331 of the training set: {'label': 1, 'sentence1': 'Concept Smoke Screen, in partnership with G4S, have developed a new way of defending cash and guards against attacks when replenishing ATM\\'s. On May 11th at the IFSEC Security Industry Awards 2009, Concept Smoke Screen were honoured with the \"Physical Security Product of the Year\" award, for the Guardian Smoke Screen. The ceremony was conducted at the Birmingham Hilton Metropole.', 'input_ids': [101, 77961, 80677, 41131, 117, 10106, 36944, 10169, 144, 11011, 10731, 117, 10529, 14628, 169, 10751, 13170, 10108, 53730, 52828, 10111, 99024, 11327, 26483, 10841, 76456, 81635, 74062, 92233, 112, 187, 119, 10576, 10725, 34062, 10160, 10105, 25000, 39039, 10858, 20924, 25539, 12357, 10195, 117, 77961, 80677, 41131, 10309, 46948, 10336, 10169, 10105, 107, 33671, 20924, 93218, 10108, 10105, 13567, 107, 17725, 117, 10142, 10105, 20206, 80677, 41131, 119, 10117, 34713, 10134, 23736, 10160, 10105, 22712, 53329, 20640, 30328, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:662] 2022-07-26 07:27:41,676 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-26 07:27:41,686 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-26 07:27:41,686 >>   Num examples = 4491\n",
            "[INFO|trainer.py:1518] 2022-07-26 07:27:41,686 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1519] 2022-07-26 07:27:41,686 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1520] 2022-07-26 07:27:41,686 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-26 07:27:41,686 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-26 07:27:41,686 >>   Total optimization steps = 1269\n",
            " 11% 141/1269 [01:33<10:16,  1.83it/s][INFO|trainer.py:662] 2022-07-26 07:29:15,039 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:29:15,041 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:29:15,041 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:29:15,041 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.97it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.33it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 18.06it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:04, 17.42it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:04, 17.27it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:04, 16.96it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:04, 16.88it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:04, 16.89it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:04, 16.97it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:04, 17.00it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:04, 16.95it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 16.96it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 17.04it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 17.04it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 17.07it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:03, 16.97it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:03, 16.97it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:03, 17.06it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 17.03it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 17.04it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 16.86it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 16.91it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 16.90it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 17.04it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 17.05it/s]\u001b[A\n",
            " 59% 55/93 [00:03<00:02, 16.91it/s]\u001b[A\n",
            " 61% 57/93 [00:03<00:02, 16.82it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:02, 16.76it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 16.70it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 16.74it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 16.91it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 16.92it/s]\u001b[A\n",
            " 74% 69/93 [00:04<00:01, 16.94it/s]\u001b[A\n",
            " 76% 71/93 [00:04<00:01, 16.92it/s]\u001b[A\n",
            " 78% 73/93 [00:04<00:01, 16.89it/s]\u001b[A\n",
            " 81% 75/93 [00:04<00:01, 16.95it/s]\u001b[A\n",
            " 83% 77/93 [00:04<00:00, 17.05it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 17.02it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 16.97it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 16.95it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 16.97it/s]\u001b[A\n",
            " 94% 87/93 [00:05<00:00, 17.08it/s]\u001b[A\n",
            " 96% 89/93 [00:05<00:00, 17.04it/s]\u001b[A\n",
            " 98% 91/93 [00:05<00:00, 16.97it/s]\u001b[A\n",
            "{'eval_loss': 0.6837854385375977, 'eval_accuracy': 0.6305818557739258, 'eval_f1': 0.6270415963411635, 'eval_runtime': 5.4704, 'eval_samples_per_second': 135.091, 'eval_steps_per_second': 17.001, 'epoch': 1.0}\n",
            "\n",
            " 11% 141/1269 [01:38<10:16,  1.83it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:29:20,513 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-141\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:29:20,514 >> Configuration saved in models/ZeroShot/0/checkpoint-141/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:29:23,807 >> Model weights saved in models/ZeroShot/0/checkpoint-141/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:29:23,807 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-141/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:29:23,807 >> Special tokens file saved in models/ZeroShot/0/checkpoint-141/special_tokens_map.json\n",
            " 22% 282/1269 [03:23<09:12,  1.79it/s][INFO|trainer.py:662] 2022-07-26 07:31:04,847 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:31:04,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:31:04,850 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:31:04,850 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.50it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.81it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.01it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.15it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 16.81it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.73it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.61it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.62it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.54it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.53it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.53it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.50it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.44it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.42it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.37it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.42it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.43it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.39it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.37it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.38it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.36it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.39it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.46it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.49it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.48it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.51it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.48it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.43it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.43it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.44it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.43it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.44it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.45it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.43it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.45it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.44it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.43it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.42it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.49it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.49it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.52it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.67it/s]\u001b[A\n",
            "{'eval_loss': 0.7844110131263733, 'eval_accuracy': 0.6400541067123413, 'eval_f1': 0.6390488431876606, 'eval_runtime': 5.6147, 'eval_samples_per_second': 131.618, 'eval_steps_per_second': 16.564, 'epoch': 2.0}\n",
            "\n",
            " 22% 282/1269 [03:28<09:12,  1.79it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:31:10,467 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-282\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:31:10,468 >> Configuration saved in models/ZeroShot/0/checkpoint-282/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:31:12,871 >> Model weights saved in models/ZeroShot/0/checkpoint-282/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:31:12,877 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-282/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:31:12,878 >> Special tokens file saved in models/ZeroShot/0/checkpoint-282/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:31:18,543 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-141] due to args.save_total_limit\n",
            " 33% 423/1269 [05:12<07:54,  1.78it/s][INFO|trainer.py:662] 2022-07-26 07:32:53,992 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:32:53,995 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:32:53,995 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:32:53,995 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.57it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.89it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.01it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.16it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 16.92it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.81it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.50it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.64it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.66it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.58it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.52it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.49it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.34it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.36it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.45it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.48it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.51it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.52it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.52it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.54it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.58it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.69it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.73it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.75it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.79it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.73it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.71it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.65it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.61it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.55it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.50it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.49it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.45it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.51it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.69it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.73it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.70it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.66it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.62it/s]\u001b[A\n",
            "{'eval_loss': 1.044437050819397, 'eval_accuracy': 0.6792963743209839, 'eval_f1': 0.6785334740513009, 'eval_runtime': 5.5821, 'eval_samples_per_second': 132.387, 'eval_steps_per_second': 16.66, 'epoch': 3.0}\n",
            "\n",
            " 33% 423/1269 [05:17<07:54,  1.78it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:32:59,579 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-423\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:32:59,580 >> Configuration saved in models/ZeroShot/0/checkpoint-423/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:33:02,075 >> Model weights saved in models/ZeroShot/0/checkpoint-423/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:33:02,076 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-423/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:33:02,076 >> Special tokens file saved in models/ZeroShot/0/checkpoint-423/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:33:07,916 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-282] due to args.save_total_limit\n",
            "{'loss': 0.3186, 'learning_rate': 1.2119779353821908e-05, 'epoch': 3.55}\n",
            " 44% 564/1269 [07:01<06:36,  1.78it/s][INFO|trainer.py:662] 2022-07-26 07:34:43,468 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:34:43,470 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:34:43,470 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:34:43,470 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 25.21it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.43it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 18.09it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:04, 17.28it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:04, 17.16it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:04, 16.75it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:04, 16.70it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:04, 16.70it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:04, 16.73it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:04, 16.74it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:04, 16.82it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 16.82it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 16.82it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 16.66it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 16.62it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:03, 16.63it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:03, 16.63it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:03, 16.64it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 16.60it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:03, 16.53it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 16.55it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 16.46it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 16.41it/s]\u001b[A\n",
            " 55% 51/93 [00:03<00:02, 16.39it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 16.32it/s]\u001b[A\n",
            " 59% 55/93 [00:03<00:02, 16.41it/s]\u001b[A\n",
            " 61% 57/93 [00:03<00:02, 16.44it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:02, 16.48it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 16.53it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 16.52it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 16.50it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 16.53it/s]\u001b[A\n",
            " 74% 69/93 [00:04<00:01, 16.61it/s]\u001b[A\n",
            " 76% 71/93 [00:04<00:01, 16.73it/s]\u001b[A\n",
            " 78% 73/93 [00:04<00:01, 16.76it/s]\u001b[A\n",
            " 81% 75/93 [00:04<00:01, 16.67it/s]\u001b[A\n",
            " 83% 77/93 [00:04<00:00, 16.69it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 16.70it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 16.69it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 16.55it/s]\u001b[A\n",
            " 91% 85/93 [00:05<00:00, 16.37it/s]\u001b[A\n",
            " 94% 87/93 [00:05<00:00, 16.42it/s]\u001b[A\n",
            " 96% 89/93 [00:05<00:00, 16.41it/s]\u001b[A\n",
            " 98% 91/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            "{'eval_loss': 1.4536385536193848, 'eval_accuracy': 0.6603518128395081, 'eval_f1': 0.6603493390610438, 'eval_runtime': 5.5752, 'eval_samples_per_second': 132.551, 'eval_steps_per_second': 16.681, 'epoch': 4.0}\n",
            "\n",
            " 44% 564/1269 [07:07<06:36,  1.78it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:34:49,047 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-564\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:34:49,048 >> Configuration saved in models/ZeroShot/0/checkpoint-564/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:34:51,456 >> Model weights saved in models/ZeroShot/0/checkpoint-564/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:34:51,457 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-564/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:34:51,457 >> Special tokens file saved in models/ZeroShot/0/checkpoint-564/special_tokens_map.json\n",
            " 56% 705/1269 [08:50<05:16,  1.78it/s][INFO|trainer.py:662] 2022-07-26 07:36:32,543 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:36:32,545 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:36:32,545 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:36:32,545 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 25.29it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.55it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 18.25it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:04, 17.47it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:04, 17.19it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:04, 16.86it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:04, 16.73it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:04, 16.75it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:04, 16.77it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:04, 16.73it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:04, 16.74it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 16.71it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 16.78it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 16.69it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:03, 16.74it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:03, 16.76it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:03, 16.76it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 16.75it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 16.69it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 16.63it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 16.61it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 16.58it/s]\u001b[A\n",
            " 55% 51/93 [00:03<00:02, 16.66it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 16.72it/s]\u001b[A\n",
            " 59% 55/93 [00:03<00:02, 16.79it/s]\u001b[A\n",
            " 61% 57/93 [00:03<00:02, 16.81it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:02, 16.77it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 16.74it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 16.74it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 16.65it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 16.50it/s]\u001b[A\n",
            " 74% 69/93 [00:04<00:01, 16.50it/s]\u001b[A\n",
            " 76% 71/93 [00:04<00:01, 16.50it/s]\u001b[A\n",
            " 78% 73/93 [00:04<00:01, 16.52it/s]\u001b[A\n",
            " 81% 75/93 [00:04<00:01, 16.61it/s]\u001b[A\n",
            " 83% 77/93 [00:04<00:00, 16.71it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 16.75it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 16.74it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 16.66it/s]\u001b[A\n",
            " 91% 85/93 [00:05<00:00, 16.60it/s]\u001b[A\n",
            " 94% 87/93 [00:05<00:00, 16.59it/s]\u001b[A\n",
            " 96% 89/93 [00:05<00:00, 16.51it/s]\u001b[A\n",
            " 98% 91/93 [00:05<00:00, 16.52it/s]\u001b[A\n",
            "{'eval_loss': 1.6704460382461548, 'eval_accuracy': 0.686062216758728, 'eval_f1': 0.6836145681090099, 'eval_runtime': 5.545, 'eval_samples_per_second': 133.274, 'eval_steps_per_second': 16.772, 'epoch': 5.0}\n",
            "\n",
            " 56% 705/1269 [08:56<05:16,  1.78it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:36:38,092 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-705\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:36:38,093 >> Configuration saved in models/ZeroShot/0/checkpoint-705/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:36:40,733 >> Model weights saved in models/ZeroShot/0/checkpoint-705/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:36:40,734 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-705/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:36:40,734 >> Special tokens file saved in models/ZeroShot/0/checkpoint-705/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:36:46,599 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-423] due to args.save_total_limit\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:36:46,634 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-564] due to args.save_total_limit\n",
            " 67% 846/1269 [10:40<03:57,  1.78it/s][INFO|trainer.py:662] 2022-07-26 07:38:22,171 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:38:22,173 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:38:22,173 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:38:22,173 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.26it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.84it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.90it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.16it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 16.90it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.76it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.46it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.58it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.60it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.64it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.62it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.66it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.57it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.45it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.33it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.45it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.45it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.50it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.55it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.48it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.50it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.42it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.37it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.39it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.40it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.48it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.53it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.56it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.66it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.73it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.77it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.71it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.70it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.70it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.69it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.68it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.60it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.55it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.52it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.43it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.43it/s]\u001b[A\n",
            "{'eval_loss': 1.943124771118164, 'eval_accuracy': 0.6887686252593994, 'eval_f1': 0.6887680363290118, 'eval_runtime': 5.5971, 'eval_samples_per_second': 132.032, 'eval_steps_per_second': 16.616, 'epoch': 6.0}\n",
            "\n",
            " 67% 846/1269 [10:46<03:57,  1.78it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:38:27,771 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-846\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:38:27,772 >> Configuration saved in models/ZeroShot/0/checkpoint-846/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:38:30,214 >> Model weights saved in models/ZeroShot/0/checkpoint-846/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:38:30,215 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-846/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:38:30,215 >> Special tokens file saved in models/ZeroShot/0/checkpoint-846/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:38:35,822 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-705] due to args.save_total_limit\n",
            " 78% 987/1269 [12:29<02:37,  1.79it/s][INFO|trainer.py:662] 2022-07-26 07:40:11,202 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:40:11,204 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:40:11,204 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:40:11,204 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.86it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.15it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.18it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.54it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.15it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.05it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.69it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.75it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.74it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.57it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.54it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.51it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.38it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.46it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.45it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.50it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.55it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.65it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.79it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.77it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.68it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.63it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.59it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.59it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.49it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.48it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.50it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.51it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.50it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.37it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.43it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.47it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.43it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.41it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.49it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.56it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.68it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.72it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.71it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.73it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.65it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.66it/s]\u001b[A\n",
            "{'eval_loss': 2.097614288330078, 'eval_accuracy': 0.6941813230514526, 'eval_f1': 0.6928308688295446, 'eval_runtime': 5.5753, 'eval_samples_per_second': 132.549, 'eval_steps_per_second': 16.681, 'epoch': 7.0}\n",
            "\n",
            " 78% 987/1269 [12:35<02:37,  1.79it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:40:16,782 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-987\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:40:16,783 >> Configuration saved in models/ZeroShot/0/checkpoint-987/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:40:19,306 >> Model weights saved in models/ZeroShot/0/checkpoint-987/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:40:19,307 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:40:19,307 >> Special tokens file saved in models/ZeroShot/0/checkpoint-987/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:40:24,923 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-846] due to args.save_total_limit\n",
            "{'loss': 0.0278, 'learning_rate': 4.239558707643815e-06, 'epoch': 7.09}\n",
            " 89% 1128/1269 [14:18<01:19,  1.78it/s][INFO|trainer.py:662] 2022-07-26 07:42:00,301 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:42:00,303 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:42:00,303 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:42:00,303 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.24it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.25it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.21it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.45it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 16.98it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.86it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.45it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.60it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.64it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.69it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.70it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.77it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.78it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.77it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.75it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.71it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.72it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.66it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.54it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.48it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.50it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.53it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.63it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.73it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.77it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.71it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.70it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.65it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.61it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.56it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.53it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.53it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.54it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.53it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.45it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.50it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.49it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.41it/s]\u001b[A\n",
            "{'eval_loss': 2.163095235824585, 'eval_accuracy': 0.6955345273017883, 'eval_f1': 0.6943502357515097, 'eval_runtime': 5.5724, 'eval_samples_per_second': 132.617, 'eval_steps_per_second': 16.689, 'epoch': 8.0}\n",
            "\n",
            " 89% 1128/1269 [14:24<01:19,  1.78it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:42:05,877 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1128\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:42:05,878 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:42:08,301 >> Model weights saved in models/ZeroShot/0/checkpoint-1128/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:42:08,302 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1128/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:42:08,302 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1128/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:42:14,005 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-987] due to args.save_total_limit\n",
            "100% 1269/1269 [16:07<00:00,  1.79it/s][INFO|trainer.py:662] 2022-07-26 07:43:49,350 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:43:49,353 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:43:49,353 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:43:49,353 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.95it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.29it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 18.02it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:04, 17.29it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:04, 17.16it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:04, 16.66it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:04, 16.64it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:04, 16.70it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:04, 16.74it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:04, 16.79it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:04, 16.80it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 16.87it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 16.89it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 16.65it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:03, 16.62it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:03, 16.64it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:03, 16.67it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 16.61it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:03, 16.62it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 16.60it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 16.56it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 16.53it/s]\u001b[A\n",
            " 55% 51/93 [00:03<00:02, 16.49it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 16.48it/s]\u001b[A\n",
            " 59% 55/93 [00:03<00:02, 16.51it/s]\u001b[A\n",
            " 61% 57/93 [00:03<00:02, 16.58it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:02, 16.73it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 16.77it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 16.76it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 16.73it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 16.68it/s]\u001b[A\n",
            " 74% 69/93 [00:04<00:01, 16.64it/s]\u001b[A\n",
            " 76% 71/93 [00:04<00:01, 16.58it/s]\u001b[A\n",
            " 78% 73/93 [00:04<00:01, 16.61it/s]\u001b[A\n",
            " 81% 75/93 [00:04<00:01, 16.57it/s]\u001b[A\n",
            " 83% 77/93 [00:04<00:00, 16.58it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 16.58it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 16.63it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 16.73it/s]\u001b[A\n",
            " 91% 85/93 [00:05<00:00, 16.77it/s]\u001b[A\n",
            " 94% 87/93 [00:05<00:00, 16.78it/s]\u001b[A\n",
            " 96% 89/93 [00:05<00:00, 16.81it/s]\u001b[A\n",
            " 98% 91/93 [00:05<00:00, 16.73it/s]\u001b[A\n",
            "{'eval_loss': 2.1796231269836426, 'eval_accuracy': 0.6901217699050903, 'eval_f1': 0.6882011655307992, 'eval_runtime': 5.5512, 'eval_samples_per_second': 133.124, 'eval_steps_per_second': 16.753, 'epoch': 9.0}\n",
            "\n",
            "100% 1269/1269 [16:13<00:00,  1.79it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:43:54,906 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1269\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:43:54,907 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:43:57,415 >> Model weights saved in models/ZeroShot/0/checkpoint-1269/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:43:57,416 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1269/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:43:57,416 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1269/special_tokens_map.json\n",
            "[INFO|trainer.py:1761] 2022-07-26 07:44:03,172 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1834] 2022-07-26 07:44:03,173 >> Loading best model from models/ZeroShot/0/checkpoint-1128 (score: 0.6943502357515097).\n",
            "{'train_runtime': 982.5139, 'train_samples_per_second': 41.138, 'train_steps_per_second': 1.292, 'train_loss': 0.13709361540040602, 'epoch': 9.0}\n",
            "100% 1269/1269 [16:22<00:00,  1.29it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 07:44:04,209 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:44:04,214 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:44:06,779 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:44:06,780 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:44:06,780 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.1371\n",
            "  train_runtime            = 0:16:22.51\n",
            "  train_samples            =       4491\n",
            "  train_samples_per_second =     41.138\n",
            "  train_steps_per_second   =      1.292\n",
            "07/26/2022 07:44:07 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:662] 2022-07-26 07:44:07,023 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:44:07,024 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:44:07,025 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:44:07,025 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 16.94it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.6955\n",
            "  eval_f1                 =     0.6944\n",
            "  eval_loss               =     2.1631\n",
            "  eval_runtime            = 0:00:05.56\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    132.734\n",
            "  eval_steps_per_second   =     16.704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 01 : Monolingual adapter-based BERT for zero-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "wZJLePe_xqR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adapter-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_Svn1vx3zA",
        "outputId": "e9adbf08-c53b-4b7c-cca6-4e65c615a420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-3.0.1-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 32.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->adapter-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.25.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=4d99ed0910beb31440062d9c86a8b9175fb5a755324ae72a25f1764a552d694c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, adapter-transformers\n",
            "Successfully installed adapter-transformers-3.0.1 sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py \\\n",
        "  --model_name_or_path 'bert-base-uncased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 10.0 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/ZeroShot/0/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --seed 0 \\\n",
        "  --train_file      Data/ZeroShot/train.csv \\\n",
        "  --validation_file Data/ZeroShot/dev.csv \\\n",
        "\t--evaluation_strategy \"epoch\" \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x427RCa4x5U9",
        "outputId": "73503b62-7a46-4a49-c1b5-8ccd37cc18c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 02:44:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 02:44:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Jul26_02-44-06_d6e7bd7111b8,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 02:44:06 - INFO - __main__ - load a local file for train: Data/ZeroShot/train.csv\n",
            "07/26/2022 02:44:06 - INFO - __main__ - load a local file for validation: Data/ZeroShot/dev.csv\n",
            "07/26/2022 02:44:07 - WARNING - datasets.builder - Using custom data configuration default-7cec94e5d2874b32\n",
            "07/26/2022 02:44:07 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/26/2022 02:44:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7cec94e5d2874b32/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
            "07/26/2022 02:44:07 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-7cec94e5d2874b32/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "07/26/2022 02:44:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7cec94e5d2874b32/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 921.62it/s]\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 02:44:07,268 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 02:44:07,269 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 02:44:07,541 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 02:44:07,542 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 02:44:08,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 02:44:08,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 02:44:08,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 02:44:08,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 02:44:08,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 02:44:08,512 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 02:44:08,512 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1431] 2022-07-26 02:44:08,691 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1694] 2022-07-26 02:44:10,027 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1711] 2022-07-26 02:44:10,027 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:616] 2022-07-26 02:44:10,037 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter Config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:617] 2022-07-26 02:44:10,038 >> Adding adapter 'glue'.\n",
            "07/26/2022 02:44:10 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-7cec94e5d2874b32/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0ab7f415461bbf4d.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/26/2022 02:44:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7cec94e5d2874b32/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-c177f09188fb9ae7.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  8.63ba/s]\n",
            "07/26/2022 02:44:10 - INFO - __main__ - Sample 1577 of the training set: {'label': 1, 'sentence1': 'Where do I stream Stag Night online? Stag Night is available to watch and stream, download, buy on demand at Amazon Prime, Amazon, Vudu, Google Play, iTunes, YouTube VOD online. Some platforms allow you to rent Stag Night for a limited time or purchase the movie and download it to your device.', 'input_ids': [101, 2073, 2079, 1045, 5460, 2358, 8490, 2305, 3784, 1029, 2358, 8490, 2305, 2003, 2800, 2000, 3422, 1998, 5460, 1010, 8816, 1010, 4965, 2006, 5157, 2012, 9733, 3539, 1010, 9733, 1010, 24728, 8566, 1010, 8224, 2377, 1010, 11943, 1010, 7858, 29536, 2094, 3784, 1012, 2070, 7248, 3499, 2017, 2000, 9278, 2358, 8490, 2305, 2005, 1037, 3132, 2051, 2030, 5309, 1996, 3185, 1998, 8816, 2009, 2000, 2115, 5080, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 02:44:10 - INFO - __main__ - Sample 3104 of the training set: {'label': 1, 'sentence1': 'Did Biden think for himself before he reversed direction on the Keystone Pipeline or did he do it just because the last Democrat president, under whom he served, was against it? I applaud Biden’s decisions to rejoin the World Health Organization and to call a world conference on climate change because, for whatever reason, the climate is changing. The atmosphere is getting warmer and the storms are getting bigger, as evidenced by the one in California earlier this week.', 'input_ids': [101, 2106, 7226, 2368, 2228, 2005, 2370, 2077, 2002, 11674, 3257, 2006, 1996, 22271, 13117, 2030, 2106, 2002, 2079, 2009, 2074, 2138, 1996, 2197, 7672, 2343, 1010, 2104, 3183, 2002, 2366, 1010, 2001, 2114, 2009, 1029, 1045, 10439, 17298, 2094, 7226, 2368, 1521, 1055, 6567, 2000, 25261, 1996, 2088, 2740, 3029, 1998, 2000, 2655, 1037, 2088, 3034, 2006, 4785, 2689, 2138, 1010, 2005, 3649, 3114, 1010, 1996, 4785, 2003, 5278, 1012, 1996, 7224, 2003, 2893, 16676, 1998, 1996, 12642, 2024, 2893, 7046, 1010, 2004, 21328, 2011, 1996, 2028, 1999, 2662, 3041, 2023, 2733, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 02:44:10 - INFO - __main__ - Sample 1722 of the training set: {'label': 1, 'sentence1': 'Mank leads the Critics Choice Awards 2021 nominations Coronavirus latest: Covid national research project will study effects of emerging mutations Jared Kushner and Ivanka Trump made up to $640 million while working in White House, report finds', 'input_ids': [101, 2158, 2243, 5260, 1996, 4401, 3601, 2982, 25682, 9930, 21887, 23350, 6745, 1024, 2522, 17258, 2120, 2470, 2622, 2097, 2817, 3896, 1997, 8361, 14494, 8334, 13970, 4095, 3678, 1998, 7332, 2912, 8398, 2081, 2039, 2000, 1002, 19714, 2454, 2096, 2551, 1999, 2317, 2160, 1010, 3189, 4858, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:230] 2022-07-26 02:44:13,023 >> The following columns in the training set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1254] 2022-07-26 02:44:13,030 >> ***** Running training *****\n",
            "[INFO|trainer.py:1255] 2022-07-26 02:44:13,030 >>   Num examples = 3327\n",
            "[INFO|trainer.py:1256] 2022-07-26 02:44:13,030 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1257] 2022-07-26 02:44:13,031 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1258] 2022-07-26 02:44:13,031 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1259] 2022-07-26 02:44:13,031 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1260] 2022-07-26 02:44:13,031 >>   Total optimization steps = 1040\n",
            " 10% 104/1040 [01:01<09:17,  1.68it/s][INFO|trainer.py:230] 2022-07-26 02:45:14,500 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:45:14,502 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:45:14,503 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:45:14,503 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.83it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.55it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.52it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.98it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.72it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.67it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.56it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.43it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.36it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.39it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.40it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.42it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.35it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.28it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.32it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.31it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.40it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.29it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.33it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.26it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.27it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.30it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.37it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.33it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.26it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.26it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6005694270133972, 'eval_accuracy': 0.6952789425849915, 'eval_f1': 0.6897877287728773, 'eval_runtime': 4.7428, 'eval_samples_per_second': 98.254, 'eval_steps_per_second': 12.44, 'epoch': 1.0}\n",
            " 10% 104/1040 [01:06<09:17,  1.68it/s]\n",
            "100% 59/59 [00:04<00:00, 13.71it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:45:19,246 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-104\n",
            "[INFO|loading.py:60] 2022-07-26 02:45:19,247 >> Configuration saved in models/ZeroShot/0/checkpoint-104/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:45:19,336 >> Module weights saved in models/ZeroShot/0/checkpoint-104/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:45:19,336 >> Configuration saved in models/ZeroShot/0/checkpoint-104/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:45:19,342 >> Module weights saved in models/ZeroShot/0/checkpoint-104/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:45:19,343 >> Configuration saved in models/ZeroShot/0/checkpoint-104/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:45:19,349 >> Module weights saved in models/ZeroShot/0/checkpoint-104/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:45:19,350 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-104/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:45:19,350 >> Special tokens file saved in models/ZeroShot/0/checkpoint-104/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:45:19,639 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-520] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:45:19,659 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1040] due to args.save_total_limit\n",
            " 20% 208/1040 [02:07<08:09,  1.70it/s][INFO|trainer.py:230] 2022-07-26 02:46:20,178 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:46:20,180 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:46:20,180 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:46:20,180 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.86it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.61it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.55it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.94it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.87it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.59it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.42it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.36it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.34it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.39it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.36it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.31it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.28it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.31it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.33it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.31it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.32it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.24it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.31it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.34it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.38it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.31it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.33it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.31it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.035306453704834, 'eval_accuracy': 0.6266094446182251, 'eval_f1': 0.6241099243435692, 'eval_runtime': 4.7359, 'eval_samples_per_second': 98.397, 'eval_steps_per_second': 12.458, 'epoch': 2.0}\n",
            " 20% 208/1040 [02:11<08:09,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.71it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:46:24,917 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-208\n",
            "[INFO|loading.py:60] 2022-07-26 02:46:24,918 >> Configuration saved in models/ZeroShot/0/checkpoint-208/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:46:25,007 >> Module weights saved in models/ZeroShot/0/checkpoint-208/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:46:25,007 >> Configuration saved in models/ZeroShot/0/checkpoint-208/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:46:25,014 >> Module weights saved in models/ZeroShot/0/checkpoint-208/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:46:25,014 >> Configuration saved in models/ZeroShot/0/checkpoint-208/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:46:25,021 >> Module weights saved in models/ZeroShot/0/checkpoint-208/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:46:25,021 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-208/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:46:25,021 >> Special tokens file saved in models/ZeroShot/0/checkpoint-208/special_tokens_map.json\n",
            " 30% 312/1040 [03:13<07:04,  1.72it/s][INFO|trainer.py:230] 2022-07-26 02:47:26,408 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:47:26,409 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:47:26,409 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:47:26,409 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.30it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.86it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.80it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.19it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.91it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.69it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.52it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.52it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.44it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.36it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.40it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.41it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.31it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.36it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.32it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.36it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.33it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.36it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.25it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.37it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.36it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.42it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.26it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.23it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8831354975700378, 'eval_accuracy': 0.7060086131095886, 'eval_f1': 0.6807820195504888, 'eval_runtime': 4.7227, 'eval_samples_per_second': 98.672, 'eval_steps_per_second': 12.493, 'epoch': 3.0}\n",
            " 30% 312/1040 [03:18<07:04,  1.72it/s]\n",
            "100% 59/59 [00:04<00:00, 13.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:47:31,133 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-312\n",
            "[INFO|loading.py:60] 2022-07-26 02:47:31,134 >> Configuration saved in models/ZeroShot/0/checkpoint-312/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:47:31,222 >> Module weights saved in models/ZeroShot/0/checkpoint-312/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:47:31,223 >> Configuration saved in models/ZeroShot/0/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:47:31,229 >> Module weights saved in models/ZeroShot/0/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:47:31,229 >> Configuration saved in models/ZeroShot/0/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:47:31,235 >> Module weights saved in models/ZeroShot/0/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:47:31,236 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:47:31,236 >> Special tokens file saved in models/ZeroShot/0/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:47:31,522 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-208] due to args.save_total_limit\n",
            " 40% 416/1040 [04:19<06:06,  1.70it/s][INFO|trainer.py:230] 2022-07-26 02:48:32,629 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:48:32,630 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:48:32,630 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:48:32,630 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.64it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.93it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.95it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.06it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.86it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.58it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.58it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.48it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.35it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.40it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.39it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.35it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.33it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.35it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.18it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.25it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.32it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.34it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.36it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.37it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.25it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.29it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.32it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.33it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.30it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.28it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.23it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.147356390953064, 'eval_accuracy': 0.6909871101379395, 'eval_f1': 0.6741258741258742, 'eval_runtime': 4.7322, 'eval_samples_per_second': 98.475, 'eval_steps_per_second': 12.468, 'epoch': 4.0}\n",
            " 40% 416/1040 [04:24<06:06,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.63it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:48:37,364 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-416\n",
            "[INFO|loading.py:60] 2022-07-26 02:48:37,365 >> Configuration saved in models/ZeroShot/0/checkpoint-416/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:48:37,458 >> Module weights saved in models/ZeroShot/0/checkpoint-416/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:48:37,458 >> Configuration saved in models/ZeroShot/0/checkpoint-416/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:48:37,465 >> Module weights saved in models/ZeroShot/0/checkpoint-416/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:48:37,465 >> Configuration saved in models/ZeroShot/0/checkpoint-416/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:48:37,472 >> Module weights saved in models/ZeroShot/0/checkpoint-416/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:48:37,473 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-416/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:48:37,473 >> Special tokens file saved in models/ZeroShot/0/checkpoint-416/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:48:37,781 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-312] due to args.save_total_limit\n",
            "{'loss': 0.2511, 'learning_rate': 5.192307692307693e-05, 'epoch': 4.81}\n",
            " 50% 520/1040 [05:25<05:03,  1.72it/s][INFO|trainer.py:230] 2022-07-26 02:49:38,804 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:49:38,806 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:49:38,806 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:49:38,806 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.96it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.11it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.90it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.28it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.89it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.60it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.56it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.39it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.38it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.32it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.34it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.28it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.33it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.28it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.30it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.33it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.28it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.36it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.31it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.30it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.35it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.32it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.33it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.40it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.4145371913909912, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6656774040372238, 'eval_runtime': 4.7221, 'eval_samples_per_second': 98.684, 'eval_steps_per_second': 12.494, 'epoch': 5.0}\n",
            " 50% 520/1040 [05:30<05:03,  1.72it/s]\n",
            "100% 59/59 [00:04<00:00, 13.68it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:49:43,530 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-520\n",
            "[INFO|loading.py:60] 2022-07-26 02:49:43,530 >> Configuration saved in models/ZeroShot/0/checkpoint-520/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:49:43,617 >> Module weights saved in models/ZeroShot/0/checkpoint-520/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:49:43,618 >> Configuration saved in models/ZeroShot/0/checkpoint-520/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:49:43,624 >> Module weights saved in models/ZeroShot/0/checkpoint-520/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:49:43,625 >> Configuration saved in models/ZeroShot/0/checkpoint-520/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:49:43,631 >> Module weights saved in models/ZeroShot/0/checkpoint-520/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:49:43,632 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:49:43,632 >> Special tokens file saved in models/ZeroShot/0/checkpoint-520/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:49:43,907 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-416] due to args.save_total_limit\n",
            " 60% 624/1040 [06:31<04:05,  1.70it/s][INFO|trainer.py:230] 2022-07-26 02:50:45,035 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:50:45,037 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:50:45,038 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:50:45,038 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.21it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.75it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.63it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.97it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.82it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.71it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.55it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.48it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.43it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.38it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.34it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.37it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.35it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.33it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.27it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.32it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.31it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.28it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.36it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.28it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.29it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.32it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.29it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.30it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.36it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.593206524848938, 'eval_accuracy': 0.7103004455566406, 'eval_f1': 0.6900908889381512, 'eval_runtime': 4.7324, 'eval_samples_per_second': 98.47, 'eval_steps_per_second': 12.467, 'epoch': 6.0}\n",
            " 60% 624/1040 [06:36<04:05,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.72it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:50:49,771 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-624\n",
            "[INFO|loading.py:60] 2022-07-26 02:50:49,772 >> Configuration saved in models/ZeroShot/0/checkpoint-624/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:50:49,861 >> Module weights saved in models/ZeroShot/0/checkpoint-624/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:50:49,861 >> Configuration saved in models/ZeroShot/0/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:50:49,868 >> Module weights saved in models/ZeroShot/0/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:50:49,868 >> Configuration saved in models/ZeroShot/0/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:50:49,874 >> Module weights saved in models/ZeroShot/0/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:50:49,875 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:50:49,875 >> Special tokens file saved in models/ZeroShot/0/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:50:50,171 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-104] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:50:50,190 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-520] due to args.save_total_limit\n",
            " 70% 728/1040 [07:38<03:02,  1.71it/s][INFO|trainer.py:230] 2022-07-26 02:51:51,412 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:51:51,414 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:51:51,414 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:51:51,414 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.28it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.87it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.95it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.02it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.84it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.59it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.55it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.38it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.29it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.34it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.35it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.32it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.28it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.35it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.36it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.28it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.38it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.28it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.26it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.30it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.30it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.36it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.30it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.25it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7214982509613037, 'eval_accuracy': 0.6995708346366882, 'eval_f1': 0.6797250859106528, 'eval_runtime': 4.7271, 'eval_samples_per_second': 98.58, 'eval_steps_per_second': 12.481, 'epoch': 7.0}\n",
            " 70% 728/1040 [07:43<03:02,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.71it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:51:56,143 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-728\n",
            "[INFO|loading.py:60] 2022-07-26 02:51:56,144 >> Configuration saved in models/ZeroShot/0/checkpoint-728/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:51:56,230 >> Module weights saved in models/ZeroShot/0/checkpoint-728/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:51:56,231 >> Configuration saved in models/ZeroShot/0/checkpoint-728/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:51:56,238 >> Module weights saved in models/ZeroShot/0/checkpoint-728/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:51:56,238 >> Configuration saved in models/ZeroShot/0/checkpoint-728/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:51:56,245 >> Module weights saved in models/ZeroShot/0/checkpoint-728/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:51:56,245 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-728/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:51:56,245 >> Special tokens file saved in models/ZeroShot/0/checkpoint-728/special_tokens_map.json\n",
            " 80% 832/1040 [08:44<02:01,  1.71it/s][INFO|trainer.py:230] 2022-07-26 02:52:57,721 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:52:57,723 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:52:57,723 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:52:57,723 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.34it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.62it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.76it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.10it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.90it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.63it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.43it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.39it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.47it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.32it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.36it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.45it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.41it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.40it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.31it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.29it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.35it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.31it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.29it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.31it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.21it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.30it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.32it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.30it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.31it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.6707267761230469, 'eval_accuracy': 0.7124463319778442, 'eval_f1': 0.6934511536573392, 'eval_runtime': 4.7331, 'eval_samples_per_second': 98.456, 'eval_steps_per_second': 12.465, 'epoch': 8.0}\n",
            " 80% 832/1040 [08:49<02:01,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.68it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:53:02,457 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-832\n",
            "[INFO|loading.py:60] 2022-07-26 02:53:02,458 >> Configuration saved in models/ZeroShot/0/checkpoint-832/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:53:02,545 >> Module weights saved in models/ZeroShot/0/checkpoint-832/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:53:02,546 >> Configuration saved in models/ZeroShot/0/checkpoint-832/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:53:02,552 >> Module weights saved in models/ZeroShot/0/checkpoint-832/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:53:02,552 >> Configuration saved in models/ZeroShot/0/checkpoint-832/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:53:02,559 >> Module weights saved in models/ZeroShot/0/checkpoint-832/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:53:02,559 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-832/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:53:02,559 >> Special tokens file saved in models/ZeroShot/0/checkpoint-832/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:53:02,853 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-624] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:53:02,873 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-728] due to args.save_total_limit\n",
            " 90% 936/1040 [09:51<01:01,  1.69it/s][INFO|trainer.py:230] 2022-07-26 02:54:04,120 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:54:04,123 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:54:04,123 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:54:04,123 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.22it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.85it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.72it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.87it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.87it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.79it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.50it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.47it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.26it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.33it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.40it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.43it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.18it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.29it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.37it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.46it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.37it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.32it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.21it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.22it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.32it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.42it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.40it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.43it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.28it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.720757007598877, 'eval_accuracy': 0.7081544995307922, 'eval_f1': 0.6859153533551392, 'eval_runtime': 4.7327, 'eval_samples_per_second': 98.464, 'eval_steps_per_second': 12.466, 'epoch': 9.0}\n",
            " 90% 936/1040 [09:55<01:01,  1.69it/s]\n",
            "100% 59/59 [00:04<00:00, 13.54it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:54:08,858 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-936\n",
            "[INFO|loading.py:60] 2022-07-26 02:54:08,859 >> Configuration saved in models/ZeroShot/0/checkpoint-936/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:54:08,947 >> Module weights saved in models/ZeroShot/0/checkpoint-936/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:54:08,947 >> Configuration saved in models/ZeroShot/0/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:54:08,954 >> Module weights saved in models/ZeroShot/0/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:54:08,954 >> Configuration saved in models/ZeroShot/0/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:54:08,960 >> Module weights saved in models/ZeroShot/0/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:54:08,961 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:54:08,961 >> Special tokens file saved in models/ZeroShot/0/checkpoint-936/special_tokens_map.json\n",
            "{'loss': 0.0165, 'learning_rate': 3.846153846153847e-06, 'epoch': 9.62}\n",
            "100% 1040/1040 [10:57<00:00,  1.71it/s][INFO|trainer.py:230] 2022-07-26 02:55:10,405 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:55:10,407 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:55:10,407 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:55:10,407 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.02it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.63it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.73it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.93it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.77it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.61it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.61it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.44it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.41it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.31it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.35it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.40it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.38it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.32it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.26it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.32it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.35it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.33it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.30it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.24it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.29it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.31it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.32it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.35it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.25it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7541111707687378, 'eval_accuracy': 0.716738224029541, 'eval_f1': 0.69733112895607, 'eval_runtime': 4.7391, 'eval_samples_per_second': 98.332, 'eval_steps_per_second': 12.45, 'epoch': 10.0}\n",
            "100% 1040/1040 [11:02<00:00,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.66it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 02:55:15,147 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1040\n",
            "[INFO|loading.py:60] 2022-07-26 02:55:15,148 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:55:15,233 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:55:15,233 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:55:15,239 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:55:15,240 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:55:15,246 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:55:15,246 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1040/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:55:15,247 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1040/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:55:15,541 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-832] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 02:55:15,560 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-936] due to args.save_total_limit\n",
            "[INFO|trainer.py:1483] 2022-07-26 02:55:15,581 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1492] 2022-07-26 02:55:15,581 >> Loading best model from models/ZeroShot/0/checkpoint-1040 (score: 0.69733112895607).\n",
            "[WARNING|trainer.py:1515] 2022-07-26 02:55:15,581 >> Could not locate the best model at models/ZeroShot/0/checkpoint-1040/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 662.5504, 'train_samples_per_second': 50.215, 'train_steps_per_second': 1.57, 'train_loss': 0.12872931736055762, 'epoch': 10.0}\n",
            "100% 1040/1040 [11:02<00:00,  1.71it/s][INFO|trainer.py:260] 2022-07-26 02:55:15,583 >> Loading best adapter(s) from models/ZeroShot/0/checkpoint-1040 (score: 0.69733112895607).\n",
            "[INFO|loading.py:77] 2022-07-26 02:55:15,583 >> Loading module configuration from models/ZeroShot/0/checkpoint-1040/glue/adapter_config.json\n",
            "[WARNING|loading.py:448] 2022-07-26 02:55:15,583 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-07-26 02:55:15,622 >> Loading module weights from models/ZeroShot/0/checkpoint-1040/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-07-26 02:55:15,644 >> Loading module configuration from models/ZeroShot/0/checkpoint-1040/glue/head_config.json\n",
            "[WARNING|loading.py:726] 2022-07-26 02:55:15,645 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:616] 2022-07-26 02:55:15,654 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-07-26 02:55:15,656 >> Loading module weights from models/ZeroShot/0/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "100% 1040/1040 [11:02<00:00,  1.57it/s]\n",
            "[INFO|trainer.py:136] 2022-07-26 02:55:15,659 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|loading.py:60] 2022-07-26 02:55:15,659 >> Configuration saved in models/ZeroShot/0/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:55:15,782 >> Module weights saved in models/ZeroShot/0/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:55:15,783 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:55:15,791 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 02:55:15,792 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 02:55:15,798 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 02:55:15,798 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 02:55:15,799 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.1287\n",
            "  train_runtime            = 0:11:02.55\n",
            "  train_samples            =       3327\n",
            "  train_samples_per_second =     50.215\n",
            "  train_steps_per_second   =       1.57\n",
            "07/26/2022 02:55:15 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:230] 2022-07-26 02:55:15,835 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 02:55:15,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 02:55:15,838 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 02:55:15,838 >>   Batch size = 8\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 676, in <module>\n",
            "    main()\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 614, in main\n",
            "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2269, in evaluate\n",
            "    metric_key_prefix=metric_key_prefix,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2436, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2646, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1991, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/models/bert.py\", line 85, in forward\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 745, in forward_head\n",
            "    return head_module(all_outputs, cls_output, attention_mask, return_dict, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 125, in forward\n",
            "    logits = super().forward(cls_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 02 : Multilingual adapter-based BERT for zero-shot multilingual idiomaticity detection"
      ],
      "metadata": {
        "id": "DMIhGZVs1SXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 10.0 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/ZeroShot/0/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --seed 0 \\\n",
        "  --train_file      Data/ZeroShot/train.csv \\\n",
        "  --validation_file Data/ZeroShot/dev.csv \\\n",
        "\t--evaluation_strategy \"epoch\" \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "UprIOfoqiaMm",
        "outputId": "6c5f6d18-d228-4707-8e49-9f04c9b5fd6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 08:09:05 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 08:09:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Jul26_08-09-05_eaa0ebcd8e09,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 08:09:05 - INFO - __main__ - load a local file for train: Data/ZeroShot/train.csv\n",
            "07/26/2022 08:09:05 - INFO - __main__ - load a local file for validation: Data/ZeroShot/dev.csv\n",
            "07/26/2022 08:09:06 - WARNING - datasets.builder - Using custom data configuration default-e3594ed76533d68d\n",
            "07/26/2022 08:09:06 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-e3594ed76533d68d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e3594ed76533d68d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11650.84it/s]\n",
            "07/26/2022 08:09:06 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/26/2022 08:09:06 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1378.57it/s]\n",
            "07/26/2022 08:09:06 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/26/2022 08:09:06 - INFO - datasets.builder - Generating train split\n",
            "07/26/2022 08:09:06 - INFO - datasets.builder - Generating validation split\n",
            "07/26/2022 08:09:06 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e3594ed76533d68d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1029.02it/s]\n",
            "[INFO|file_utils.py:2215] 2022-07-26 08:09:07,758 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp37lfrphz\n",
            "Downloading: 100% 625/625 [00:00<00:00, 588kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 08:09:08,656 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|file_utils.py:2227] 2022-07-26 08:09:08,657 >> creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 08:09:08,657 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 08:09:08,658 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-07-26 08:09:09,545 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptbw__55r\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 27.0kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 08:09:10,440 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:2227] 2022-07-26 08:09:10,440 >> creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 08:09:11,332 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 08:09:11,333 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-07-26 08:09:13,123 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6oc6rla7\n",
            "Downloading: 100% 972k/972k [00:01<00:00, 749kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 08:09:15,389 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:2227] 2022-07-26 08:09:15,389 >> creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:2215] 2022-07-26 08:09:16,297 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpev3hipsp\n",
            "Downloading: 100% 1.87M/1.87M [00:01<00:00, 1.27MB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 08:09:18,788 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|file_utils.py:2227] 2022-07-26 08:09:18,788 >> creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:09:21,484 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:09:21,484 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:09:21,484 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:09:21,484 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:09:21,484 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 08:09:22,390 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 08:09:22,390 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-07-26 08:09:23,402 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_z81ufuc\n",
            "Downloading: 100% 681M/681M [00:11<00:00, 62.0MB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 08:09:34,962 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|file_utils.py:2227] 2022-07-26 08:09:34,963 >> creating metadata file for /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|modeling_utils.py:1431] 2022-07-26 08:09:34,963 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:1694] 2022-07-26 08:09:37,061 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1711] 2022-07-26 08:09:37,062 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:616] 2022-07-26 08:09:37,071 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter Config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:617] 2022-07-26 08:09:37,072 >> Adding adapter 'glue'.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]07/26/2022 08:09:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e3594ed76533d68d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-75f628a225c1e336.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:00<00:00,  5.23ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/26/2022 08:09:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e3594ed76533d68d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-c0bd0cb4de649da9.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  6.55ba/s]\n",
            "07/26/2022 08:09:38 - INFO - __main__ - Sample 3155 of the training set: {'label': 0, 'sentence1': 'According to history.com, the leap day was originally discovered by Egyptian astronomers, but this discovery did not reach the western world until Julius Caesar’s reign in 45 BC. Caesar then created the leap year calendar to fix the problem, which was later adapted in accordance with new knowledge about the earth’s orbit by Pope Gregory into the Gregorian calendar that we observe today. Feb. 29 happens every four years, because the earth technically requires 365.25 days to complete its orbit around the sun.', 'input_ids': [101, 14156, 10114, 11486, 119, 10212, 117, 10105, 20169, 10410, 11940, 10134, 15556, 21756, 10155, 34624, 50575, 10901, 117, 10473, 10531, 30419, 12172, 10472, 24278, 10105, 16672, 11356, 11444, 18703, 30159, 100, 187, 38587, 10106, 10827, 19376, 119, 30159, 11059, 13745, 10105, 20169, 10410, 10924, 61637, 10114, 14045, 10686, 10105, 18077, 117, 10319, 10134, 10873, 40851, 10106, 88651, 10169, 10751, 22975, 10978, 10105, 39189, 100, 187, 17090, 10155, 23874, 22392, 10708, 10105, 47723, 11630, 61637, 10189, 11951, 78275, 18745, 119, 21194, 119, 10386, 105315, 14234, 11598, 10855, 117, 12373, 10105, 39189, 29914, 10454, 39575, 25385, 119, 10258, 13990, 10114, 17876, 10474, 17090, 12166, 10105, 42230, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 08:09:38 - INFO - __main__ - Sample 3445 of the training set: {'label': 1, 'sentence1': 'A saída da crise, segundo Bachelet, depende de ações que garantam renda para os mais pobres e vacina para todos São Paulo – Alta comissária para os Direitos Humanos da Organização das Nações Unidas (ONU), a ex-presidenta do Chile Michelle Bachelet criticou governantes de países pobres e ricos que optaram pela economia em vez de promover a saúde da população. O resultado, segundo ela, foi o aprofundamento das desigualdades sociais causadas pela histórica falta de investimento em áreas sociais, entre elas a saúde.', 'input_ids': [101, 138, 70747, 10143, 34862, 117, 12943, 18965, 41583, 117, 59216, 10104, 77302, 10121, 24457, 14732, 10147, 21367, 10220, 10427, 10614, 64440, 173, 10321, 22849, 10220, 12656, 12114, 13360, 100, 22798, 10212, 47097, 19075, 10220, 10427, 66130, 10107, 87537, 10143, 72154, 27187, 10242, 95322, 49323, 113, 46743, 114, 117, 169, 11419, 118, 107041, 10149, 13218, 27062, 18965, 41583, 82389, 10138, 25574, 25819, 10104, 15395, 64440, 173, 99702, 10121, 10303, 35474, 10147, 11793, 28223, 10266, 11675, 10104, 57833, 169, 54728, 10143, 17857, 119, 152, 20229, 117, 12943, 12593, 117, 10448, 183, 26219, 10567, 55227, 16686, 10242, 10139, 104915, 110285, 55167, 102453, 10107, 11793, 36818, 23821, 10104, 10106, 63996, 11498, 10266, 23571, 55167, 117, 10402, 34338, 169, 54728, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 08:09:38 - INFO - __main__ - Sample 331 of the training set: {'label': 1, 'sentence1': 'Concept Smoke Screen, in partnership with G4S, have developed a new way of defending cash and guards against attacks when replenishing ATM\\'s. On May 11th at the IFSEC Security Industry Awards 2009, Concept Smoke Screen were honoured with the \"Physical Security Product of the Year\" award, for the Guardian Smoke Screen. The ceremony was conducted at the Birmingham Hilton Metropole.', 'input_ids': [101, 77961, 80677, 41131, 117, 10106, 36944, 10169, 144, 11011, 10731, 117, 10529, 14628, 169, 10751, 13170, 10108, 53730, 52828, 10111, 99024, 11327, 26483, 10841, 76456, 81635, 74062, 92233, 112, 187, 119, 10576, 10725, 34062, 10160, 10105, 25000, 39039, 10858, 20924, 25539, 12357, 10195, 117, 77961, 80677, 41131, 10309, 46948, 10336, 10169, 10105, 107, 33671, 20924, 93218, 10108, 10105, 13567, 107, 17725, 117, 10142, 10105, 20206, 80677, 41131, 119, 10117, 34713, 10134, 23736, 10160, 10105, 22712, 53329, 20640, 30328, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:230] 2022-07-26 08:09:45,554 >> The following columns in the training set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1254] 2022-07-26 08:09:45,567 >> ***** Running training *****\n",
            "[INFO|trainer.py:1255] 2022-07-26 08:09:45,567 >>   Num examples = 4491\n",
            "[INFO|trainer.py:1256] 2022-07-26 08:09:45,567 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1257] 2022-07-26 08:09:45,567 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1258] 2022-07-26 08:09:45,567 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1259] 2022-07-26 08:09:45,567 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1260] 2022-07-26 08:09:45,567 >>   Total optimization steps = 1410\n",
            " 10% 141/1410 [01:14<09:14,  2.29it/s][INFO|trainer.py:230] 2022-07-26 08:11:00,258 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:11:00,260 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:11:00,260 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:11:00,260 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.09it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.31it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 15.24it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.54it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 14.34it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.99it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.87it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.88it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.87it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.81it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.63it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.65it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 13.61it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.56it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.60it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.67it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.71it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.63it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.60it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.57it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.57it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.57it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.63it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.57it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.62it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.57it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.55it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.51it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.53it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.60it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.51it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 13.53it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.55it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.46it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.48it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.52it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.50it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.45it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 13.37it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.43it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.46it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.48it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.45it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6421480178833008, 'eval_accuracy': 0.6576454639434814, 'eval_f1': 0.656639505179782, 'eval_runtime': 6.8107, 'eval_samples_per_second': 108.505, 'eval_steps_per_second': 13.655, 'epoch': 1.0}\n",
            " 10% 141/1410 [01:21<09:14,  2.29it/s]\n",
            "100% 93/93 [00:06<00:00, 13.49it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:11:07,072 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-141\n",
            "[INFO|loading.py:60] 2022-07-26 08:11:07,073 >> Configuration saved in models/ZeroShot/0/checkpoint-141/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:11:07,201 >> Module weights saved in models/ZeroShot/0/checkpoint-141/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:11:07,201 >> Configuration saved in models/ZeroShot/0/checkpoint-141/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:11:07,210 >> Module weights saved in models/ZeroShot/0/checkpoint-141/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:11:07,210 >> Configuration saved in models/ZeroShot/0/checkpoint-141/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:11:07,218 >> Module weights saved in models/ZeroShot/0/checkpoint-141/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:11:07,218 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-141/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:11:07,218 >> Special tokens file saved in models/ZeroShot/0/checkpoint-141/special_tokens_map.json\n",
            " 20% 282/1410 [02:39<08:19,  2.26it/s][INFO|trainer.py:230] 2022-07-26 08:12:24,982 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:12:24,984 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:12:24,984 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:12:24,984 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.19it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.93it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.90it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.28it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 14.07it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.76it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.64it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.76it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.77it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.76it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.64it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.50it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 13.51it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.49it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.51it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.56it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.56it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.46it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.36it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.42it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.37it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.42it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.32it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.37it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.42it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.40it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.39it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.39it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.42it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.32it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.31it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.28it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.37it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.34it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.45it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.40it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7077714204788208, 'eval_accuracy': 0.622462809085846, 'eval_f1': 0.6204361152787622, 'eval_runtime': 6.8736, 'eval_samples_per_second': 107.513, 'eval_steps_per_second': 13.53, 'epoch': 2.0}\n",
            " 20% 282/1410 [02:46<08:19,  2.26it/s]\n",
            "100% 93/93 [00:06<00:00, 13.43it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:12:31,859 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-282\n",
            "[INFO|loading.py:60] 2022-07-26 08:12:31,860 >> Configuration saved in models/ZeroShot/0/checkpoint-282/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:12:31,948 >> Module weights saved in models/ZeroShot/0/checkpoint-282/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:12:31,948 >> Configuration saved in models/ZeroShot/0/checkpoint-282/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:12:31,955 >> Module weights saved in models/ZeroShot/0/checkpoint-282/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:12:31,955 >> Configuration saved in models/ZeroShot/0/checkpoint-282/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:12:31,961 >> Module weights saved in models/ZeroShot/0/checkpoint-282/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:12:31,961 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-282/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:12:31,961 >> Special tokens file saved in models/ZeroShot/0/checkpoint-282/special_tokens_map.json\n",
            " 30% 423/1410 [04:04<07:20,  2.24it/s][INFO|trainer.py:230] 2022-07-26 08:13:49,644 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:13:49,646 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:13:49,646 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:13:49,646 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 19.84it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:05, 16.42it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:05, 15.11it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:05, 14.49it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:05, 13.99it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:05, 13.70it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:05, 13.60it/s]\u001b[A\n",
            " 18% 17/93 [00:01<00:05, 13.56it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:05, 13.60it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:05, 13.44it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:05, 13.42it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:05, 13.40it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:04, 13.33it/s]\u001b[A\n",
            " 31% 29/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 33% 31/93 [00:02<00:04, 13.47it/s]\u001b[A\n",
            " 35% 33/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:04, 13.45it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:04, 13.43it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:04, 13.36it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 13.42it/s]\u001b[A\n",
            " 46% 43/93 [00:03<00:03, 13.39it/s]\u001b[A\n",
            " 48% 45/93 [00:03<00:03, 13.37it/s]\u001b[A\n",
            " 51% 47/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 53% 49/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 55% 51/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 13.42it/s]\u001b[A\n",
            " 59% 55/93 [00:04<00:02, 13.42it/s]\u001b[A\n",
            " 61% 57/93 [00:04<00:02, 13.38it/s]\u001b[A\n",
            " 63% 59/93 [00:04<00:02, 13.44it/s]\u001b[A\n",
            " 66% 61/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 68% 63/93 [00:04<00:02, 13.35it/s]\u001b[A\n",
            " 70% 65/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 72% 67/93 [00:04<00:01, 13.32it/s]\u001b[A\n",
            " 74% 69/93 [00:05<00:01, 13.36it/s]\u001b[A\n",
            " 76% 71/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 78% 73/93 [00:05<00:01, 13.26it/s]\u001b[A\n",
            " 81% 75/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 83% 77/93 [00:05<00:01, 13.33it/s]\u001b[A\n",
            " 85% 79/93 [00:05<00:01, 13.30it/s]\u001b[A\n",
            " 87% 81/93 [00:05<00:00, 13.34it/s]\u001b[A\n",
            " 89% 83/93 [00:06<00:00, 13.26it/s]\u001b[A\n",
            " 91% 85/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 94% 87/93 [00:06<00:00, 13.37it/s]\u001b[A\n",
            " 96% 89/93 [00:06<00:00, 13.36it/s]\u001b[A\n",
            " 98% 91/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9349392652511597, 'eval_accuracy': 0.6535859107971191, 'eval_f1': 0.6531890243008198, 'eval_runtime': 6.9038, 'eval_samples_per_second': 107.042, 'eval_steps_per_second': 13.471, 'epoch': 3.0}\n",
            " 30% 423/1410 [04:10<07:20,  2.24it/s]\n",
            "100% 93/93 [00:06<00:00, 14.50it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:13:56,551 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-423\n",
            "[INFO|loading.py:60] 2022-07-26 08:13:56,552 >> Configuration saved in models/ZeroShot/0/checkpoint-423/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:13:56,638 >> Module weights saved in models/ZeroShot/0/checkpoint-423/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:13:56,638 >> Configuration saved in models/ZeroShot/0/checkpoint-423/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:13:56,645 >> Module weights saved in models/ZeroShot/0/checkpoint-423/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:13:56,645 >> Configuration saved in models/ZeroShot/0/checkpoint-423/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:13:56,651 >> Module weights saved in models/ZeroShot/0/checkpoint-423/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:13:56,652 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-423/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:13:56,652 >> Special tokens file saved in models/ZeroShot/0/checkpoint-423/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:13:57,104 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-282] due to args.save_total_limit\n",
            "{'loss': 0.4454, 'learning_rate': 6.453900709219859e-05, 'epoch': 3.55}\n",
            " 40% 564/1410 [05:28<06:16,  2.25it/s][INFO|trainer.py:230] 2022-07-26 08:15:14,300 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:15:14,302 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:15:14,302 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:15:14,302 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.52it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.82it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.87it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.34it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.96it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.61it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.56it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.58it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.49it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.35it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.56it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.64it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.53it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.52it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.46it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.46it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.52it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.49it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.44it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.49it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.49it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.51it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.50it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.50it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.37it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.43it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.42it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.52it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.50it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.41it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.38it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.50it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9964357614517212, 'eval_accuracy': 0.6711772680282593, 'eval_f1': 0.6700074423220044, 'eval_runtime': 6.8665, 'eval_samples_per_second': 107.623, 'eval_steps_per_second': 13.544, 'epoch': 4.0}\n",
            " 40% 564/1410 [05:35<06:16,  2.25it/s]\n",
            "100% 93/93 [00:06<00:00, 13.43it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:15:21,170 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-564\n",
            "[INFO|loading.py:60] 2022-07-26 08:15:21,171 >> Configuration saved in models/ZeroShot/0/checkpoint-564/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:15:21,257 >> Module weights saved in models/ZeroShot/0/checkpoint-564/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:15:21,257 >> Configuration saved in models/ZeroShot/0/checkpoint-564/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:15:21,264 >> Module weights saved in models/ZeroShot/0/checkpoint-564/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:15:21,264 >> Configuration saved in models/ZeroShot/0/checkpoint-564/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:15:21,270 >> Module weights saved in models/ZeroShot/0/checkpoint-564/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:15:21,270 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-564/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:15:21,270 >> Special tokens file saved in models/ZeroShot/0/checkpoint-564/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:15:21,710 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-141] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:15:21,730 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-423] due to args.save_total_limit\n",
            " 50% 705/1410 [06:53<05:14,  2.24it/s][INFO|trainer.py:230] 2022-07-26 08:16:38,920 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:16:38,922 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:16:38,922 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:16:38,922 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 19.91it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:05, 16.32it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:05, 14.94it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:05, 14.30it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:05, 13.99it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:05, 13.54it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:05, 13.59it/s]\u001b[A\n",
            " 18% 17/93 [00:01<00:05, 13.64it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:05, 13.58it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:05, 13.51it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:05, 13.42it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:04, 13.40it/s]\u001b[A\n",
            " 31% 29/93 [00:02<00:04, 13.39it/s]\u001b[A\n",
            " 33% 31/93 [00:02<00:04, 13.40it/s]\u001b[A\n",
            " 35% 33/93 [00:02<00:04, 13.51it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:04, 13.50it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:04, 13.46it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 13.37it/s]\u001b[A\n",
            " 46% 43/93 [00:03<00:03, 13.39it/s]\u001b[A\n",
            " 48% 45/93 [00:03<00:03, 13.48it/s]\u001b[A\n",
            " 51% 47/93 [00:03<00:03, 13.48it/s]\u001b[A\n",
            " 53% 49/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 55% 51/93 [00:03<00:03, 13.42it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 13.40it/s]\u001b[A\n",
            " 59% 55/93 [00:04<00:02, 13.37it/s]\u001b[A\n",
            " 61% 57/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 63% 59/93 [00:04<00:02, 13.46it/s]\u001b[A\n",
            " 66% 61/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 68% 63/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 70% 65/93 [00:04<00:02, 13.39it/s]\u001b[A\n",
            " 72% 67/93 [00:04<00:01, 13.31it/s]\u001b[A\n",
            " 74% 69/93 [00:05<00:01, 13.35it/s]\u001b[A\n",
            " 76% 71/93 [00:05<00:01, 13.37it/s]\u001b[A\n",
            " 78% 73/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 81% 75/93 [00:05<00:01, 13.42it/s]\u001b[A\n",
            " 83% 77/93 [00:05<00:01, 13.30it/s]\u001b[A\n",
            " 85% 79/93 [00:05<00:01, 13.36it/s]\u001b[A\n",
            " 87% 81/93 [00:05<00:00, 13.33it/s]\u001b[A\n",
            " 89% 83/93 [00:06<00:00, 13.34it/s]\u001b[A\n",
            " 91% 85/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 94% 87/93 [00:06<00:00, 13.45it/s]\u001b[A\n",
            " 96% 89/93 [00:06<00:00, 13.37it/s]\u001b[A\n",
            " 98% 91/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.4931191205978394, 'eval_accuracy': 0.6495263576507568, 'eval_f1': 0.6487829493637205, 'eval_runtime': 6.8925, 'eval_samples_per_second': 107.218, 'eval_steps_per_second': 13.493, 'epoch': 5.0}\n",
            " 50% 705/1410 [07:00<05:14,  2.24it/s]\n",
            "100% 93/93 [00:06<00:00, 14.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:16:45,816 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-705\n",
            "[INFO|loading.py:60] 2022-07-26 08:16:45,816 >> Configuration saved in models/ZeroShot/0/checkpoint-705/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:16:45,907 >> Module weights saved in models/ZeroShot/0/checkpoint-705/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:16:45,908 >> Configuration saved in models/ZeroShot/0/checkpoint-705/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:16:45,914 >> Module weights saved in models/ZeroShot/0/checkpoint-705/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:16:45,914 >> Configuration saved in models/ZeroShot/0/checkpoint-705/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:16:45,920 >> Module weights saved in models/ZeroShot/0/checkpoint-705/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:16:45,921 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-705/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:16:45,921 >> Special tokens file saved in models/ZeroShot/0/checkpoint-705/special_tokens_map.json\n",
            " 60% 846/1410 [08:17<04:11,  2.25it/s][INFO|trainer.py:230] 2022-07-26 08:18:03,515 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:18:03,517 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:18:03,517 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:18:03,517 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.05it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.71it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.58it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.20it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.89it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.64it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.63it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.63it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.57it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.59it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.52it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.35it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.47it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.50it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.50it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.40it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.40it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.46it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.33it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.36it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.48it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.44it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.40it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.44it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.47it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.45it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.44it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.45it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.39it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.46it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.43it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.48it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.46it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.4189863204956055, 'eval_accuracy': 0.665764570236206, 'eval_f1': 0.6643484848763463, 'eval_runtime': 6.8829, 'eval_samples_per_second': 107.368, 'eval_steps_per_second': 13.512, 'epoch': 6.0}\n",
            " 60% 846/1410 [08:24<04:11,  2.25it/s]\n",
            "100% 93/93 [00:06<00:00, 13.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:18:10,401 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-846\n",
            "[INFO|loading.py:60] 2022-07-26 08:18:10,402 >> Configuration saved in models/ZeroShot/0/checkpoint-846/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:18:10,486 >> Module weights saved in models/ZeroShot/0/checkpoint-846/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:18:10,487 >> Configuration saved in models/ZeroShot/0/checkpoint-846/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:18:10,493 >> Module weights saved in models/ZeroShot/0/checkpoint-846/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:18:10,493 >> Configuration saved in models/ZeroShot/0/checkpoint-846/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:18:10,499 >> Module weights saved in models/ZeroShot/0/checkpoint-846/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:18:10,499 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-846/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:18:10,499 >> Special tokens file saved in models/ZeroShot/0/checkpoint-846/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:18:10,916 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-705] due to args.save_total_limit\n",
            " 70% 987/1410 [09:42<03:08,  2.24it/s][INFO|trainer.py:230] 2022-07-26 08:19:28,115 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:19:28,117 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:19:28,117 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:19:28,117 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.11it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.77it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.29it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.87it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.72it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.61it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.62it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.57it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.49it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.52it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.52it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.43it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.45it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.39it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.39it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.41it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.41it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.49it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.47it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.47it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.48it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.46it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.45it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.39it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.42it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.44it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.50it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.49it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.45it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.46it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8241209983825684, 'eval_accuracy': 0.672530472278595, 'eval_f1': 0.6725298469200908, 'eval_runtime': 6.8754, 'eval_samples_per_second': 107.485, 'eval_steps_per_second': 13.527, 'epoch': 7.0}\n",
            " 70% 987/1410 [09:49<03:08,  2.24it/s]\n",
            "100% 93/93 [00:06<00:00, 13.50it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:19:34,994 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-987\n",
            "[INFO|loading.py:60] 2022-07-26 08:19:34,994 >> Configuration saved in models/ZeroShot/0/checkpoint-987/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:19:35,077 >> Module weights saved in models/ZeroShot/0/checkpoint-987/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:19:35,078 >> Configuration saved in models/ZeroShot/0/checkpoint-987/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:19:35,084 >> Module weights saved in models/ZeroShot/0/checkpoint-987/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:19:35,084 >> Configuration saved in models/ZeroShot/0/checkpoint-987/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:19:35,090 >> Module weights saved in models/ZeroShot/0/checkpoint-987/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:19:35,091 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:19:35,091 >> Special tokens file saved in models/ZeroShot/0/checkpoint-987/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:19:35,533 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-564] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:19:35,554 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-846] due to args.save_total_limit\n",
            "{'loss': 0.0969, 'learning_rate': 2.9078014184397162e-05, 'epoch': 7.09}\n",
            " 80% 1128/1410 [11:07<02:06,  2.24it/s][INFO|trainer.py:230] 2022-07-26 08:20:52,618 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:20:52,620 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:20:52,620 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:20:52,620 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.09it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.70it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.60it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.15it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.96it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.73it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.64it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.52it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.50it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.44it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.38it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.38it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.38it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.50it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.45it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.36it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.29it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.42it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.30it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.37it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.39it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.38it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.36it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.29it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.37it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 73% 68/93 [00:05<00:01, 13.34it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.40it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.37it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.29it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.36it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.34it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.40it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.37it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.20it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.29it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.180638551712036, 'eval_accuracy': 0.6441136598587036, 'eval_f1': 0.641074650459744, 'eval_runtime': 6.9132, 'eval_samples_per_second': 106.897, 'eval_steps_per_second': 13.452, 'epoch': 8.0}\n",
            " 80% 1128/1410 [11:13<02:06,  2.24it/s]\n",
            "100% 93/93 [00:06<00:00, 13.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:20:59,534 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1128\n",
            "[INFO|loading.py:60] 2022-07-26 08:20:59,535 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:20:59,624 >> Module weights saved in models/ZeroShot/0/checkpoint-1128/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:20:59,625 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:20:59,631 >> Module weights saved in models/ZeroShot/0/checkpoint-1128/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:20:59,631 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:20:59,638 >> Module weights saved in models/ZeroShot/0/checkpoint-1128/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:20:59,638 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1128/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:20:59,638 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1128/special_tokens_map.json\n",
            " 90% 1269/1410 [12:31<01:02,  2.24it/s][INFO|trainer.py:230] 2022-07-26 08:22:17,332 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:22:17,334 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:22:17,334 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:22:17,334 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.03it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.73it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.48it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.10it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.78it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.68it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.53it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.52it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.53it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.43it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.39it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.39it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.26it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.33it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.33it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.34it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.26it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.34it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.36it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.35it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.42it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.37it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.36it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.37it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 73% 68/93 [00:05<00:01, 13.31it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.27it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.27it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.29it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.28it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.33it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.36it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.30it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.38it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.31it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.31it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.26it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.03139066696167, 'eval_accuracy': 0.6698240637779236, 'eval_f1': 0.6692420573776506, 'eval_runtime': 6.9244, 'eval_samples_per_second': 106.724, 'eval_steps_per_second': 13.431, 'epoch': 9.0}\n",
            " 90% 1269/1410 [12:38<01:02,  2.24it/s]\n",
            "100% 93/93 [00:06<00:00, 13.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:22:24,259 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1269\n",
            "[INFO|loading.py:60] 2022-07-26 08:22:24,260 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:22:24,352 >> Module weights saved in models/ZeroShot/0/checkpoint-1269/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:22:24,352 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:22:24,359 >> Module weights saved in models/ZeroShot/0/checkpoint-1269/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:22:24,359 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:22:24,365 >> Module weights saved in models/ZeroShot/0/checkpoint-1269/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:22:24,366 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1269/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:22:24,366 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1269/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:22:24,825 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1128] due to args.save_total_limit\n",
            "100% 1410/1410 [13:56<00:00,  2.24it/s][INFO|trainer.py:230] 2022-07-26 08:23:42,126 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:23:42,128 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:23:42,128 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:23:42,128 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.14it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.53it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.13it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.90it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.65it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.62it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.44it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.52it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.43it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.39it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.36it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.29it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.43it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.32it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.29it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.29it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.42it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.39it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.31it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.29it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.22it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.30it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.30it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.35it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.30it/s]\u001b[A\n",
            " 73% 68/93 [00:05<00:01, 13.37it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.34it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.37it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.29it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.31it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.34it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.45it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.32it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.32it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.29it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.195699453353882, 'eval_accuracy': 0.6617050170898438, 'eval_f1': 0.6605557515359965, 'eval_runtime': 6.9291, 'eval_samples_per_second': 106.652, 'eval_steps_per_second': 13.422, 'epoch': 10.0}\n",
            "100% 1410/1410 [14:03<00:00,  2.24it/s]\n",
            "100% 93/93 [00:06<00:00, 13.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:23:49,059 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1410\n",
            "[INFO|loading.py:60] 2022-07-26 08:23:49,059 >> Configuration saved in models/ZeroShot/0/checkpoint-1410/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:23:49,150 >> Module weights saved in models/ZeroShot/0/checkpoint-1410/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:23:49,151 >> Configuration saved in models/ZeroShot/0/checkpoint-1410/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:23:49,158 >> Module weights saved in models/ZeroShot/0/checkpoint-1410/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:23:49,158 >> Configuration saved in models/ZeroShot/0/checkpoint-1410/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:23:49,165 >> Module weights saved in models/ZeroShot/0/checkpoint-1410/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:23:49,165 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1410/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:23:49,165 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1410/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:23:49,589 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1269] due to args.save_total_limit\n",
            "[INFO|trainer.py:1483] 2022-07-26 08:23:49,619 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1492] 2022-07-26 08:23:49,619 >> Loading best model from models/ZeroShot/0/checkpoint-987 (score: 0.6725298469200908).\n",
            "[WARNING|trainer.py:1515] 2022-07-26 08:23:49,619 >> Could not locate the best model at models/ZeroShot/0/checkpoint-987/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 844.052, 'train_samples_per_second': 53.208, 'train_steps_per_second': 1.671, 'train_loss': 0.19955814510372513, 'epoch': 10.0}\n",
            "100% 1410/1410 [14:04<00:00,  2.24it/s][INFO|trainer.py:260] 2022-07-26 08:23:49,620 >> Loading best adapter(s) from models/ZeroShot/0/checkpoint-987 (score: 0.6725298469200908).\n",
            "[INFO|loading.py:77] 2022-07-26 08:23:49,621 >> Loading module configuration from models/ZeroShot/0/checkpoint-987/glue/adapter_config.json\n",
            "[WARNING|loading.py:448] 2022-07-26 08:23:49,621 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-07-26 08:23:49,643 >> Loading module weights from models/ZeroShot/0/checkpoint-987/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-07-26 08:23:49,664 >> Loading module configuration from models/ZeroShot/0/checkpoint-987/glue/head_config.json\n",
            "[WARNING|loading.py:726] 2022-07-26 08:23:49,664 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:616] 2022-07-26 08:23:49,673 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-07-26 08:23:49,674 >> Loading module weights from models/ZeroShot/0/checkpoint-987/glue/pytorch_model_head.bin\n",
            "100% 1410/1410 [14:04<00:00,  1.67it/s]\n",
            "[INFO|trainer.py:136] 2022-07-26 08:23:49,677 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|loading.py:60] 2022-07-26 08:23:49,678 >> Configuration saved in models/ZeroShot/0/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:23:49,796 >> Module weights saved in models/ZeroShot/0/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:23:49,796 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:23:49,803 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:23:49,804 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:23:49,810 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:23:49,810 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:23:49,810 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.1996\n",
            "  train_runtime            = 0:14:04.05\n",
            "  train_samples            =       4491\n",
            "  train_samples_per_second =     53.208\n",
            "  train_steps_per_second   =      1.671\n",
            "07/26/2022 08:23:50 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:230] 2022-07-26 08:23:50,010 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:23:50,012 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:23:50,012 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:23:50,012 >>   Batch size = 8\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 676, in <module>\n",
            "    main()\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 614, in main\n",
            "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2269, in evaluate\n",
            "    metric_key_prefix=metric_key_prefix,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2436, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2646, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1991, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/models/bert.py\", line 85, in forward\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 745, in forward_head\n",
            "    return head_module(all_outputs, cls_output, attention_mask, return_dict, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 125, in forward\n",
            "    logits = super().forward(cls_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 03 : Monolingual MirrorWiC BERT for zero-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "OSVIwo9icjqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cb0naD4cicp",
        "outputId": "3065af9f-38dc-43f8-e987-478359e6daa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'cambridgeltl/mirrorwic-bert-base-uncased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 15 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N09Hm5CxnL0",
        "outputId": "da16123e-690b-4f78-c702-ad3f1b96d32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 05:56:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 05:56:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Jul26_05-56-32_5a915ffe101d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 05:56:32 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "07/26/2022 05:56:32 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "07/26/2022 05:56:33 - WARNING - datasets.builder -   Using custom data configuration default-ddcb5645fd22925d\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-ddcb5645fd22925d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10965.50it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1203.19it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ddcb5645fd22925d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 639.47it/s]\n",
            "[INFO|hub.py:592] 2022-07-26 05:56:34,141 >> https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk_6k8mxa\n",
            "Downloading: 100% 599/599 [00:00<00:00, 184kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 05:56:35,080 >> storing https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|hub.py:604] 2022-07-26 05:56:35,080 >> creating metadata file for /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 05:56:35,081 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 05:56:35,090 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 05:56:36,002 >> https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpx11xqyv0\n",
            "Downloading: 100% 252/252 [00:00<00:00, 164kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 05:56:36,914 >> storing https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/d18bed018631fcadd197f6c150bccd823012f9747d6c14b19fff54b269a8d6d7.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
            "[INFO|hub.py:604] 2022-07-26 05:56:36,921 >> creating metadata file for /root/.cache/huggingface/transformers/d18bed018631fcadd197f6c150bccd823012f9747d6c14b19fff54b269a8d6d7.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 05:56:37,872 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 05:56:37,873 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 05:56:39,672 >> https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8bi4jef4\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 266kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 05:56:41,441 >> storing https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/462202e3879c0bc38b0f112d6513bd775a86e228e46ca8ceb6b7cc271fa96fe1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:604] 2022-07-26 05:56:41,441 >> creating metadata file for /root/.cache/huggingface/transformers/462202e3879c0bc38b0f112d6513bd775a86e228e46ca8ceb6b7cc271fa96fe1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:592] 2022-07-26 05:56:44,128 >> https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3kj8_m41\n",
            "Downloading: 100% 112/112 [00:00<00:00, 86.8kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 05:56:45,030 >> storing https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/eddf333a0b59fecb3d9a870823738ea744160a6e2c333ab92ce97e7149e09cd8.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|hub.py:604] 2022-07-26 05:56:45,030 >> creating metadata file for /root/.cache/huggingface/transformers/eddf333a0b59fecb3d9a870823738ea744160a6e2c333ab92ce97e7149e09cd8.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 05:56:45,939 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/462202e3879c0bc38b0f112d6513bd775a86e228e46ca8ceb6b7cc271fa96fe1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 05:56:45,939 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 05:56:45,939 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 05:56:45,939 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/eddf333a0b59fecb3d9a870823738ea744160a6e2c333ab92ce97e7149e09cd8.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 05:56:45,939 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d18bed018631fcadd197f6c150bccd823012f9747d6c14b19fff54b269a8d6d7.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 05:56:46,840 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 05:56:46,841 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 05:56:47,761 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 05:56:47,762 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 05:56:48,706 >> https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgdttvvcg\n",
            "Downloading: 100% 418M/418M [00:06<00:00, 71.8MB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 05:56:54,839 >> storing https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/d57b0eb5c893ede0ea4088e156c4c06f1157a296783e8e23dca56a9f3c68237b.ec3a6b253565b365acd2fadaaac221a38539339839ed69cbd7c22ca292ee9066\n",
            "[INFO|hub.py:604] 2022-07-26 05:56:54,839 >> creating metadata file for /root/.cache/huggingface/transformers/d57b0eb5c893ede0ea4088e156c4c06f1157a296783e8e23dca56a9f3c68237b.ec3a6b253565b365acd2fadaaac221a38539339839ed69cbd7c22ca292ee9066\n",
            "[INFO|modeling_utils.py:2107] 2022-07-26 05:56:54,839 >> loading weights file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/d57b0eb5c893ede0ea4088e156c4c06f1157a296783e8e23dca56a9f3c68237b.ec3a6b253565b365acd2fadaaac221a38539339839ed69cbd7c22ca292ee9066\n",
            "[INFO|modeling_utils.py:2483] 2022-07-26 05:56:56,119 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:2486] 2022-07-26 05:56:56,119 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cambridgeltl/mirrorwic-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  5.05ba/s]\n",
            "100% 1/1 [00:00<00:00,  8.58ba/s]\n",
            "07/26/2022 05:56:57 - INFO - __main__ -   Sample 1577 of the training set: {'label': 1, 'sentence1': 'Where do I stream Stag Night online? Stag Night is available to watch and stream, download, buy on demand at Amazon Prime, Amazon, Vudu, Google Play, iTunes, YouTube VOD online. Some platforms allow you to rent Stag Night for a limited time or purchase the movie and download it to your device.', 'input_ids': [101, 2073, 2079, 1045, 5460, 2358, 8490, 2305, 3784, 1029, 2358, 8490, 2305, 2003, 2800, 2000, 3422, 1998, 5460, 1010, 8816, 1010, 4965, 2006, 5157, 2012, 9733, 3539, 1010, 9733, 1010, 24728, 8566, 1010, 8224, 2377, 1010, 11943, 1010, 7858, 29536, 2094, 3784, 1012, 2070, 7248, 3499, 2017, 2000, 9278, 2358, 8490, 2305, 2005, 1037, 3132, 2051, 2030, 5309, 1996, 3185, 1998, 8816, 2009, 2000, 2115, 5080, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 05:56:57 - INFO - __main__ -   Sample 3104 of the training set: {'label': 1, 'sentence1': 'Did Biden think for himself before he reversed direction on the Keystone Pipeline or did he do it just because the last Democrat president, under whom he served, was against it? I applaud Biden’s decisions to rejoin the World Health Organization and to call a world conference on climate change because, for whatever reason, the climate is changing. The atmosphere is getting warmer and the storms are getting bigger, as evidenced by the one in California earlier this week.', 'input_ids': [101, 2106, 7226, 2368, 2228, 2005, 2370, 2077, 2002, 11674, 3257, 2006, 1996, 22271, 13117, 2030, 2106, 2002, 2079, 2009, 2074, 2138, 1996, 2197, 7672, 2343, 1010, 2104, 3183, 2002, 2366, 1010, 2001, 2114, 2009, 1029, 1045, 10439, 17298, 2094, 7226, 2368, 1521, 1055, 6567, 2000, 25261, 1996, 2088, 2740, 3029, 1998, 2000, 2655, 1037, 2088, 3034, 2006, 4785, 2689, 2138, 1010, 2005, 3649, 3114, 1010, 1996, 4785, 2003, 5278, 1012, 1996, 7224, 2003, 2893, 16676, 1998, 1996, 12642, 2024, 2893, 7046, 1010, 2004, 21328, 2011, 1996, 2028, 1999, 2662, 3041, 2023, 2733, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 05:56:57 - INFO - __main__ -   Sample 1722 of the training set: {'label': 1, 'sentence1': 'Mank leads the Critics Choice Awards 2021 nominations Coronavirus latest: Covid national research project will study effects of emerging mutations Jared Kushner and Ivanka Trump made up to $640 million while working in White House, report finds', 'input_ids': [101, 2158, 2243, 5260, 1996, 4401, 3601, 2982, 25682, 9930, 21887, 23350, 6745, 1024, 2522, 17258, 2120, 2470, 2622, 2097, 2817, 3896, 1997, 8361, 14494, 8334, 13970, 4095, 3678, 1998, 7332, 2912, 8398, 2081, 2039, 2000, 1002, 19714, 2454, 2096, 2551, 1999, 2317, 2160, 1010, 3189, 4858, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:662] 2022-07-26 05:57:01,511 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-26 05:57:01,522 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-26 05:57:01,522 >>   Num examples = 3327\n",
            "[INFO|trainer.py:1518] 2022-07-26 05:57:01,522 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1519] 2022-07-26 05:57:01,522 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1520] 2022-07-26 05:57:01,522 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-26 05:57:01,522 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-26 05:57:01,522 >>   Total optimization steps = 1560\n",
            "  7% 104/1560 [01:06<15:13,  1.59it/s][INFO|trainer.py:662] 2022-07-26 05:58:08,337 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 05:58:08,339 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 05:58:08,339 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 05:58:08,339 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 25.65it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 19.02it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:02, 18.04it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:02, 17.64it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:02, 17.57it/s]\u001b[A\n",
            " 25% 15/59 [00:00<00:02, 17.28it/s]\u001b[A\n",
            " 29% 17/59 [00:00<00:02, 17.05it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 17.07it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 16.76it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 16.72it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 16.71it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:01, 16.73it/s]\u001b[A\n",
            " 49% 29/59 [00:01<00:01, 16.86it/s]\u001b[A\n",
            " 53% 31/59 [00:01<00:01, 16.88it/s]\u001b[A\n",
            " 56% 33/59 [00:01<00:01, 16.82it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 16.80it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 16.69it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 16.69it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 16.81it/s]\u001b[A\n",
            " 73% 43/59 [00:02<00:00, 16.79it/s]\u001b[A\n",
            " 76% 45/59 [00:02<00:00, 16.76it/s]\u001b[A\n",
            " 80% 47/59 [00:02<00:00, 16.79it/s]\u001b[A\n",
            " 83% 49/59 [00:02<00:00, 16.72it/s]\u001b[A\n",
            " 86% 51/59 [00:02<00:00, 16.72it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 16.78it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 16.89it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6325397491455078, 'eval_accuracy': 0.6695278882980347, 'eval_f1': 0.6656790399522958, 'eval_runtime': 3.4781, 'eval_samples_per_second': 133.979, 'eval_steps_per_second': 16.963, 'epoch': 1.0}\n",
            "  7% 104/1560 [01:10<15:13,  1.59it/s]\n",
            "100% 59/59 [00:03<00:00, 16.89it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 05:58:11,819 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-104\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 05:58:11,820 >> Configuration saved in models/ZeroShot/0/checkpoint-104/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 05:58:13,088 >> Model weights saved in models/ZeroShot/0/checkpoint-104/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 05:58:13,089 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-104/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 05:58:13,089 >> Special tokens file saved in models/ZeroShot/0/checkpoint-104/special_tokens_map.json\n",
            " 13% 208/1560 [02:22<14:46,  1.52it/s][INFO|trainer.py:662] 2022-07-26 05:59:23,945 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 05:59:23,947 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 05:59:23,947 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 05:59:23,947 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 24.09it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 18.33it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:02, 17.65it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:02, 16.95it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 16.81it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 16.76it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 16.54it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 16.48it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 16.46it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 16.40it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 16.40it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 16.28it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:01, 16.25it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 16.36it/s]\u001b[A\n",
            " 54% 32/59 [00:01<00:01, 16.41it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 16.48it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 16.46it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 16.39it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 16.35it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 16.24it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 16.23it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 16.27it/s]\u001b[A\n",
            " 81% 48/59 [00:02<00:00, 16.37it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 16.47it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 16.51it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 16.52it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 16.39it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7381405830383301, 'eval_accuracy': 0.6845493316650391, 'eval_f1': 0.6714642673867064, 'eval_runtime': 3.5729, 'eval_samples_per_second': 130.427, 'eval_steps_per_second': 16.513, 'epoch': 2.0}\n",
            " 13% 208/1560 [02:25<14:46,  1.52it/s]\n",
            "100% 59/59 [00:03<00:00, 16.25it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 05:59:27,521 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-208\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 05:59:27,522 >> Configuration saved in models/ZeroShot/0/checkpoint-208/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 05:59:28,559 >> Model weights saved in models/ZeroShot/0/checkpoint-208/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 05:59:28,559 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-208/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 05:59:28,560 >> Special tokens file saved in models/ZeroShot/0/checkpoint-208/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 05:59:32,143 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-104] due to args.save_total_limit\n",
            " 20% 312/1560 [03:40<14:22,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:00:42,260 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:00:42,262 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:00:42,263 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:00:42,263 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.75it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.16it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.23it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.81it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.52it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.08it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.05it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.03it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 14.89it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.80it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 14.89it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 14.80it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 14.76it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 14.88it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 14.83it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.00it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.93it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.95it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 14.91it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 14.83it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 14.93it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 14.88it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 14.87it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 14.96it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.842491090297699, 'eval_accuracy': 0.716738224029541, 'eval_f1': 0.7047235023041474, 'eval_runtime': 3.9023, 'eval_samples_per_second': 119.418, 'eval_steps_per_second': 15.119, 'epoch': 3.0}\n",
            " 20% 312/1560 [03:44<14:22,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 14.92it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:00:46,166 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-312\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:00:46,167 >> Configuration saved in models/ZeroShot/0/checkpoint-312/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:00:47,096 >> Model weights saved in models/ZeroShot/0/checkpoint-312/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:00:47,096 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:00:47,096 >> Special tokens file saved in models/ZeroShot/0/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:00:50,685 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-208] due to args.save_total_limit\n",
            " 27% 416/1560 [05:01<13:12,  1.44it/s][INFO|trainer.py:662] 2022-07-26 06:02:02,547 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:02:02,549 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:02:02,549 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:02:02,549 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.24it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.05it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.30it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.96it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.75it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.35it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 14.94it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 14.93it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.10it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.16it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.16it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.21it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 14.89it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.03it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.20it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 15.00it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.01it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 14.94it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 14.79it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 14.96it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 14.94it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2581082582473755, 'eval_accuracy': 0.7124463319778442, 'eval_f1': 0.6761539259412923, 'eval_runtime': 3.885, 'eval_samples_per_second': 119.949, 'eval_steps_per_second': 15.187, 'epoch': 4.0}\n",
            " 27% 416/1560 [05:04<13:12,  1.44it/s]\n",
            "100% 59/59 [00:03<00:00, 14.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:02:06,435 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-416\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:02:06,436 >> Configuration saved in models/ZeroShot/0/checkpoint-416/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:02:07,353 >> Model weights saved in models/ZeroShot/0/checkpoint-416/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:02:07,354 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-416/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:02:07,354 >> Special tokens file saved in models/ZeroShot/0/checkpoint-416/special_tokens_map.json\n",
            "{'loss': 0.2244, 'learning_rate': 1.3589743589743592e-05, 'epoch': 4.81}\n",
            " 33% 520/1560 [06:21<12:00,  1.44it/s][INFO|trainer.py:662] 2022-07-26 06:03:23,323 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:03:23,324 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:03:23,324 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:03:23,325 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.44it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.46it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.25it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.86it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.61it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.20it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.40it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 14.89it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.08it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.05it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.00it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.5041438341140747, 'eval_accuracy': 0.7145922780036926, 'eval_f1': 0.6974247565113384, 'eval_runtime': 3.8548, 'eval_samples_per_second': 120.889, 'eval_steps_per_second': 15.306, 'epoch': 5.0}\n",
            " 33% 520/1560 [06:25<12:00,  1.44it/s]\n",
            "100% 59/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:03:27,181 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-520\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:03:27,182 >> Configuration saved in models/ZeroShot/0/checkpoint-520/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:03:28,240 >> Model weights saved in models/ZeroShot/0/checkpoint-520/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:03:28,241 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:03:28,241 >> Special tokens file saved in models/ZeroShot/0/checkpoint-520/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:03:31,793 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-416] due to args.save_total_limit\n",
            " 40% 624/1560 [07:42<10:44,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:04:44,375 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:04:44,377 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:04:44,377 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:04:44,377 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.81it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.26it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.42it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.88it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.81it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.39it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.40it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.07it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.22it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.30it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.22it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.18it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.08it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.731469988822937, 'eval_accuracy': 0.7038626670837402, 'eval_f1': 0.6843004418262151, 'eval_runtime': 3.8478, 'eval_samples_per_second': 121.107, 'eval_steps_per_second': 15.333, 'epoch': 6.0}\n",
            " 40% 624/1560 [07:46<10:44,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:04:48,227 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-624\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:04:48,227 >> Configuration saved in models/ZeroShot/0/checkpoint-624/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:04:49,199 >> Model weights saved in models/ZeroShot/0/checkpoint-624/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:04:49,199 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:04:49,200 >> Special tokens file saved in models/ZeroShot/0/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:04:52,847 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-520] due to args.save_total_limit\n",
            " 47% 728/1560 [09:03<09:35,  1.44it/s][INFO|trainer.py:662] 2022-07-26 06:06:05,515 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:06:05,517 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:06:05,517 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:06:05,517 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.15it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.52it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.56it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.88it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.63it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.16it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.04it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.90it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.02it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.06it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8379034996032715, 'eval_accuracy': 0.729613721370697, 'eval_f1': 0.7019371345029239, 'eval_runtime': 3.8509, 'eval_samples_per_second': 121.009, 'eval_steps_per_second': 15.321, 'epoch': 7.0}\n",
            " 47% 728/1560 [09:07<09:35,  1.44it/s]\n",
            "100% 59/59 [00:03<00:00, 15.03it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:06:09,369 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-728\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:06:09,370 >> Configuration saved in models/ZeroShot/0/checkpoint-728/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:06:10,371 >> Model weights saved in models/ZeroShot/0/checkpoint-728/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:06:10,372 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-728/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:06:10,372 >> Special tokens file saved in models/ZeroShot/0/checkpoint-728/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:06:14,219 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-624] due to args.save_total_limit\n",
            " 53% 832/1560 [10:25<08:20,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:07:26,700 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:07:26,702 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:07:26,702 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:07:26,702 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.97it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.42it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.47it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.84it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.85it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.42it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.55it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.10it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.10it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.09it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.9013175964355469, 'eval_accuracy': 0.721030056476593, 'eval_f1': 0.6942133742529478, 'eval_runtime': 3.8414, 'eval_samples_per_second': 121.311, 'eval_steps_per_second': 15.359, 'epoch': 8.0}\n",
            " 53% 832/1560 [10:29<08:20,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:07:30,544 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-832\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:07:30,545 >> Configuration saved in models/ZeroShot/0/checkpoint-832/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:07:31,518 >> Model weights saved in models/ZeroShot/0/checkpoint-832/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:07:31,518 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-832/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:07:31,519 >> Special tokens file saved in models/ZeroShot/0/checkpoint-832/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:07:35,193 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-728] due to args.save_total_limit\n",
            " 60% 936/1560 [11:46<07:09,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:08:47,689 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:08:47,691 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:08:47,691 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:08:47,691 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.14it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.39it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.39it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.97it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.65it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.32it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.45it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.10it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.11it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.99it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.18it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.08it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.036965847015381, 'eval_accuracy': 0.6995708346366882, 'eval_f1': 0.6831779331779332, 'eval_runtime': 3.8476, 'eval_samples_per_second': 121.114, 'eval_steps_per_second': 15.334, 'epoch': 9.0}\n",
            " 60% 936/1560 [11:49<07:09,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:08:51,540 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-936\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:08:51,541 >> Configuration saved in models/ZeroShot/0/checkpoint-936/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:08:52,502 >> Model weights saved in models/ZeroShot/0/checkpoint-936/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:08:52,502 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:08:52,503 >> Special tokens file saved in models/ZeroShot/0/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:08:56,191 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-832] due to args.save_total_limit\n",
            "{'loss': 0.0032, 'learning_rate': 7.17948717948718e-06, 'epoch': 9.62}\n",
            " 67% 1040/1560 [13:07<05:57,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:10:08,828 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:10:08,830 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:10:08,830 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:10:08,830 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.87it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.33it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.32it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.78it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.72it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.41it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.01it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.16it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.41it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.07it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1227664947509766, 'eval_accuracy': 0.6952789425849915, 'eval_f1': 0.6779825978626906, 'eval_runtime': 3.8513, 'eval_samples_per_second': 120.997, 'eval_steps_per_second': 15.319, 'epoch': 10.0}\n",
            " 67% 1040/1560 [13:11<05:57,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:10:12,682 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1040\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:10:12,683 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:10:13,815 >> Model weights saved in models/ZeroShot/0/checkpoint-1040/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:10:13,816 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1040/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:10:13,816 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1040/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:10:17,396 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-936] due to args.save_total_limit\n",
            " 73% 1144/1560 [14:28<04:48,  1.44it/s][INFO|trainer.py:662] 2022-07-26 06:11:29,950 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:11:29,953 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:11:29,953 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:11:29,953 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.75it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.30it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.31it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.92it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.75it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.17it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.15it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1506032943725586, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6823729594813932, 'eval_runtime': 3.8522, 'eval_samples_per_second': 120.968, 'eval_steps_per_second': 15.316, 'epoch': 11.0}\n",
            " 73% 1144/1560 [14:32<04:48,  1.44it/s]\n",
            "100% 59/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:11:33,808 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1144\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:11:33,809 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:11:35,086 >> Model weights saved in models/ZeroShot/0/checkpoint-1144/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:11:35,169 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1144/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:11:35,169 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1144/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:11:38,436 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1040] due to args.save_total_limit\n",
            " 80% 1248/1560 [15:49<03:33,  1.46it/s][INFO|trainer.py:662] 2022-07-26 06:12:51,016 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:12:51,018 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:12:51,018 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:12:51,018 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.05it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.08it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.10it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.63it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.65it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.29it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.42it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.05it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.06it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.08it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1296164989471436, 'eval_accuracy': 0.7103004455566406, 'eval_f1': 0.6846094842755945, 'eval_runtime': 3.8543, 'eval_samples_per_second': 120.904, 'eval_steps_per_second': 15.308, 'epoch': 12.0}\n",
            " 80% 1248/1560 [15:53<03:33,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:12:54,873 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1248\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:12:54,874 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:12:55,903 >> Model weights saved in models/ZeroShot/0/checkpoint-1248/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:12:55,903 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1248/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:12:55,903 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1248/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:12:59,522 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1144] due to args.save_total_limit\n",
            " 87% 1352/1560 [17:10<02:23,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:14:12,109 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:14:12,111 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:14:12,111 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:14:12,111 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.64it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.24it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.47it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.82it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.61it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.35it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.22it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.15it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1436686515808105, 'eval_accuracy': 0.7060086131095886, 'eval_f1': 0.6816098546243423, 'eval_runtime': 3.8471, 'eval_samples_per_second': 121.129, 'eval_steps_per_second': 15.336, 'epoch': 13.0}\n",
            " 87% 1352/1560 [17:14<02:23,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:14:15,960 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1352\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:14:15,961 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:14:17,010 >> Model weights saved in models/ZeroShot/0/checkpoint-1352/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:14:17,011 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1352/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:14:17,011 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1352/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:14:20,753 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1248] due to args.save_total_limit\n",
            " 93% 1456/1560 [18:31<01:11,  1.45it/s][INFO|trainer.py:662] 2022-07-26 06:15:33,262 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:15:33,264 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:15:33,264 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:15:33,264 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.73it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.22it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.21it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.86it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.60it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.45it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 14.95it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.08it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.06it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.27it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.156165599822998, 'eval_accuracy': 0.7038626670837402, 'eval_f1': 0.6796963720040643, 'eval_runtime': 3.8522, 'eval_samples_per_second': 120.968, 'eval_steps_per_second': 15.316, 'epoch': 14.0}\n",
            " 93% 1456/1560 [18:35<01:11,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:15:37,118 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1456\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:15:37,119 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:15:38,250 >> Model weights saved in models/ZeroShot/0/checkpoint-1456/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:15:38,251 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1456/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:15:38,251 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1456/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:15:41,748 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1352] due to args.save_total_limit\n",
            "{'loss': 0.0009, 'learning_rate': 7.692307692307694e-07, 'epoch': 14.42}\n",
            "100% 1560/1560 [19:52<00:00,  1.46it/s][INFO|trainer.py:662] 2022-07-26 06:16:54,326 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:16:54,328 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:16:54,328 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:16:54,328 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.32it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.29it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.35it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.82it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.70it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.32it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.54it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.13it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.16it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.30it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.16093111038208, 'eval_accuracy': 0.7038626670837402, 'eval_f1': 0.6805047694753578, 'eval_runtime': 3.8393, 'eval_samples_per_second': 121.376, 'eval_steps_per_second': 15.367, 'epoch': 15.0}\n",
            "100% 1560/1560 [19:56<00:00,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:16:58,169 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1560\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:16:58,169 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:16:59,202 >> Model weights saved in models/ZeroShot/0/checkpoint-1560/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:16:59,202 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:16:59,203 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1560/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:17:02,861 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1456] due to args.save_total_limit\n",
            "[INFO|trainer.py:1761] 2022-07-26 06:17:03,008 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1834] 2022-07-26 06:17:03,008 >> Loading best model from models/ZeroShot/0/checkpoint-312 (score: 0.7047235023041474).\n",
            "{'train_runtime': 1204.6575, 'train_samples_per_second': 41.427, 'train_steps_per_second': 1.295, 'train_loss': 0.07322433569187967, 'epoch': 15.0}\n",
            "100% 1560/1560 [20:04<00:00,  1.29it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:17:06,181 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:17:06,184 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:17:07,226 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:17:07,228 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:17:07,228 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.0732\n",
            "  train_runtime            = 0:20:04.65\n",
            "  train_samples            =       3327\n",
            "  train_samples_per_second =     41.427\n",
            "  train_steps_per_second   =      1.295\n",
            "07/26/2022 06:17:07 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:662] 2022-07-26 06:17:07,268 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:17:07,270 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:17:07,270 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:17:07,270 >>   Batch size = 8\n",
            "100% 59/59 [00:03<00:00, 16.32it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       15.0\n",
            "  eval_accuracy           =     0.7167\n",
            "  eval_f1                 =     0.7047\n",
            "  eval_loss               =     0.8425\n",
            "  eval_runtime            = 0:00:03.69\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    126.056\n",
            "  eval_steps_per_second   =      15.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrfwDiORPDyS",
        "outputId": "1160aadf-126a-4f07-e2d9-3605e8082a66"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibb2Uo0vPPc3"
      },
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/ZeroShot/0/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etL-Ic6bPmtA"
      },
      "source": [
        "## Bring back saved model here. \n",
        "#!mkdir -p /content/models/ZeroShot/0/\n",
        "# !cp -r /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/* /content/models/ZeroShot/0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7Irn_YvIni"
      },
      "source": [
        "# One Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 3 : Monolingual BERT for one-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "Np_fganSL6oi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQO751yzvVJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16620f2c-a4a3-4a05-ab28-233e67225ea7"
      },
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-uncased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 06:21:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 06:21:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Jul26_06-21-56_5a915ffe101d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 06:21:56 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "07/26/2022 06:21:56 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "07/26/2022 06:21:57 - WARNING - datasets.builder -   Using custom data configuration default-ca5a028a8281ff16\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-ca5a028a8281ff16/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11275.01it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1499.30it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ca5a028a8281ff16/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1055.83it/s]\n",
            "[INFO|hub.py:592] 2022-07-26 06:21:58,570 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpehwwjonk\n",
            "Downloading: 100% 570/570 [00:00<00:00, 535kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 06:21:59,474 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|hub.py:604] 2022-07-26 06:21:59,474 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:21:59,475 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:21:59,478 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 06:22:00,368 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkweemsg4\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 26.6kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 06:22:01,273 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|hub.py:604] 2022-07-26 06:22:01,273 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:22:02,165 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:22:02,166 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 06:22:03,965 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw6u1vzl7\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 265kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 06:22:05,737 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:604] 2022-07-26 06:22:05,737 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:592] 2022-07-26 06:22:06,638 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi0avlt_7\n",
            "Downloading: 100% 455k/455k [00:01<00:00, 423kB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 06:22:08,650 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|hub.py:604] 2022-07-26 06:22:08,650 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:22:11,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:22:11,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:22:11,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:22:11,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:22:11,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:22:12,228 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:22:12,229 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-26 06:22:13,177 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdz8udk68\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 71.4MB/s]\n",
            "[INFO|hub.py:596] 2022-07-26 06:22:19,374 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|hub.py:604] 2022-07-26 06:22:19,374 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:2107] 2022-07-26 06:22:19,374 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:2474] 2022-07-26 06:22:21,497 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2486] 2022-07-26 06:22:21,497 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  4.99ba/s]\n",
            "100% 1/1 [00:00<00:00, 10.18ba/s]\n",
            "07/26/2022 06:22:22 - INFO - __main__ -   Sample 550 of the training set: {'label': 0, 'sentence1': 'When in a current bull market it can be difficult to ask the question: when will the bull market end?', 'sentence2': 'bull market', 'input_ids': [101, 2043, 1999, 1037, 2783, 7087, 3006, 2009, 2064, 2022, 3697, 2000, 3198, 1996, 3160, 1024, 2043, 2097, 1996, 7087, 3006, 2203, 1029, 102, 7087, 3006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 06:22:22 - INFO - __main__ -   Sample 2331 of the training set: {'label': 1, 'sentence1': 'The Pain Killer consists of light and dark rums, pineapple and orange juices, cream of coconut and nutmeg — it’s blended, making it the ideal drink to sip on the beach at the lakefront resort.', 'sentence2': 'pain killer', 'input_ids': [101, 1996, 3255, 6359, 3774, 1997, 2422, 1998, 2601, 19379, 2015, 1010, 7222, 23804, 1998, 4589, 10869, 2015, 1010, 6949, 1997, 16027, 1998, 17490, 4168, 2290, 1517, 2009, 1521, 1055, 19803, 1010, 2437, 2009, 1996, 7812, 4392, 2000, 10668, 2006, 1996, 3509, 2012, 1996, 2697, 12792, 7001, 1012, 102, 3255, 6359, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 06:22:22 - INFO - __main__ -   Sample 3286 of the training set: {'label': 1, 'sentence1': 'Renovations are scheduled to begin Monday for the P4 parking lot in Colonial Williamsburg, located behind the Merchants Square bus stop on South Henry Street, which will reopen as paid parking.', 'sentence2': 'parking lot', 'input_ids': [101, 15576, 2024, 5115, 2000, 4088, 6928, 2005, 1996, 1052, 2549, 5581, 2843, 1999, 5336, 26366, 1010, 2284, 2369, 1996, 10310, 2675, 3902, 2644, 2006, 2148, 2888, 2395, 1010, 2029, 2097, 2128, 26915, 2004, 3825, 5581, 1012, 102, 5581, 2843, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:662] 2022-07-26 06:22:24,450 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-26 06:22:24,459 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-26 06:22:24,459 >>   Num examples = 3414\n",
            "[INFO|trainer.py:1518] 2022-07-26 06:22:24,459 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1519] 2022-07-26 06:22:24,459 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1520] 2022-07-26 06:22:24,459 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-26 06:22:24,459 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-26 06:22:24,459 >>   Total optimization steps = 963\n",
            " 11% 107/963 [01:05<08:06,  1.76it/s][INFO|trainer.py:662] 2022-07-26 06:23:29,725 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:23:29,727 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:23:29,727 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:23:29,727 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 25.33it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 19.65it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:02, 17.94it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:02, 17.60it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:02, 17.13it/s]\u001b[A\n",
            " 25% 15/59 [00:00<00:02, 16.82it/s]\u001b[A\n",
            " 29% 17/59 [00:00<00:02, 16.84it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 16.76it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 16.75it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 16.61it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 16.44it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:01, 16.44it/s]\u001b[A\n",
            " 49% 29/59 [00:01<00:01, 16.42it/s]\u001b[A\n",
            " 53% 31/59 [00:01<00:01, 16.27it/s]\u001b[A\n",
            " 56% 33/59 [00:01<00:01, 16.39it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 16.46it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 16.54it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 16.49it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 16.43it/s]\u001b[A\n",
            " 73% 43/59 [00:02<00:00, 16.38it/s]\u001b[A\n",
            " 76% 45/59 [00:02<00:00, 16.34it/s]\u001b[A\n",
            " 80% 47/59 [00:02<00:00, 16.30it/s]\u001b[A\n",
            " 83% 49/59 [00:02<00:00, 16.39it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 16.41it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 16.49it/s]\u001b[A\n",
            " 93% 55/59 [00:03<00:00, 16.47it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.600585401058197, 'eval_accuracy': 0.7467811107635498, 'eval_f1': 0.744707324320309, 'eval_runtime': 3.5365, 'eval_samples_per_second': 131.768, 'eval_steps_per_second': 16.683, 'epoch': 1.0}\n",
            " 11% 107/963 [01:08<08:06,  1.76it/s]\n",
            "100% 59/59 [00:03<00:00, 16.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:23:33,265 >> Saving model checkpoint to models/OneShot/1/checkpoint-107\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:23:33,266 >> Configuration saved in models/OneShot/1/checkpoint-107/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:23:34,477 >> Model weights saved in models/OneShot/1/checkpoint-107/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:23:34,477 >> tokenizer config file saved in models/OneShot/1/checkpoint-107/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:23:34,478 >> Special tokens file saved in models/OneShot/1/checkpoint-107/special_tokens_map.json\n",
            " 22% 214/963 [02:21<07:26,  1.68it/s][INFO|trainer.py:662] 2022-07-26 06:24:46,030 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:24:46,033 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:24:46,033 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:24:46,033 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.82it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 18.54it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:02, 17.40it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:02, 17.11it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 16.87it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 16.74it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 16.60it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 16.63it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 16.62it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 16.58it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 16.47it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 16.44it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:01, 16.36it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 16.32it/s]\u001b[A\n",
            " 54% 32/59 [00:01<00:01, 16.30it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 16.43it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 16.49it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 16.44it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 16.47it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 16.43it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 16.33it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 16.27it/s]\u001b[A\n",
            " 81% 48/59 [00:02<00:00, 16.28it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 16.40it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 16.43it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 16.43it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 16.47it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.6522393822669983, 'eval_accuracy': 0.7875536680221558, 'eval_f1': 0.7805932457304285, 'eval_runtime': 3.5604, 'eval_samples_per_second': 130.883, 'eval_steps_per_second': 16.571, 'epoch': 2.0}\n",
            " 22% 214/963 [02:25<07:26,  1.68it/s]\n",
            "100% 59/59 [00:03<00:00, 16.49it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:24:49,595 >> Saving model checkpoint to models/OneShot/1/checkpoint-214\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:24:49,596 >> Configuration saved in models/OneShot/1/checkpoint-214/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:24:50,691 >> Model weights saved in models/OneShot/1/checkpoint-214/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:24:50,692 >> tokenizer config file saved in models/OneShot/1/checkpoint-214/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:24:50,692 >> Special tokens file saved in models/OneShot/1/checkpoint-214/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:24:54,039 >> Deleting older checkpoint [models/OneShot/1/checkpoint-107] due to args.save_total_limit\n",
            " 33% 321/963 [03:40<06:33,  1.63it/s][INFO|trainer.py:662] 2022-07-26 06:26:04,810 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:26:04,812 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:26:04,812 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:26:04,812 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.21it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 18.05it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.83it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:02, 16.46it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.67it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.68it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.60it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.61it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.61it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.48it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.70it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.79it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.68it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.62it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.63it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.44it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.48it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.43it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.42it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.62it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.66it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.54it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.54it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7573878765106201, 'eval_accuracy': 0.7768240571022034, 'eval_f1': 0.7740100348796, 'eval_runtime': 3.7578, 'eval_samples_per_second': 124.009, 'eval_steps_per_second': 15.701, 'epoch': 3.0}\n",
            " 33% 321/963 [03:44<06:33,  1.63it/s]\n",
            "100% 59/59 [00:03<00:00, 15.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:26:08,571 >> Saving model checkpoint to models/OneShot/1/checkpoint-321\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:26:08,572 >> Configuration saved in models/OneShot/1/checkpoint-321/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:26:09,559 >> Model weights saved in models/OneShot/1/checkpoint-321/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:26:09,560 >> tokenizer config file saved in models/OneShot/1/checkpoint-321/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:26:09,560 >> Special tokens file saved in models/OneShot/1/checkpoint-321/special_tokens_map.json\n",
            " 44% 428/963 [05:01<05:30,  1.62it/s][INFO|trainer.py:662] 2022-07-26 06:27:25,567 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:27:25,570 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:27:25,570 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:27:25,570 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.48it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.23it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.48it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.96it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.55it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.37it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.18it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.36it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.18it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.20it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.34it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.03761625289917, 'eval_accuracy': 0.7618025541305542, 'eval_f1': 0.7561417155788134, 'eval_runtime': 3.8338, 'eval_samples_per_second': 121.551, 'eval_steps_per_second': 15.389, 'epoch': 4.0}\n",
            " 44% 428/963 [05:04<05:30,  1.62it/s]\n",
            "100% 59/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:27:29,405 >> Saving model checkpoint to models/OneShot/1/checkpoint-428\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:27:29,406 >> Configuration saved in models/OneShot/1/checkpoint-428/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:27:30,427 >> Model weights saved in models/OneShot/1/checkpoint-428/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:27:30,428 >> tokenizer config file saved in models/OneShot/1/checkpoint-428/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:27:30,428 >> Special tokens file saved in models/OneShot/1/checkpoint-428/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:27:33,957 >> Deleting older checkpoint [models/OneShot/1/checkpoint-321] due to args.save_total_limit\n",
            "{'loss': 0.1501, 'learning_rate': 9.615784008307374e-06, 'epoch': 4.67}\n",
            " 56% 535/963 [06:21<04:24,  1.62it/s][INFO|trainer.py:662] 2022-07-26 06:28:46,307 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:28:46,309 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:28:46,309 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:28:46,309 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.74it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.09it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.48it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.89it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.55it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.34it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.25it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.15it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9283922910690308, 'eval_accuracy': 0.8004291653633118, 'eval_f1': 0.7893217051437238, 'eval_runtime': 3.8507, 'eval_samples_per_second': 121.017, 'eval_steps_per_second': 15.322, 'epoch': 5.0}\n",
            " 56% 535/963 [06:25<04:24,  1.62it/s]\n",
            "100% 59/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:28:50,161 >> Saving model checkpoint to models/OneShot/1/checkpoint-535\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:28:50,162 >> Configuration saved in models/OneShot/1/checkpoint-535/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:28:51,180 >> Model weights saved in models/OneShot/1/checkpoint-535/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:28:51,180 >> tokenizer config file saved in models/OneShot/1/checkpoint-535/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:28:51,181 >> Special tokens file saved in models/OneShot/1/checkpoint-535/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:28:54,694 >> Deleting older checkpoint [models/OneShot/1/checkpoint-214] due to args.save_total_limit\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:28:54,728 >> Deleting older checkpoint [models/OneShot/1/checkpoint-428] due to args.save_total_limit\n",
            " 67% 642/963 [07:42<03:19,  1.61it/s][INFO|trainer.py:662] 2022-07-26 06:30:07,287 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:30:07,289 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:30:07,289 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:30:07,289 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.09it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.27it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.29it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.05it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.59it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.45it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.45it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.52it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.26it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.30it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0929992198944092, 'eval_accuracy': 0.7918455004692078, 'eval_f1': 0.7891176456866669, 'eval_runtime': 3.8406, 'eval_samples_per_second': 121.334, 'eval_steps_per_second': 15.362, 'epoch': 6.0}\n",
            " 67% 642/963 [07:46<03:19,  1.61it/s]\n",
            "100% 59/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:30:11,131 >> Saving model checkpoint to models/OneShot/1/checkpoint-642\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:30:11,132 >> Configuration saved in models/OneShot/1/checkpoint-642/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:30:12,137 >> Model weights saved in models/OneShot/1/checkpoint-642/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:30:12,138 >> tokenizer config file saved in models/OneShot/1/checkpoint-642/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:30:12,138 >> Special tokens file saved in models/OneShot/1/checkpoint-642/special_tokens_map.json\n",
            " 78% 749/963 [09:03<02:12,  1.62it/s][INFO|trainer.py:662] 2022-07-26 06:31:27,958 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:31:27,960 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:31:27,961 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:31:27,961 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 20.99it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.82it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.15it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.81it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.40it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.30it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.07it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.13it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.04it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0364301204681396, 'eval_accuracy': 0.8025751113891602, 'eval_f1': 0.795944788196097, 'eval_runtime': 3.8663, 'eval_samples_per_second': 120.528, 'eval_steps_per_second': 15.26, 'epoch': 7.0}\n",
            " 78% 749/963 [09:07<02:12,  1.62it/s]\n",
            "100% 59/59 [00:03<00:00, 15.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:31:31,828 >> Saving model checkpoint to models/OneShot/1/checkpoint-749\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:31:31,829 >> Configuration saved in models/OneShot/1/checkpoint-749/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:31:32,883 >> Model weights saved in models/OneShot/1/checkpoint-749/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:31:32,884 >> tokenizer config file saved in models/OneShot/1/checkpoint-749/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:31:32,884 >> Special tokens file saved in models/OneShot/1/checkpoint-749/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:31:36,416 >> Deleting older checkpoint [models/OneShot/1/checkpoint-535] due to args.save_total_limit\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:31:36,589 >> Deleting older checkpoint [models/OneShot/1/checkpoint-642] due to args.save_total_limit\n",
            " 89% 856/963 [10:24<01:06,  1.61it/s][INFO|trainer.py:662] 2022-07-26 06:32:48,921 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:32:48,923 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:32:48,923 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:32:48,923 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.65it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.91it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.22it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.83it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.42it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.37it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.57it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.01it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.17it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.24it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.20it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0655014514923096, 'eval_accuracy': 0.8025751113891602, 'eval_f1': 0.7968844754396605, 'eval_runtime': 3.8503, 'eval_samples_per_second': 121.029, 'eval_steps_per_second': 15.323, 'epoch': 8.0}\n",
            " 89% 856/963 [10:28<01:06,  1.61it/s]\n",
            "100% 59/59 [00:03<00:00, 15.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:32:52,775 >> Saving model checkpoint to models/OneShot/1/checkpoint-856\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:32:52,776 >> Configuration saved in models/OneShot/1/checkpoint-856/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:32:53,772 >> Model weights saved in models/OneShot/1/checkpoint-856/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:32:53,772 >> tokenizer config file saved in models/OneShot/1/checkpoint-856/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:32:53,772 >> Special tokens file saved in models/OneShot/1/checkpoint-856/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:32:57,389 >> Deleting older checkpoint [models/OneShot/1/checkpoint-749] due to args.save_total_limit\n",
            "100% 963/963 [11:45<00:00,  1.61it/s][INFO|trainer.py:662] 2022-07-26 06:34:09,823 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:34:09,825 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:34:09,825 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:34:09,825 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.61it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.01it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.30it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.69it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.21it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.06it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.42it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 14.90it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.87it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 14.91it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.94it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.05it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0649014711380005, 'eval_accuracy': 0.7961373329162598, 'eval_f1': 0.7880550568521842, 'eval_runtime': 3.8769, 'eval_samples_per_second': 120.198, 'eval_steps_per_second': 15.218, 'epoch': 9.0}\n",
            "100% 963/963 [11:49<00:00,  1.61it/s]\n",
            "100% 59/59 [00:03<00:00, 14.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 06:34:13,703 >> Saving model checkpoint to models/OneShot/1/checkpoint-963\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:34:13,704 >> Configuration saved in models/OneShot/1/checkpoint-963/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:34:14,762 >> Model weights saved in models/OneShot/1/checkpoint-963/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:34:14,762 >> tokenizer config file saved in models/OneShot/1/checkpoint-963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:34:14,763 >> Special tokens file saved in models/OneShot/1/checkpoint-963/special_tokens_map.json\n",
            "[INFO|trainer.py:1761] 2022-07-26 06:34:18,268 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1834] 2022-07-26 06:34:18,269 >> Loading best model from models/OneShot/1/checkpoint-856 (score: 0.7968844754396605).\n",
            "{'train_runtime': 714.4623, 'train_samples_per_second': 43.006, 'train_steps_per_second': 1.348, 'train_loss': 0.0805788629399158, 'epoch': 9.0}\n",
            "100% 963/963 [11:54<00:00,  1.35it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:34:19,172 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:34:19,173 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:34:20,478 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:34:20,478 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:34:20,478 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.0806\n",
            "  train_runtime            = 0:11:54.46\n",
            "  train_samples            =       3414\n",
            "  train_samples_per_second =     43.006\n",
            "  train_steps_per_second   =      1.348\n",
            "07/26/2022 06:34:20 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:662] 2022-07-26 06:34:20,520 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:34:20,521 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:34:20,521 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:34:20,521 >>   Batch size = 8\n",
            "100% 59/59 [00:03<00:00, 15.97it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.8026\n",
            "  eval_f1                 =     0.7969\n",
            "  eval_loss               =     1.0655\n",
            "  eval_runtime            = 0:00:03.77\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    123.424\n",
            "  eval_steps_per_second   =     15.627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 4 : Multilingual BERT for one-shot multilingual idiomaticity detection "
      ],
      "metadata": {
        "id": "LgoRNxSD2ygG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "id": "ob4gOl0VZtSz",
        "outputId": "8a851a8c-d01b-487c-ed81-f4ebae474944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 07:46:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 07:46:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Jul26_07-46-50_db65fb7f0caf,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 07:46:50 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "07/26/2022 07:46:50 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "07/26/2022 07:46:51 - WARNING - datasets.builder -   Using custom data configuration default-f445d9feed8e2338\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-f445d9feed8e2338/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10631.95it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1319.17it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f445d9feed8e2338/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 934.46it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 07:46:51,504 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 07:46:51,512 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 07:46:52,234 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 07:46:52,235 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:46:54,406 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:46:54,406 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:46:54,406 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:46:54,406 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 07:46:54,406 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 07:46:54,764 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 07:46:54,765 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2107] 2022-07-26 07:46:55,315 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2474] 2022-07-26 07:46:59,924 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2486] 2022-07-26 07:46:59,924 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 5/5 [00:00<00:00,  8.33ba/s]\n",
            "100% 1/1 [00:00<00:00, 11.01ba/s]\n",
            "07/26/2022 07:47:00 - INFO - __main__ -   Sample 1100 of the training set: {'label': 0, 'sentence1': 'Given how many hours of beauty sleep most kitties like to clock up, choosing the right place for them to lay their heads is critical.', 'sentence2': 'beauty sleep', 'input_ids': [101, 90491, 14796, 11299, 19573, 10108, 54883, 63658, 10992, 72812, 14197, 11850, 10114, 52843, 10741, 117, 11257, 90739, 10105, 13448, 11192, 10142, 11345, 10114, 47413, 10455, 42399, 10124, 24523, 119, 102, 54883, 63658, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 07:47:00 - INFO - __main__ -   Sample 516 of the training set: {'label': 0, 'sentence1': 'Yet Sergeant Mark Brady, who oversees major collision investigations for South Yorkshire Police, told the inquiry, “Had there been a hard shoulder, had Jason and Alexandru pulled on to the hard shoulder, my opinion is that Mr Szuba would have driven clean past them.”', 'sentence2': 'hard shoulder', 'input_ids': [101, 71547, 54118, 11997, 45982, 117, 10479, 10491, 20262, 10107, 11922, 94460, 87748, 10142, 11056, 27577, 18051, 117, 21937, 10105, 10106, 56914, 117, 100, 66434, 11155, 10590, 169, 19118, 78681, 117, 10374, 16796, 10111, 43816, 65884, 10135, 10114, 10105, 19118, 78681, 117, 15127, 32282, 10124, 10189, 12916, 156, 13078, 10537, 10894, 10529, 39803, 55911, 17781, 11345, 119, 100, 102, 19118, 78681, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 07:47:00 - INFO - __main__ -   Sample 2089 of the training set: {'label': 0, 'sentence1': 'While the family fun hub holds back from turning into a ghost town for Japanese Yokai (demons) to roam around this year, Downtown East is bringing its scary stories online with a series of episodes on Instagram Stories.', 'sentence2': 'ghost town', 'input_ids': [101, 14600, 10105, 11365, 41807, 65896, 28278, 12014, 10188, 48448, 10708, 169, 100766, 12221, 10142, 13847, 30665, 18511, 113, 30776, 10891, 114, 10114, 25470, 11008, 12166, 10531, 10924, 117, 68339, 11830, 10124, 45749, 10474, 187, 15983, 10157, 21158, 13893, 10169, 169, 11366, 10108, 23604, 10135, 83019, 25955, 119, 102, 100766, 12221, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:662] 2022-07-26 07:47:04,420 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-26 07:47:04,427 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-26 07:47:04,427 >>   Num examples = 4631\n",
            "[INFO|trainer.py:1518] 2022-07-26 07:47:04,427 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1519] 2022-07-26 07:47:04,427 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1520] 2022-07-26 07:47:04,427 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-26 07:47:04,427 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-26 07:47:04,427 >>   Total optimization steps = 1305\n",
            " 11% 145/1305 [01:34<11:55,  1.62it/s][INFO|trainer.py:662] 2022-07-26 07:48:38,570 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:48:38,572 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:48:38,572 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:48:38,572 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 22.84it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.72it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.96it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.58it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.31it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.14it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.72it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.78it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.76it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.64it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.69it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.78it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.79it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.80it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.77it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.69it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.69it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.72it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.64it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.56it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.53it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.55it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.50it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.45it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.52it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.57it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.71it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.71it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.70it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.77it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.72it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.57it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.56it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.56it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.58it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.53it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.48it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.49it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.50it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.47it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6746252775192261, 'eval_accuracy': 0.7334235310554504, 'eval_f1': 0.7333923015362865, 'eval_runtime': 5.5696, 'eval_samples_per_second': 132.686, 'eval_steps_per_second': 16.698, 'epoch': 1.0}\n",
            " 11% 145/1305 [01:39<11:55,  1.62it/s]\n",
            "100% 93/93 [00:05<00:00, 16.47it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:48:44,143 >> Saving model checkpoint to models/OneShot/1/checkpoint-145\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:48:44,144 >> Configuration saved in models/OneShot/1/checkpoint-145/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:48:46,722 >> Model weights saved in models/OneShot/1/checkpoint-145/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:48:46,723 >> tokenizer config file saved in models/OneShot/1/checkpoint-145/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:48:46,723 >> Special tokens file saved in models/OneShot/1/checkpoint-145/special_tokens_map.json\n",
            " 22% 290/1305 [03:23<10:24,  1.63it/s][INFO|trainer.py:662] 2022-07-26 07:50:28,085 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:50:28,087 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:50:28,087 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:50:28,087 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.40it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.09it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.12it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.57it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.28it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.08it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.65it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.70it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.62it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.72it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.80it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.73it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.73it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.74it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.67it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.65it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.77it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.79it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.77it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.77it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.78it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.72it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.73it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.67it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.63it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.68it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.78it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.76it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.81it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.77it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.77it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.76it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.66it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.57it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.52it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 16.49it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.52it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.51it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.71it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8740957975387573, 'eval_accuracy': 0.7794316411018372, 'eval_f1': 0.7793734900920735, 'eval_runtime': 5.5397, 'eval_samples_per_second': 133.402, 'eval_steps_per_second': 16.788, 'epoch': 2.0}\n",
            " 22% 290/1305 [03:29<10:24,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.84it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:50:33,628 >> Saving model checkpoint to models/OneShot/1/checkpoint-290\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:50:33,629 >> Configuration saved in models/OneShot/1/checkpoint-290/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:50:35,811 >> Model weights saved in models/OneShot/1/checkpoint-290/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:50:35,812 >> tokenizer config file saved in models/OneShot/1/checkpoint-290/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:50:35,812 >> Special tokens file saved in models/OneShot/1/checkpoint-290/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:50:41,601 >> Deleting older checkpoint [models/OneShot/1/checkpoint-145] due to args.save_total_limit\n",
            " 33% 435/1305 [05:13<08:55,  1.63it/s][INFO|trainer.py:662] 2022-07-26 07:52:17,680 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:52:17,683 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:52:17,683 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:52:17,684 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 22.62it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.61it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.74it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.37it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.11it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.97it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.60it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.70it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.64it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.60it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.73it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.79it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.72it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.76it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.73it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.72it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.70it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.66it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.64it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.72it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.83it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.83it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.82it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.76it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.64it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.60it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.59it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.57it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.52it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.54it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.54it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.49it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.50it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.54it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.62it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.72it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.75it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.67it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.72it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.58it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.60it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7724287509918213, 'eval_accuracy': 0.8349120616912842, 'eval_f1': 0.8342415684890203, 'eval_runtime': 5.5616, 'eval_samples_per_second': 132.874, 'eval_steps_per_second': 16.722, 'epoch': 3.0}\n",
            " 33% 435/1305 [05:18<08:55,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.60it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:52:23,247 >> Saving model checkpoint to models/OneShot/1/checkpoint-435\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:52:23,248 >> Configuration saved in models/OneShot/1/checkpoint-435/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:52:25,576 >> Model weights saved in models/OneShot/1/checkpoint-435/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:52:25,576 >> tokenizer config file saved in models/OneShot/1/checkpoint-435/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:52:25,576 >> Special tokens file saved in models/OneShot/1/checkpoint-435/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:52:31,236 >> Deleting older checkpoint [models/OneShot/1/checkpoint-290] due to args.save_total_limit\n",
            "{'loss': 0.1585, 'learning_rate': 1.2337164750957855e-05, 'epoch': 3.45}\n",
            " 44% 580/1305 [07:02<07:24,  1.63it/s][INFO|trainer.py:662] 2022-07-26 07:54:07,222 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:54:07,224 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:54:07,225 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:54:07,225 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 22.68it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.64it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.72it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.45it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.21it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.99it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.63it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.66it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.59it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.61it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.59it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.52it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.46it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.48it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.51it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.65it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.70it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.76it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.79it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.75it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.65it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.66it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.63it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.55it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.58it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.56it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.56it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.63it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.74it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.75it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.74it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.78it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.72it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.71it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.70it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.63it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.67it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.69it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.76it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.77it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.76it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.74it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8102343678474426, 'eval_accuracy': 0.8416779637336731, 'eval_f1': 0.8407635570043371, 'eval_runtime': 5.5561, 'eval_samples_per_second': 133.007, 'eval_steps_per_second': 16.738, 'epoch': 4.0}\n",
            " 44% 580/1305 [07:08<07:24,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:54:12,782 >> Saving model checkpoint to models/OneShot/1/checkpoint-580\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:54:12,784 >> Configuration saved in models/OneShot/1/checkpoint-580/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:54:15,120 >> Model weights saved in models/OneShot/1/checkpoint-580/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:54:15,121 >> tokenizer config file saved in models/OneShot/1/checkpoint-580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:54:15,121 >> Special tokens file saved in models/OneShot/1/checkpoint-580/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:54:20,816 >> Deleting older checkpoint [models/OneShot/1/checkpoint-435] due to args.save_total_limit\n",
            " 56% 725/1305 [08:52<05:56,  1.63it/s][INFO|trainer.py:662] 2022-07-26 07:55:56,758 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:55:56,760 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:55:56,760 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:55:56,760 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.26it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.89it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.95it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.55it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.24it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.03it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.62it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.71it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.79it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 16.84it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.80it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.63it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.65it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.62it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.62it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.63it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.57it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.60it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.73it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.71it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.73it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.74it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.75it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.74it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.74it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.78it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.74it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.74it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.76it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.73it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.69it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.69it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.65it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.60it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.58it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.68it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.76it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 16.76it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.76it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.75it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.72it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.0670360326766968, 'eval_accuracy': 0.8213802576065063, 'eval_f1': 0.8185058942605382, 'eval_runtime': 5.5439, 'eval_samples_per_second': 133.3, 'eval_steps_per_second': 16.775, 'epoch': 5.0}\n",
            " 56% 725/1305 [08:57<05:56,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.59it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:56:02,306 >> Saving model checkpoint to models/OneShot/1/checkpoint-725\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:56:02,307 >> Configuration saved in models/OneShot/1/checkpoint-725/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:56:04,736 >> Model weights saved in models/OneShot/1/checkpoint-725/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:56:04,737 >> tokenizer config file saved in models/OneShot/1/checkpoint-725/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:56:04,737 >> Special tokens file saved in models/OneShot/1/checkpoint-725/special_tokens_map.json\n",
            " 67% 870/1305 [10:42<04:26,  1.63it/s][INFO|trainer.py:662] 2022-07-26 07:57:46,472 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:57:46,485 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:57:46,485 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:57:46,485 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.33it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.18it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.04it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.55it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.28it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.06it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.61it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.71it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.71it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.73it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.72it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 16.81it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.83it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.81it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.70it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.71it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.69it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.65it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.51it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.50it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.49it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.53it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.61it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.76it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.80it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.83it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.76it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.69it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.64it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.63it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.56it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.57it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.53it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.49it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.44it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.47it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.48it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.47it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9478880167007446, 'eval_accuracy': 0.8376184105873108, 'eval_f1': 0.8375824175824176, 'eval_runtime': 5.5671, 'eval_samples_per_second': 132.743, 'eval_steps_per_second': 16.705, 'epoch': 6.0}\n",
            " 67% 870/1305 [10:47<04:26,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.46it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:57:52,072 >> Saving model checkpoint to models/OneShot/1/checkpoint-870\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:57:52,073 >> Configuration saved in models/OneShot/1/checkpoint-870/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:57:54,450 >> Model weights saved in models/OneShot/1/checkpoint-870/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:57:54,450 >> tokenizer config file saved in models/OneShot/1/checkpoint-870/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:57:54,450 >> Special tokens file saved in models/OneShot/1/checkpoint-870/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:58:00,171 >> Deleting older checkpoint [models/OneShot/1/checkpoint-725] due to args.save_total_limit\n",
            "{'loss': 0.0194, 'learning_rate': 4.674329501915709e-06, 'epoch': 6.9}\n",
            " 78% 1015/1305 [12:31<02:58,  1.63it/s][INFO|trainer.py:662] 2022-07-26 07:59:36,178 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 07:59:36,180 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 07:59:36,180 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 07:59:36,180 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 25.03it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.46it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 18.08it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:04, 17.68it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:04, 17.36it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:04, 16.95it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:04, 16.82it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:04, 16.81it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:04, 16.78it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:04, 16.68it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 16.59it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 16.60it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 16.44it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 16.39it/s]\u001b[A\n",
            " 38% 35/93 [00:02<00:03, 16.48it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:03, 16.56it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:03, 16.60it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 16.70it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 16.71it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 16.77it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 16.70it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 16.73it/s]\u001b[A\n",
            " 55% 51/93 [00:03<00:02, 16.72it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 16.66it/s]\u001b[A\n",
            " 59% 55/93 [00:03<00:02, 16.64it/s]\u001b[A\n",
            " 61% 57/93 [00:03<00:02, 16.56it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:02, 16.54it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 16.56it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 16.52it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 16.50it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 16.57it/s]\u001b[A\n",
            " 74% 69/93 [00:04<00:01, 16.75it/s]\u001b[A\n",
            " 76% 71/93 [00:04<00:01, 16.79it/s]\u001b[A\n",
            " 78% 73/93 [00:04<00:01, 16.77it/s]\u001b[A\n",
            " 81% 75/93 [00:04<00:01, 16.77it/s]\u001b[A\n",
            " 83% 77/93 [00:04<00:00, 16.67it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 16.65it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 16.57it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 16.53it/s]\u001b[A\n",
            " 91% 85/93 [00:05<00:00, 16.55it/s]\u001b[A\n",
            " 94% 87/93 [00:05<00:00, 16.53it/s]\u001b[A\n",
            " 96% 89/93 [00:05<00:00, 16.53it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.048614263534546, 'eval_accuracy': 0.8416779637336731, 'eval_f1': 0.8408901451371598, 'eval_runtime': 5.5575, 'eval_samples_per_second': 132.974, 'eval_steps_per_second': 16.734, 'epoch': 7.0}\n",
            " 78% 1015/1305 [12:37<02:58,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.54it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 07:59:41,739 >> Saving model checkpoint to models/OneShot/1/checkpoint-1015\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 07:59:41,740 >> Configuration saved in models/OneShot/1/checkpoint-1015/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 07:59:44,161 >> Model weights saved in models/OneShot/1/checkpoint-1015/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 07:59:44,162 >> tokenizer config file saved in models/OneShot/1/checkpoint-1015/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 07:59:44,162 >> Special tokens file saved in models/OneShot/1/checkpoint-1015/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:59:49,727 >> Deleting older checkpoint [models/OneShot/1/checkpoint-580] due to args.save_total_limit\n",
            "[INFO|trainer.py:2581] 2022-07-26 07:59:49,781 >> Deleting older checkpoint [models/OneShot/1/checkpoint-870] due to args.save_total_limit\n",
            " 89% 1160/1305 [14:21<01:28,  1.63it/s][INFO|trainer.py:662] 2022-07-26 08:01:25,563 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 08:01:25,565 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 08:01:25,565 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 08:01:25,565 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.18it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.76it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.80it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.46it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.16it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.96it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.66it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.70it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.75it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.77it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.87it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 16.89it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.82it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.81it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.70it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.68it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.65it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.65it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.60it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.52it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.51it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.46it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.46it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.50it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.46it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.49it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.53it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.63it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.72it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.77it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.77it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.70it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.71it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.67it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.67it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.62it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.58it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.59it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.73it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.77it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 16.79it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.75it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.69it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.65it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0324324369430542, 'eval_accuracy': 0.8389715552330017, 'eval_f1': 0.8383987063222985, 'eval_runtime': 5.5551, 'eval_samples_per_second': 133.032, 'eval_steps_per_second': 16.741, 'epoch': 8.0}\n",
            " 89% 1160/1305 [14:26<01:28,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.60it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 08:01:31,122 >> Saving model checkpoint to models/OneShot/1/checkpoint-1160\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 08:01:31,123 >> Configuration saved in models/OneShot/1/checkpoint-1160/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 08:01:33,565 >> Model weights saved in models/OneShot/1/checkpoint-1160/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 08:01:33,566 >> tokenizer config file saved in models/OneShot/1/checkpoint-1160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 08:01:33,566 >> Special tokens file saved in models/OneShot/1/checkpoint-1160/special_tokens_map.json\n",
            "100% 1305/1305 [16:10<00:00,  1.63it/s][INFO|trainer.py:662] 2022-07-26 08:03:14,795 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 08:03:14,797 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 08:03:14,798 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 08:03:14,798 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 22.99it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.76it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.84it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.51it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.16it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.97it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.63it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.69it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.70it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 16.77it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.93it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.86it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.69it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.69it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.66it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.64it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.56it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.58it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.55it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.54it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.67it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.70it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.76it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.79it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.74it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.75it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.57it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.53it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.54it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.49it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.44it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.50it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.50it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.49it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.39it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.40it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.40it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.36it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.44it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.039906620979309, 'eval_accuracy': 0.8389715552330017, 'eval_f1': 0.8382310469978165, 'eval_runtime': 5.5774, 'eval_samples_per_second': 132.498, 'eval_steps_per_second': 16.674, 'epoch': 9.0}\n",
            "100% 1305/1305 [16:15<00:00,  1.63it/s]\n",
            "100% 93/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2503] 2022-07-26 08:03:20,377 >> Saving model checkpoint to models/OneShot/1/checkpoint-1305\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 08:03:20,378 >> Configuration saved in models/OneShot/1/checkpoint-1305/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 08:03:22,817 >> Model weights saved in models/OneShot/1/checkpoint-1305/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 08:03:22,818 >> tokenizer config file saved in models/OneShot/1/checkpoint-1305/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 08:03:22,818 >> Special tokens file saved in models/OneShot/1/checkpoint-1305/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 08:03:28,515 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1160] due to args.save_total_limit\n",
            "[INFO|trainer.py:1761] 2022-07-26 08:03:28,723 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1834] 2022-07-26 08:03:28,723 >> Loading best model from models/OneShot/1/checkpoint-1015 (score: 0.8408901451371598).\n",
            "{'train_runtime': 987.7357, 'train_samples_per_second': 42.197, 'train_steps_per_second': 1.321, 'train_loss': 0.068887326315445, 'epoch': 9.0}\n",
            "100% 1305/1305 [16:27<00:00,  1.32it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 08:03:32,173 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 08:03:32,174 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 08:03:34,581 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 08:03:34,581 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 08:03:34,582 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.0689\n",
            "  train_runtime            = 0:16:27.73\n",
            "  train_samples            =       4631\n",
            "  train_samples_per_second =     42.197\n",
            "  train_steps_per_second   =      1.321\n",
            "07/26/2022 08:03:34 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:662] 2022-07-26 08:03:34,853 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 08:03:34,855 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 08:03:34,855 >>   Num examples = 739\n",
            "[INFO|trainer.py:2758] 2022-07-26 08:03:34,855 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 16.97it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.8417\n",
            "  eval_f1                 =     0.8409\n",
            "  eval_loss               =     1.0486\n",
            "  eval_runtime            = 0:00:05.54\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    133.343\n",
            "  eval_steps_per_second   =     16.781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 04 : Monolingual adapter-based BERT for one-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "AzC-WNAbREEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adapter-transformers"
      ],
      "metadata": {
        "id": "8XpLBCTUSuup",
        "outputId": "b5c4cd7e-85a1-4060-c5c0-edda3f48e585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-3.0.1-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->adapter-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7f013bb2fbe2bf71bf76d6812eb60064f0bbbe7b9e122426451f070874801893\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, adapter-transformers\n",
            "Successfully installed adapter-transformers-3.0.1 sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py \\\n",
        "  --model_name_or_path 'bert-base-uncased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 10.0 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/OneShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/OneShot/train.csv \\\n",
        "  --validation_file Data/OneShot/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "CX4cERiTRCjn",
        "outputId": "c90ae3ac-2a3b-4bfd-de11-8bb16a843311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 07:00:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 07:00:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Jul26_07-00-42_b9344d9022a6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 07:00:42 - INFO - __main__ - load a local file for train: Data/OneShot/train.csv\n",
            "07/26/2022 07:00:42 - INFO - __main__ - load a local file for validation: Data/OneShot/dev.csv\n",
            "07/26/2022 07:00:42 - WARNING - datasets.builder - Using custom data configuration default-c230a41b0da25f23\n",
            "07/26/2022 07:00:42 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-c230a41b0da25f23/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c230a41b0da25f23/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11715.93it/s]\n",
            "07/26/2022 07:00:42 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/26/2022 07:00:42 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1345.84it/s]\n",
            "07/26/2022 07:00:42 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/26/2022 07:00:42 - INFO - datasets.builder - Generating train split\n",
            "07/26/2022 07:00:43 - INFO - datasets.builder - Generating validation split\n",
            "07/26/2022 07:00:43 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c230a41b0da25f23/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1036.78it/s]\n",
            "[INFO|file_utils.py:2215] 2022-07-26 07:00:43,305 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_jh1zmag\n",
            "Downloading: 100% 570/570 [00:00<00:00, 680kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 07:00:43,578 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|file_utils.py:2227] 2022-07-26 07:00:43,579 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 07:00:43,579 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 07:00:43,580 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-07-26 07:00:43,859 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0l_gg922\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 37.9kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 07:00:44,129 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:2227] 2022-07-26 07:00:44,129 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 07:00:44,396 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 07:00:44,397 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-07-26 07:00:44,935 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4754oer0\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 926kB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 07:00:45,459 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:2227] 2022-07-26 07:00:45,459 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:2215] 2022-07-26 07:00:45,727 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2umxhwld\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 1.87MB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 07:00:46,313 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:2227] 2022-07-26 07:00:46,314 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 07:00:47,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 07:00:47,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 07:00:47,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 07:00:47,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 07:00:47,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 07:00:47,390 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 07:00:47,390 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2215] 2022-07-26 07:00:47,705 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0t_1vqxs\n",
            "Downloading: 100% 420M/420M [00:09<00:00, 46.6MB/s]\n",
            "[INFO|file_utils.py:2219] 2022-07-26 07:00:57,247 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|file_utils.py:2227] 2022-07-26 07:00:57,247 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:1431] 2022-07-26 07:00:57,248 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1694] 2022-07-26 07:00:59,167 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1711] 2022-07-26 07:00:59,167 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:616] 2022-07-26 07:00:59,177 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter Config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:617] 2022-07-26 07:00:59,178 >> Adding adapter 'glue'.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]07/26/2022 07:00:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c230a41b0da25f23/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-a49f20fb9ff76def.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  8.53ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/26/2022 07:01:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c230a41b0da25f23/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-c718370484784ce7.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 16.21ba/s]\n",
            "07/26/2022 07:01:00 - INFO - __main__ - Sample 550 of the training set: {'label': 0, 'sentence1': 'When in a current bull market it can be difficult to ask the question: when will the bull market end?', 'sentence2': 'bull market', 'input_ids': [101, 2043, 1999, 1037, 2783, 7087, 3006, 2009, 2064, 2022, 3697, 2000, 3198, 1996, 3160, 1024, 2043, 2097, 1996, 7087, 3006, 2203, 1029, 102, 7087, 3006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 07:01:00 - INFO - __main__ - Sample 2331 of the training set: {'label': 1, 'sentence1': 'The Pain Killer consists of light and dark rums, pineapple and orange juices, cream of coconut and nutmeg — it’s blended, making it the ideal drink to sip on the beach at the lakefront resort.', 'sentence2': 'pain killer', 'input_ids': [101, 1996, 3255, 6359, 3774, 1997, 2422, 1998, 2601, 19379, 2015, 1010, 7222, 23804, 1998, 4589, 10869, 2015, 1010, 6949, 1997, 16027, 1998, 17490, 4168, 2290, 1517, 2009, 1521, 1055, 19803, 1010, 2437, 2009, 1996, 7812, 4392, 2000, 10668, 2006, 1996, 3509, 2012, 1996, 2697, 12792, 7001, 1012, 102, 3255, 6359, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 07:01:00 - INFO - __main__ - Sample 3286 of the training set: {'label': 1, 'sentence1': 'Renovations are scheduled to begin Monday for the P4 parking lot in Colonial Williamsburg, located behind the Merchants Square bus stop on South Henry Street, which will reopen as paid parking.', 'sentence2': 'parking lot', 'input_ids': [101, 15576, 2024, 5115, 2000, 4088, 6928, 2005, 1996, 1052, 2549, 5581, 2843, 1999, 5336, 26366, 1010, 2284, 2369, 1996, 10310, 2675, 3902, 2644, 2006, 2148, 2888, 2395, 1010, 2029, 2097, 2128, 26915, 2004, 3825, 5581, 1012, 102, 5581, 2843, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:230] 2022-07-26 07:01:06,790 >> The following columns in the training set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1254] 2022-07-26 07:01:06,801 >> ***** Running training *****\n",
            "[INFO|trainer.py:1255] 2022-07-26 07:01:06,801 >>   Num examples = 3414\n",
            "[INFO|trainer.py:1256] 2022-07-26 07:01:06,802 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1257] 2022-07-26 07:01:06,802 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1258] 2022-07-26 07:01:06,802 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1259] 2022-07-26 07:01:06,802 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1260] 2022-07-26 07:01:06,802 >>   Total optimization steps = 1070\n",
            " 10% 107/1070 [00:54<07:16,  2.21it/s][INFO|trainer.py:230] 2022-07-26 07:02:01,251 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:02:01,253 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:02:01,253 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:02:01,253 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.06it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.88it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 15.74it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.33it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.09it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:03, 14.76it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 14.70it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 14.68it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 14.59it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 14.49it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 14.45it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 14.41it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.33it/s]\u001b[A\n",
            " 51% 30/59 [00:02<00:02, 14.33it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 14.36it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 14.41it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 14.38it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 14.40it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 14.41it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.34it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.25it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 14.26it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 14.28it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 14.26it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 14.23it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 14.18it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 14.13it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7185636162757874, 'eval_accuracy': 0.733905553817749, 'eval_f1': 0.7319304496279389, 'eval_runtime': 4.0683, 'eval_samples_per_second': 114.544, 'eval_steps_per_second': 14.502, 'epoch': 1.0}\n",
            " 10% 107/1070 [00:58<07:16,  2.21it/s]\n",
            "100% 59/59 [00:04<00:00, 14.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:02:05,323 >> Saving model checkpoint to models/OneShot/1/checkpoint-107\n",
            "[INFO|loading.py:60] 2022-07-26 07:02:05,323 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:02:05,424 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:02:05,425 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:02:05,431 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:02:05,432 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:02:05,438 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:02:05,439 >> tokenizer config file saved in models/OneShot/1/checkpoint-107/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:02:05,439 >> Special tokens file saved in models/OneShot/1/checkpoint-107/special_tokens_map.json\n",
            " 20% 214/1070 [01:54<07:08,  2.00it/s][INFO|trainer.py:230] 2022-07-26 07:03:01,390 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:03:01,392 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:03:01,393 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:03:01,393 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.95it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.15it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.08it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.85it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.15it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.06it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.21it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.09it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.82it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.93it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.85it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.71it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.81it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.92it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.73it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.73it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.75it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.78it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.82it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.84it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.87it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.79it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.75it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.78it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.85it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.86it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.92it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.79it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7090798616409302, 'eval_accuracy': 0.7618025541305542, 'eval_f1': 0.7558101658428813, 'eval_runtime': 4.548, 'eval_samples_per_second': 102.462, 'eval_steps_per_second': 12.973, 'epoch': 2.0}\n",
            " 20% 214/1070 [01:59<07:08,  2.00it/s]\n",
            "100% 59/59 [00:04<00:00, 14.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:03:05,942 >> Saving model checkpoint to models/OneShot/1/checkpoint-214\n",
            "[INFO|loading.py:60] 2022-07-26 07:03:05,942 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:03:06,029 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:03:06,030 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:03:06,036 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:03:06,036 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:03:06,042 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:03:06,043 >> tokenizer config file saved in models/OneShot/1/checkpoint-214/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:03:06,043 >> Special tokens file saved in models/OneShot/1/checkpoint-214/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:03:06,302 >> Deleting older checkpoint [models/OneShot/1/checkpoint-107] due to args.save_total_limit\n",
            " 30% 321/1070 [02:58<06:12,  2.01it/s][INFO|trainer.py:230] 2022-07-26 07:04:04,910 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:04:04,911 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:04:04,911 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:04:04,911 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 20.15it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 14.83it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 14.21it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 13.78it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 13.25it/s]\u001b[A\n",
            " 24% 14/59 [00:01<00:03, 13.34it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:03, 13.32it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:03, 13.24it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 13.09it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 13.10it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 13.01it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 12.88it/s]\u001b[A\n",
            " 47% 28/59 [00:02<00:02, 13.01it/s]\u001b[A\n",
            " 51% 30/59 [00:02<00:02, 13.04it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:02, 13.06it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 13.07it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 12.91it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 12.74it/s]\u001b[A\n",
            " 68% 40/59 [00:03<00:01, 12.89it/s]\u001b[A\n",
            " 71% 42/59 [00:03<00:01, 12.93it/s]\u001b[A\n",
            " 75% 44/59 [00:03<00:01, 13.02it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 13.01it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 13.04it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 12.98it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 12.96it/s]\u001b[A\n",
            " 92% 54/59 [00:04<00:00, 13.05it/s]\u001b[A\n",
            " 95% 56/59 [00:04<00:00, 12.96it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7598928809165955, 'eval_accuracy': 0.7725321650505066, 'eval_f1': 0.769436146377894, 'eval_runtime': 4.4832, 'eval_samples_per_second': 103.945, 'eval_steps_per_second': 13.16, 'epoch': 3.0}\n",
            " 30% 321/1070 [03:02<06:12,  2.01it/s]\n",
            "100% 59/59 [00:04<00:00, 13.08it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:04:09,396 >> Saving model checkpoint to models/OneShot/1/checkpoint-321\n",
            "[INFO|loading.py:60] 2022-07-26 07:04:09,396 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:04:09,480 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:04:09,480 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:04:09,487 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:04:09,488 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:04:09,495 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:04:09,495 >> tokenizer config file saved in models/OneShot/1/checkpoint-321/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:04:09,495 >> Special tokens file saved in models/OneShot/1/checkpoint-321/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:04:09,782 >> Deleting older checkpoint [models/OneShot/1/checkpoint-214] due to args.save_total_limit\n",
            " 40% 428/1070 [04:01<05:23,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:05:08,000 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:05:08,001 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:05:08,001 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:05:08,001 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.60it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.09it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.01it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.77it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.13it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.00it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.13it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.95it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.85it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.92it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.89it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.81it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.83it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.98it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.88it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.87it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.87it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.73it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.79it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.91it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.89it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.72it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.66it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.75it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.79it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.77it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.88it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.92it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2408242225646973, 'eval_accuracy': 0.7553648352622986, 'eval_f1': 0.7549992621015349, 'eval_runtime': 4.5469, 'eval_samples_per_second': 102.487, 'eval_steps_per_second': 12.976, 'epoch': 4.0}\n",
            " 40% 428/1070 [04:05<05:23,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 14.26it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:05:12,550 >> Saving model checkpoint to models/OneShot/1/checkpoint-428\n",
            "[INFO|loading.py:60] 2022-07-26 07:05:12,550 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:05:12,633 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:05:12,634 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:05:12,640 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:05:12,640 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:05:12,646 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:05:12,646 >> tokenizer config file saved in models/OneShot/1/checkpoint-428/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:05:12,646 >> Special tokens file saved in models/OneShot/1/checkpoint-428/special_tokens_map.json\n",
            "{'loss': 0.1408, 'learning_rate': 5.327102803738318e-05, 'epoch': 4.67}\n",
            " 50% 535/1070 [05:04<04:29,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:06:11,303 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:06:11,305 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:06:11,305 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:06:11,305 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.52it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.44it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.21it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.81it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.43it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.19it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.27it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.10it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.99it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.97it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.91it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.83it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.91it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.97it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.87it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.77it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.83it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.75it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.75it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.84it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.90it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.87it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.97it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.84it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.81it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.91it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.92it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.93it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9071073532104492, 'eval_accuracy': 0.8090128898620605, 'eval_f1': 0.8065100048052923, 'eval_runtime': 4.5201, 'eval_samples_per_second': 103.096, 'eval_steps_per_second': 13.053, 'epoch': 5.0}\n",
            " 50% 535/1070 [05:09<04:29,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 14.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:06:15,826 >> Saving model checkpoint to models/OneShot/1/checkpoint-535\n",
            "[INFO|loading.py:60] 2022-07-26 07:06:15,827 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:06:15,910 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:06:15,911 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:06:15,917 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:06:15,917 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:06:15,924 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:06:15,925 >> tokenizer config file saved in models/OneShot/1/checkpoint-535/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:06:15,925 >> Special tokens file saved in models/OneShot/1/checkpoint-535/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:06:16,215 >> Deleting older checkpoint [models/OneShot/1/checkpoint-321] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:06:16,234 >> Deleting older checkpoint [models/OneShot/1/checkpoint-428] due to args.save_total_limit\n",
            " 60% 642/1070 [06:07<03:34,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:07:14,607 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:07:14,609 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:07:14,609 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:07:14,609 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.99it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.28it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.09it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.84it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.26it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.05it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.19it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.11it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.92it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.99it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.92it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.75it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.77it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.92it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.83it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.93it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.90it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.73it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.71it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.87it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.84it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.83it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.95it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.81it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.69it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.67it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.84it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.81it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.19125235080719, 'eval_accuracy': 0.770386278629303, 'eval_f1': 0.766111442067293, 'eval_runtime': 4.5406, 'eval_samples_per_second': 102.629, 'eval_steps_per_second': 12.994, 'epoch': 6.0}\n",
            " 60% 642/1070 [06:12<03:34,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 14.25it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:07:19,151 >> Saving model checkpoint to models/OneShot/1/checkpoint-642\n",
            "[INFO|loading.py:60] 2022-07-26 07:07:19,152 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:07:19,235 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:07:19,235 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:07:19,242 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:07:19,242 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:07:19,250 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:07:19,250 >> tokenizer config file saved in models/OneShot/1/checkpoint-642/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:07:19,250 >> Special tokens file saved in models/OneShot/1/checkpoint-642/special_tokens_map.json\n",
            " 70% 749/1070 [07:11<02:40,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:08:17,847 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:08:17,848 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:08:17,848 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:08:17,848 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 20.00it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 14.69it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 13.93it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 13.61it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 13.14it/s]\u001b[A\n",
            " 24% 14/59 [00:01<00:03, 13.17it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:03, 13.22it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:03, 13.04it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:03, 12.90it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 12.94it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 12.81it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 12.84it/s]\u001b[A\n",
            " 47% 28/59 [00:02<00:02, 12.94it/s]\u001b[A\n",
            " 51% 30/59 [00:02<00:02, 12.88it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:02, 12.80it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 12.88it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 12.79it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 12.80it/s]\u001b[A\n",
            " 68% 40/59 [00:03<00:01, 12.93it/s]\u001b[A\n",
            " 71% 42/59 [00:03<00:01, 12.89it/s]\u001b[A\n",
            " 75% 44/59 [00:03<00:01, 12.85it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:01, 12.90it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 12.87it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 12.78it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 12.91it/s]\u001b[A\n",
            " 92% 54/59 [00:04<00:00, 12.92it/s]\u001b[A\n",
            " 95% 56/59 [00:04<00:00, 12.94it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.1297807693481445, 'eval_accuracy': 0.8197425007820129, 'eval_f1': 0.8136887196573062, 'eval_runtime': 4.5292, 'eval_samples_per_second': 102.888, 'eval_steps_per_second': 13.027, 'epoch': 7.0}\n",
            " 70% 749/1070 [07:15<02:40,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 12.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:08:22,379 >> Saving model checkpoint to models/OneShot/1/checkpoint-749\n",
            "[INFO|loading.py:60] 2022-07-26 07:08:22,380 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:08:22,463 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:08:22,463 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:08:22,469 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:08:22,469 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:08:22,475 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:08:22,476 >> tokenizer config file saved in models/OneShot/1/checkpoint-749/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:08:22,476 >> Special tokens file saved in models/OneShot/1/checkpoint-749/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:08:22,758 >> Deleting older checkpoint [models/OneShot/1/checkpoint-535] due to args.save_total_limit\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:08:22,778 >> Deleting older checkpoint [models/OneShot/1/checkpoint-642] due to args.save_total_limit\n",
            " 80% 856/1070 [08:14<01:47,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:09:21,134 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:09:21,136 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:09:21,136 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:09:21,136 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.35it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.23it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.20it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.92it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.24it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.08it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.16it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.12it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 13.01it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.03it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.88it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.81it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.81it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.93it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.91it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.97it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.92it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.79it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.87it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.91it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.85it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.92it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.90it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.70it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.62it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.82it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.84it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.1725698709487915, 'eval_accuracy': 0.8133047223091125, 'eval_f1': 0.805903052064632, 'eval_runtime': 4.5272, 'eval_samples_per_second': 102.932, 'eval_steps_per_second': 13.032, 'epoch': 8.0}\n",
            " 80% 856/1070 [08:18<01:47,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 12.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:09:25,665 >> Saving model checkpoint to models/OneShot/1/checkpoint-856\n",
            "[INFO|loading.py:60] 2022-07-26 07:09:25,665 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:09:25,749 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:09:25,750 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:09:25,756 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:09:25,756 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:09:25,763 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:09:25,764 >> tokenizer config file saved in models/OneShot/1/checkpoint-856/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:09:25,764 >> Special tokens file saved in models/OneShot/1/checkpoint-856/special_tokens_map.json\n",
            " 90% 963/1070 [09:17<00:53,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:10:24,384 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:10:24,386 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:10:24,386 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:10:24,386 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 20.02it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 14.74it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 14.09it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 13.78it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 13.11it/s]\u001b[A\n",
            " 24% 14/59 [00:01<00:03, 13.14it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:03, 13.19it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:03, 12.97it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:03, 12.83it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 12.81it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 12.73it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 12.78it/s]\u001b[A\n",
            " 47% 28/59 [00:02<00:02, 12.79it/s]\u001b[A\n",
            " 51% 30/59 [00:02<00:02, 12.87it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:02, 12.87it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 12.76it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 12.82it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 12.82it/s]\u001b[A\n",
            " 68% 40/59 [00:03<00:01, 12.87it/s]\u001b[A\n",
            " 71% 42/59 [00:03<00:01, 12.89it/s]\u001b[A\n",
            " 75% 44/59 [00:03<00:01, 12.88it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:01, 12.92it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 12.89it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 12.80it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 12.90it/s]\u001b[A\n",
            " 92% 54/59 [00:04<00:00, 12.88it/s]\u001b[A\n",
            " 95% 56/59 [00:04<00:00, 12.93it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.320465326309204, 'eval_accuracy': 0.8025751113891602, 'eval_f1': 0.7987796864732939, 'eval_runtime': 4.53, 'eval_samples_per_second': 102.87, 'eval_steps_per_second': 13.024, 'epoch': 9.0}\n",
            " 90% 963/1070 [09:22<00:53,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 12.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:10:28,918 >> Saving model checkpoint to models/OneShot/1/checkpoint-963\n",
            "[INFO|loading.py:60] 2022-07-26 07:10:28,918 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:10:29,002 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:10:29,002 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:10:29,008 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:10:29,009 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:10:29,015 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:10:29,015 >> tokenizer config file saved in models/OneShot/1/checkpoint-963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:10:29,015 >> Special tokens file saved in models/OneShot/1/checkpoint-963/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:10:29,314 >> Deleting older checkpoint [models/OneShot/1/checkpoint-856] due to args.save_total_limit\n",
            "{'loss': 0.0073, 'learning_rate': 6.542056074766355e-06, 'epoch': 9.35}\n",
            "100% 1070/1070 [10:20<00:00,  1.99it/s][INFO|trainer.py:230] 2022-07-26 07:11:27,638 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:11:27,640 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:11:27,640 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:11:27,640 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.44it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.49it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.18it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.71it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.22it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.13it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.17it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.18it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 13.10it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.09it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.85it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.76it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.81it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.90it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.93it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.97it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.90it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.74it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.68it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.73it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.82it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.85it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.92it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.88it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.76it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.74it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.87it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.80it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.215505599975586, 'eval_accuracy': 0.8197425007820129, 'eval_f1': 0.8130766145206578, 'eval_runtime': 4.5339, 'eval_samples_per_second': 102.781, 'eval_steps_per_second': 13.013, 'epoch': 10.0}\n",
            "100% 1070/1070 [10:25<00:00,  1.99it/s]\n",
            "100% 59/59 [00:04<00:00, 14.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 07:11:32,175 >> Saving model checkpoint to models/OneShot/1/checkpoint-1070\n",
            "[INFO|loading.py:60] 2022-07-26 07:11:32,176 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:11:32,268 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:11:32,268 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:11:32,275 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:11:32,275 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:11:32,282 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:11:32,282 >> tokenizer config file saved in models/OneShot/1/checkpoint-1070/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:11:32,282 >> Special tokens file saved in models/OneShot/1/checkpoint-1070/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 07:11:32,578 >> Deleting older checkpoint [models/OneShot/1/checkpoint-963] due to args.save_total_limit\n",
            "[INFO|trainer.py:1483] 2022-07-26 07:11:32,598 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1492] 2022-07-26 07:11:32,598 >> Loading best model from models/OneShot/1/checkpoint-749 (score: 0.8136887196573062).\n",
            "[WARNING|trainer.py:1515] 2022-07-26 07:11:32,598 >> Could not locate the best model at models/OneShot/1/checkpoint-749/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 625.7964, 'train_samples_per_second': 54.554, 'train_steps_per_second': 1.71, 'train_loss': 0.06928209107125474, 'epoch': 10.0}\n",
            "100% 1070/1070 [10:25<00:00,  1.99it/s][INFO|trainer.py:260] 2022-07-26 07:11:32,600 >> Loading best adapter(s) from models/OneShot/1/checkpoint-749 (score: 0.8136887196573062).\n",
            "[INFO|loading.py:77] 2022-07-26 07:11:32,600 >> Loading module configuration from models/OneShot/1/checkpoint-749/glue/adapter_config.json\n",
            "[WARNING|loading.py:448] 2022-07-26 07:11:32,600 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-07-26 07:11:32,635 >> Loading module weights from models/OneShot/1/checkpoint-749/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-07-26 07:11:32,657 >> Loading module configuration from models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[WARNING|loading.py:726] 2022-07-26 07:11:32,657 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:616] 2022-07-26 07:11:32,666 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-07-26 07:11:32,668 >> Loading module weights from models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "100% 1070/1070 [10:25<00:00,  1.71it/s]\n",
            "[INFO|trainer.py:136] 2022-07-26 07:11:32,671 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-07-26 07:11:32,672 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:11:32,802 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:11:32,803 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:11:32,812 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 07:11:32,813 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 07:11:32,822 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 07:11:32,822 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 07:11:32,822 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.0693\n",
            "  train_runtime            = 0:10:25.79\n",
            "  train_samples            =       3414\n",
            "  train_samples_per_second =     54.554\n",
            "  train_steps_per_second   =       1.71\n",
            "07/26/2022 07:11:32 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:230] 2022-07-26 07:11:32,862 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2394] 2022-07-26 07:11:32,864 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 07:11:32,865 >>   Num examples = 466\n",
            "[INFO|trainer.py:2399] 2022-07-26 07:11:32,865 >>   Batch size = 8\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 676, in <module>\n",
            "    main()\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 614, in main\n",
            "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2269, in evaluate\n",
            "    metric_key_prefix=metric_key_prefix,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2436, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2646, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1991, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/models/bert.py\", line 85, in forward\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 745, in forward_head\n",
            "    return head_module(all_outputs, cls_output, attention_mask, return_dict, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 125, in forward\n",
            "    logits = super().forward(cls_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 05 : Multilingual adapter-based BERT for one-shot multilingual idiomaticity detection"
      ],
      "metadata": {
        "id": "Ghm5ew_G3RtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 10.0 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/OneShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/OneShot/train.csv \\\n",
        "  --validation_file Data/OneShot/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "6tnrwbV2jPCu",
        "outputId": "913cec55-2852-4d5f-84e5-bdabdb1f9dbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 08:23:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 08:23:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Jul26_08-23-54_eaa0ebcd8e09,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 08:23:54 - INFO - __main__ - load a local file for train: Data/OneShot/train.csv\n",
            "07/26/2022 08:23:54 - INFO - __main__ - load a local file for validation: Data/OneShot/dev.csv\n",
            "07/26/2022 08:23:55 - WARNING - datasets.builder - Using custom data configuration default-f5f851abf7a41658\n",
            "07/26/2022 08:23:55 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-f5f851abf7a41658/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-f5f851abf7a41658/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11570.49it/s]\n",
            "07/26/2022 08:23:55 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/26/2022 08:23:55 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1378.35it/s]\n",
            "07/26/2022 08:23:55 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/26/2022 08:23:55 - INFO - datasets.builder - Generating train split\n",
            "07/26/2022 08:23:55 - INFO - datasets.builder - Generating validation split\n",
            "07/26/2022 08:23:55 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f5f851abf7a41658/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1050.15it/s]\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 08:23:56,864 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 08:23:56,865 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 08:23:58,643 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 08:23:58,643 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:24:04,017 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:24:04,017 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:24:04,017 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:24:04,017 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-07-26 08:24:04,017 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:648] 2022-07-26 08:24:04,904 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:684] 2022-07-26 08:24:04,904 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1431] 2022-07-26 08:24:05,914 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:1694] 2022-07-26 08:24:07,939 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1711] 2022-07-26 08:24:07,939 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:616] 2022-07-26 08:24:07,949 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter Config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:617] 2022-07-26 08:24:07,950 >> Adding adapter 'glue'.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]07/26/2022 08:24:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f5f851abf7a41658/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-4bcc4701a0e26f0d.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:00<00:00,  9.01ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/26/2022 08:24:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f5f851abf7a41658/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-2263fe4433e7cafe.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 11.89ba/s]\n",
            "07/26/2022 08:24:09 - INFO - __main__ - Sample 1100 of the training set: {'label': 0, 'sentence1': 'Given how many hours of beauty sleep most kitties like to clock up, choosing the right place for them to lay their heads is critical.', 'sentence2': 'beauty sleep', 'input_ids': [101, 90491, 14796, 11299, 19573, 10108, 54883, 63658, 10992, 72812, 14197, 11850, 10114, 52843, 10741, 117, 11257, 90739, 10105, 13448, 11192, 10142, 11345, 10114, 47413, 10455, 42399, 10124, 24523, 119, 102, 54883, 63658, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 08:24:09 - INFO - __main__ - Sample 516 of the training set: {'label': 0, 'sentence1': 'Yet Sergeant Mark Brady, who oversees major collision investigations for South Yorkshire Police, told the inquiry, “Had there been a hard shoulder, had Jason and Alexandru pulled on to the hard shoulder, my opinion is that Mr Szuba would have driven clean past them.”', 'sentence2': 'hard shoulder', 'input_ids': [101, 71547, 54118, 11997, 45982, 117, 10479, 10491, 20262, 10107, 11922, 94460, 87748, 10142, 11056, 27577, 18051, 117, 21937, 10105, 10106, 56914, 117, 100, 66434, 11155, 10590, 169, 19118, 78681, 117, 10374, 16796, 10111, 43816, 65884, 10135, 10114, 10105, 19118, 78681, 117, 15127, 32282, 10124, 10189, 12916, 156, 13078, 10537, 10894, 10529, 39803, 55911, 17781, 11345, 119, 100, 102, 19118, 78681, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 08:24:09 - INFO - __main__ - Sample 2089 of the training set: {'label': 0, 'sentence1': 'While the family fun hub holds back from turning into a ghost town for Japanese Yokai (demons) to roam around this year, Downtown East is bringing its scary stories online with a series of episodes on Instagram Stories.', 'sentence2': 'ghost town', 'input_ids': [101, 14600, 10105, 11365, 41807, 65896, 28278, 12014, 10188, 48448, 10708, 169, 100766, 12221, 10142, 13847, 30665, 18511, 113, 30776, 10891, 114, 10114, 25470, 11008, 12166, 10531, 10924, 117, 68339, 11830, 10124, 45749, 10474, 187, 15983, 10157, 21158, 13893, 10169, 169, 11366, 10108, 23604, 10135, 83019, 25955, 119, 102, 100766, 12221, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:230] 2022-07-26 08:24:11,514 >> The following columns in the training set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1254] 2022-07-26 08:24:11,522 >> ***** Running training *****\n",
            "[INFO|trainer.py:1255] 2022-07-26 08:24:11,522 >>   Num examples = 4631\n",
            "[INFO|trainer.py:1256] 2022-07-26 08:24:11,522 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1257] 2022-07-26 08:24:11,522 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1258] 2022-07-26 08:24:11,522 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1259] 2022-07-26 08:24:11,522 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1260] 2022-07-26 08:24:11,522 >>   Total optimization steps = 1450\n",
            " 10% 145/1450 [01:18<10:37,  2.05it/s][INFO|trainer.py:230] 2022-07-26 08:25:29,793 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:25:29,794 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:25:29,795 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:25:29,795 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.08it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.79it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.85it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.55it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 14.09it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.84it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.90it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.80it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.73it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.74it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.72it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.56it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 13.67it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.80it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.77it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.75it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.77it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 13.79it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.61it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.64it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.77it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.76it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.77it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.70it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 13.71it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.61it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.57it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.64it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.72it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.76it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.72it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 13.70it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.63it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.69it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.72it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.71it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.77it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.75it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.66it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 13.69it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.64it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.60it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.62it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.68it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6580412983894348, 'eval_accuracy': 0.7307171821594238, 'eval_f1': 0.7301459726771444, 'eval_runtime': 6.7548, 'eval_samples_per_second': 109.404, 'eval_steps_per_second': 13.768, 'epoch': 1.0}\n",
            " 10% 145/1450 [01:25<10:37,  2.05it/s]\n",
            "100% 93/93 [00:06<00:00, 13.68it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:25:36,551 >> Saving model checkpoint to models/OneShot/1/checkpoint-145\n",
            "[INFO|loading.py:60] 2022-07-26 08:25:36,551 >> Configuration saved in models/OneShot/1/checkpoint-145/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:25:36,660 >> Module weights saved in models/OneShot/1/checkpoint-145/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:25:36,660 >> Configuration saved in models/OneShot/1/checkpoint-145/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:25:36,667 >> Module weights saved in models/OneShot/1/checkpoint-145/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:25:36,668 >> Configuration saved in models/OneShot/1/checkpoint-145/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:25:36,675 >> Module weights saved in models/OneShot/1/checkpoint-145/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:25:36,675 >> tokenizer config file saved in models/OneShot/1/checkpoint-145/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:25:36,675 >> Special tokens file saved in models/OneShot/1/checkpoint-145/special_tokens_map.json\n",
            " 20% 290/1450 [02:43<09:29,  2.04it/s][INFO|trainer.py:230] 2022-07-26 08:26:54,582 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:26:54,584 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:26:54,584 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:26:54,584 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.68it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.57it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.78it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.45it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 14.04it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.93it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.92it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.82it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.58it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.56it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.42it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 13.55it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.69it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.75it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.63it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.53it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.50it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.46it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.57it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.60it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.59it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.51it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.53it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.58it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.53it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.56it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.54it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.39it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.47it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.44it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.32it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.44it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 13.42it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.48it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.57it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.56it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6823128461837769, 'eval_accuracy': 0.7740189433097839, 'eval_f1': 0.7739775576397245, 'eval_runtime': 6.8386, 'eval_samples_per_second': 108.063, 'eval_steps_per_second': 13.599, 'epoch': 2.0}\n",
            " 20% 290/1450 [02:49<09:29,  2.04it/s]\n",
            "100% 93/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:27:01,424 >> Saving model checkpoint to models/OneShot/1/checkpoint-290\n",
            "[INFO|loading.py:60] 2022-07-26 08:27:01,425 >> Configuration saved in models/OneShot/1/checkpoint-290/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:27:01,514 >> Module weights saved in models/OneShot/1/checkpoint-290/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:27:01,515 >> Configuration saved in models/OneShot/1/checkpoint-290/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:27:01,521 >> Module weights saved in models/OneShot/1/checkpoint-290/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:27:01,522 >> Configuration saved in models/OneShot/1/checkpoint-290/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:27:01,528 >> Module weights saved in models/OneShot/1/checkpoint-290/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:27:01,529 >> tokenizer config file saved in models/OneShot/1/checkpoint-290/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:27:01,529 >> Special tokens file saved in models/OneShot/1/checkpoint-290/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:27:01,980 >> Deleting older checkpoint [models/OneShot/1/checkpoint-145] due to args.save_total_limit\n",
            " 30% 435/1450 [04:07<08:21,  2.02it/s][INFO|trainer.py:230] 2022-07-26 08:28:19,400 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:28:19,402 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:28:19,402 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:28:19,402 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.58it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.33it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.57it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.24it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.83it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.65it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.67it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.61it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.41it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.51it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.56it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.50it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.47it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.54it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.43it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.50it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.50it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.50it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.40it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.48it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.44it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.40it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.48it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.48it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.40it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.48it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.49it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.40it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.47it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.44it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.40it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.35it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.45it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7745033502578735, 'eval_accuracy': 0.8037889003753662, 'eval_f1': 0.8036724276608336, 'eval_runtime': 6.874, 'eval_samples_per_second': 107.507, 'eval_steps_per_second': 13.529, 'epoch': 3.0}\n",
            " 30% 435/1450 [04:14<08:21,  2.02it/s]\n",
            "100% 93/93 [00:06<00:00, 13.48it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:28:26,277 >> Saving model checkpoint to models/OneShot/1/checkpoint-435\n",
            "[INFO|loading.py:60] 2022-07-26 08:28:26,278 >> Configuration saved in models/OneShot/1/checkpoint-435/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:28:26,362 >> Module weights saved in models/OneShot/1/checkpoint-435/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:28:26,363 >> Configuration saved in models/OneShot/1/checkpoint-435/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:28:26,370 >> Module weights saved in models/OneShot/1/checkpoint-435/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:28:26,370 >> Configuration saved in models/OneShot/1/checkpoint-435/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:28:26,376 >> Module weights saved in models/OneShot/1/checkpoint-435/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:28:26,377 >> tokenizer config file saved in models/OneShot/1/checkpoint-435/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:28:26,377 >> Special tokens file saved in models/OneShot/1/checkpoint-435/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:28:26,817 >> Deleting older checkpoint [models/OneShot/1/checkpoint-290] due to args.save_total_limit\n",
            "{'loss': 0.1736, 'learning_rate': 6.551724137931034e-05, 'epoch': 3.45}\n",
            " 40% 580/1450 [05:32<07:11,  2.02it/s][INFO|trainer.py:230] 2022-07-26 08:29:44,375 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:29:44,376 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:29:44,376 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:29:44,376 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.61it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.77it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.61it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.32it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.90it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.68it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.74it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.70it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.55it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.50it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.28it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.36it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.49it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.61it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.53it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.55it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.35it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.41it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.34it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.36it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.39it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.49it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.46it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.35it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.43it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.48it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.52it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.53it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.45it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.42it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.36it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.40it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.56it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.713748574256897, 'eval_accuracy': 0.8186739087104797, 'eval_f1': 0.8169845494196792, 'eval_runtime': 6.8701, 'eval_samples_per_second': 107.568, 'eval_steps_per_second': 13.537, 'epoch': 4.0}\n",
            " 40% 580/1450 [05:39<07:11,  2.02it/s]\n",
            "100% 93/93 [00:06<00:00, 13.51it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:29:51,248 >> Saving model checkpoint to models/OneShot/1/checkpoint-580\n",
            "[INFO|loading.py:60] 2022-07-26 08:29:51,249 >> Configuration saved in models/OneShot/1/checkpoint-580/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:29:51,336 >> Module weights saved in models/OneShot/1/checkpoint-580/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:29:51,337 >> Configuration saved in models/OneShot/1/checkpoint-580/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:29:51,343 >> Module weights saved in models/OneShot/1/checkpoint-580/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:29:51,343 >> Configuration saved in models/OneShot/1/checkpoint-580/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:29:51,349 >> Module weights saved in models/OneShot/1/checkpoint-580/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:29:51,350 >> tokenizer config file saved in models/OneShot/1/checkpoint-580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:29:51,350 >> Special tokens file saved in models/OneShot/1/checkpoint-580/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:29:51,817 >> Deleting older checkpoint [models/OneShot/1/checkpoint-435] due to args.save_total_limit\n",
            " 50% 725/1450 [06:57<05:59,  2.02it/s][INFO|trainer.py:230] 2022-07-26 08:31:09,329 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:31:09,331 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:31:09,331 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:31:09,331 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.91it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.67it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.85it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.47it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.82it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.63it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.62it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.64it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.56it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.53it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.48it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.32it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.39it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.40it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.47it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.54it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.47it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.42it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.51it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.45it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.48it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.37it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.44it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.46it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.48it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.40it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.43it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.51it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.48it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.52it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.54it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.52it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.36it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.37it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.33it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.827161967754364, 'eval_accuracy': 0.8200270533561707, 'eval_f1': 0.8173175396499394, 'eval_runtime': 6.8738, 'eval_samples_per_second': 107.51, 'eval_steps_per_second': 13.53, 'epoch': 5.0}\n",
            " 50% 725/1450 [07:04<05:59,  2.02it/s]\n",
            "100% 93/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:31:16,206 >> Saving model checkpoint to models/OneShot/1/checkpoint-725\n",
            "[INFO|loading.py:60] 2022-07-26 08:31:16,207 >> Configuration saved in models/OneShot/1/checkpoint-725/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:31:16,323 >> Module weights saved in models/OneShot/1/checkpoint-725/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:31:16,323 >> Configuration saved in models/OneShot/1/checkpoint-725/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:31:16,332 >> Module weights saved in models/OneShot/1/checkpoint-725/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:31:16,333 >> Configuration saved in models/OneShot/1/checkpoint-725/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:31:16,342 >> Module weights saved in models/OneShot/1/checkpoint-725/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:31:16,343 >> tokenizer config file saved in models/OneShot/1/checkpoint-725/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:31:16,343 >> Special tokens file saved in models/OneShot/1/checkpoint-725/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:31:16,880 >> Deleting older checkpoint [models/OneShot/1/checkpoint-580] due to args.save_total_limit\n",
            " 60% 870/1450 [08:22<04:49,  2.01it/s][INFO|trainer.py:230] 2022-07-26 08:32:34,352 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:32:34,354 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:32:34,354 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:32:34,354 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.53it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.60it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.55it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.20it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.87it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.64it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.60it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.61it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.54it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.51it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.44it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.25it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.33it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.40it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.41it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.34it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.31it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.46it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.36it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.24it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.31it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.32it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.30it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.39it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.42it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.28it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.30it/s]\u001b[A\n",
            " 73% 68/93 [00:05<00:01, 13.27it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.39it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.47it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.41it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.47it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.43it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.36it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7318314909934998, 'eval_accuracy': 0.8511502146720886, 'eval_f1': 0.8509512424091297, 'eval_runtime': 6.9058, 'eval_samples_per_second': 107.012, 'eval_steps_per_second': 13.467, 'epoch': 6.0}\n",
            " 60% 870/1450 [08:29<04:49,  2.01it/s]\n",
            "100% 93/93 [00:06<00:00, 13.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:32:41,261 >> Saving model checkpoint to models/OneShot/1/checkpoint-870\n",
            "[INFO|loading.py:60] 2022-07-26 08:32:41,262 >> Configuration saved in models/OneShot/1/checkpoint-870/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:32:41,346 >> Module weights saved in models/OneShot/1/checkpoint-870/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:32:41,347 >> Configuration saved in models/OneShot/1/checkpoint-870/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:32:41,352 >> Module weights saved in models/OneShot/1/checkpoint-870/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:32:41,353 >> Configuration saved in models/OneShot/1/checkpoint-870/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:32:41,359 >> Module weights saved in models/OneShot/1/checkpoint-870/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:32:41,359 >> tokenizer config file saved in models/OneShot/1/checkpoint-870/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:32:41,359 >> Special tokens file saved in models/OneShot/1/checkpoint-870/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:32:41,818 >> Deleting older checkpoint [models/OneShot/1/checkpoint-725] due to args.save_total_limit\n",
            "{'loss': 0.0328, 'learning_rate': 3.103448275862069e-05, 'epoch': 6.9}\n",
            " 70% 1015/1450 [09:47<03:35,  2.02it/s][INFO|trainer.py:230] 2022-07-26 08:33:59,314 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:33:59,316 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:33:59,316 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:33:59,316 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.58it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.74it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.35it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.92it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.70it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.69it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.68it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.68it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.58it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.51it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.33it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.40it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.49it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.43it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.48it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.36it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.39it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.48it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.42it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.39it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.28it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.32it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.44it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.42it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.36it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.37it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.44it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.42it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.49it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.43it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.38it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.44it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.47it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.42it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.840283215045929, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.8564869776781134, 'eval_runtime': 6.8811, 'eval_samples_per_second': 107.395, 'eval_steps_per_second': 13.515, 'epoch': 7.0}\n",
            " 70% 1015/1450 [09:54<03:35,  2.02it/s]\n",
            "100% 93/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:34:06,199 >> Saving model checkpoint to models/OneShot/1/checkpoint-1015\n",
            "[INFO|loading.py:60] 2022-07-26 08:34:06,199 >> Configuration saved in models/OneShot/1/checkpoint-1015/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:34:06,291 >> Module weights saved in models/OneShot/1/checkpoint-1015/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:34:06,292 >> Configuration saved in models/OneShot/1/checkpoint-1015/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:34:06,298 >> Module weights saved in models/OneShot/1/checkpoint-1015/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:34:06,298 >> Configuration saved in models/OneShot/1/checkpoint-1015/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:34:06,304 >> Module weights saved in models/OneShot/1/checkpoint-1015/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:34:06,304 >> tokenizer config file saved in models/OneShot/1/checkpoint-1015/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:34:06,305 >> Special tokens file saved in models/OneShot/1/checkpoint-1015/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:34:06,774 >> Deleting older checkpoint [models/OneShot/1/checkpoint-870] due to args.save_total_limit\n",
            " 80% 1160/1450 [11:12<02:23,  2.02it/s][INFO|trainer.py:230] 2022-07-26 08:35:24,165 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:35:24,167 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:35:24,167 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:35:24,167 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.52it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.47it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.70it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.34it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.83it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.73it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.79it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.63it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.56it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.53it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.47it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 13.44it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.43it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.36it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.49it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.46it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.41it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.37it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.39it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.40it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.41it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.49it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.42it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 13.30it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.37it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.27it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.30it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.22it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.35it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.38it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.32it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9577918648719788, 'eval_accuracy': 0.8538565635681152, 'eval_f1': 0.85315719752723, 'eval_runtime': 6.8968, 'eval_samples_per_second': 107.151, 'eval_steps_per_second': 13.484, 'epoch': 8.0}\n",
            " 80% 1160/1450 [11:19<02:23,  2.02it/s]\n",
            "100% 93/93 [00:06<00:00, 13.34it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:35:31,065 >> Saving model checkpoint to models/OneShot/1/checkpoint-1160\n",
            "[INFO|loading.py:60] 2022-07-26 08:35:31,066 >> Configuration saved in models/OneShot/1/checkpoint-1160/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:35:31,162 >> Module weights saved in models/OneShot/1/checkpoint-1160/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:35:31,162 >> Configuration saved in models/OneShot/1/checkpoint-1160/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:35:31,169 >> Module weights saved in models/OneShot/1/checkpoint-1160/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:35:31,170 >> Configuration saved in models/OneShot/1/checkpoint-1160/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:35:31,176 >> Module weights saved in models/OneShot/1/checkpoint-1160/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:35:31,176 >> tokenizer config file saved in models/OneShot/1/checkpoint-1160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:35:31,177 >> Special tokens file saved in models/OneShot/1/checkpoint-1160/special_tokens_map.json\n",
            " 90% 1305/1450 [12:37<01:11,  2.02it/s][INFO|trainer.py:230] 2022-07-26 08:36:49,129 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:36:49,131 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:36:49,131 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:36:49,131 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.46it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.42it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.56it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.23it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.71it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.57it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.62it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.57it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.51it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.45it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.38it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.19it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.32it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.32it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.38it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.36it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.33it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.35it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.38it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.42it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.55it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.43it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.43it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.38it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.38it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.35it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.47it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.46it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.28it/s]\u001b[A\n",
            " 73% 68/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.43it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.45it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.35it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.38it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.31it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.36it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.25it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.23it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.30it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9640564918518066, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.856447001905879, 'eval_runtime': 6.9134, 'eval_samples_per_second': 106.894, 'eval_steps_per_second': 13.452, 'epoch': 9.0}\n",
            " 90% 1305/1450 [12:44<01:11,  2.02it/s]\n",
            "100% 93/93 [00:06<00:00, 13.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:36:56,045 >> Saving model checkpoint to models/OneShot/1/checkpoint-1305\n",
            "[INFO|loading.py:60] 2022-07-26 08:36:56,046 >> Configuration saved in models/OneShot/1/checkpoint-1305/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:36:56,137 >> Module weights saved in models/OneShot/1/checkpoint-1305/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:36:56,138 >> Configuration saved in models/OneShot/1/checkpoint-1305/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:36:56,147 >> Module weights saved in models/OneShot/1/checkpoint-1305/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:36:56,147 >> Configuration saved in models/OneShot/1/checkpoint-1305/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:36:56,154 >> Module weights saved in models/OneShot/1/checkpoint-1305/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:36:56,154 >> tokenizer config file saved in models/OneShot/1/checkpoint-1305/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:36:56,154 >> Special tokens file saved in models/OneShot/1/checkpoint-1305/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:36:56,656 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1160] due to args.save_total_limit\n",
            "100% 1450/1450 [14:02<00:00,  2.01it/s][INFO|trainer.py:230] 2022-07-26 08:38:14,213 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:38:14,214 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:38:14,214 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:38:14,214 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.63it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 15.30it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 14.52it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 14.24it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 13.69it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 13.59it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 13.67it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:05, 13.57it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:05, 13.48it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:05, 13.52it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:05, 13.44it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:05, 13.37it/s]\u001b[A\n",
            " 30% 28/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 32% 30/93 [00:02<00:04, 13.44it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 13.42it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:04, 13.35it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:04, 13.39it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:04, 13.37it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 13.37it/s]\u001b[A\n",
            " 45% 42/93 [00:03<00:03, 13.41it/s]\u001b[A\n",
            " 47% 44/93 [00:03<00:03, 13.47it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 13.35it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 13.44it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:03, 13.39it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:03, 13.34it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 13.41it/s]\u001b[A\n",
            " 60% 56/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 62% 58/93 [00:04<00:02, 13.39it/s]\u001b[A\n",
            " 65% 60/93 [00:04<00:02, 13.45it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 13.43it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:02, 13.34it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:02, 13.39it/s]\u001b[A\n",
            " 73% 68/93 [00:05<00:01, 13.36it/s]\u001b[A\n",
            " 75% 70/93 [00:05<00:01, 13.35it/s]\u001b[A\n",
            " 77% 72/93 [00:05<00:01, 13.40it/s]\u001b[A\n",
            " 80% 74/93 [00:05<00:01, 13.38it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 13.33it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 13.33it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 13.32it/s]\u001b[A\n",
            " 88% 82/93 [00:06<00:00, 13.39it/s]\u001b[A\n",
            " 90% 84/93 [00:06<00:00, 13.36it/s]\u001b[A\n",
            " 92% 86/93 [00:06<00:00, 13.30it/s]\u001b[A\n",
            " 95% 88/93 [00:06<00:00, 13.34it/s]\u001b[A\n",
            " 97% 90/93 [00:06<00:00, 13.26it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0108026266098022, 'eval_accuracy': 0.8552097678184509, 'eval_f1': 0.8550813086355946, 'eval_runtime': 6.9081, 'eval_samples_per_second': 106.976, 'eval_steps_per_second': 13.462, 'epoch': 10.0}\n",
            "100% 1450/1450 [14:09<00:00,  2.01it/s]\n",
            "100% 93/93 [00:06<00:00, 13.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:136] 2022-07-26 08:38:21,124 >> Saving model checkpoint to models/OneShot/1/checkpoint-1450\n",
            "[INFO|loading.py:60] 2022-07-26 08:38:21,125 >> Configuration saved in models/OneShot/1/checkpoint-1450/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:38:21,213 >> Module weights saved in models/OneShot/1/checkpoint-1450/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:38:21,213 >> Configuration saved in models/OneShot/1/checkpoint-1450/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:38:21,219 >> Module weights saved in models/OneShot/1/checkpoint-1450/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:38:21,220 >> Configuration saved in models/OneShot/1/checkpoint-1450/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:38:21,226 >> Module weights saved in models/OneShot/1/checkpoint-1450/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:38:21,226 >> tokenizer config file saved in models/OneShot/1/checkpoint-1450/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:38:21,227 >> Special tokens file saved in models/OneShot/1/checkpoint-1450/special_tokens_map.json\n",
            "[INFO|trainer.py:2222] 2022-07-26 08:38:21,693 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1305] due to args.save_total_limit\n",
            "[INFO|trainer.py:1483] 2022-07-26 08:38:21,714 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1492] 2022-07-26 08:38:21,714 >> Loading best model from models/OneShot/1/checkpoint-1015 (score: 0.8564869776781134).\n",
            "[WARNING|trainer.py:1515] 2022-07-26 08:38:21,714 >> Could not locate the best model at models/OneShot/1/checkpoint-1015/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 850.1918, 'train_samples_per_second': 54.47, 'train_steps_per_second': 1.705, 'train_loss': 0.07274158773751094, 'epoch': 10.0}\n",
            "100% 1450/1450 [14:10<00:00,  2.01it/s][INFO|trainer.py:260] 2022-07-26 08:38:21,715 >> Loading best adapter(s) from models/OneShot/1/checkpoint-1015 (score: 0.8564869776781134).\n",
            "[INFO|loading.py:77] 2022-07-26 08:38:21,716 >> Loading module configuration from models/OneShot/1/checkpoint-1015/glue/adapter_config.json\n",
            "[WARNING|loading.py:448] 2022-07-26 08:38:21,716 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-07-26 08:38:21,741 >> Loading module weights from models/OneShot/1/checkpoint-1015/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-07-26 08:38:21,761 >> Loading module configuration from models/OneShot/1/checkpoint-1015/glue/head_config.json\n",
            "[WARNING|loading.py:726] 2022-07-26 08:38:21,761 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:616] 2022-07-26 08:38:21,770 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-07-26 08:38:21,771 >> Loading module weights from models/OneShot/1/checkpoint-1015/glue/pytorch_model_head.bin\n",
            "100% 1450/1450 [14:10<00:00,  1.71it/s]\n",
            "[INFO|trainer.py:136] 2022-07-26 08:38:21,774 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-07-26 08:38:21,775 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:38:21,897 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:38:21,898 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:38:21,906 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-07-26 08:38:21,906 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-07-26 08:38:21,914 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-07-26 08:38:21,914 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-07-26 08:38:21,914 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.0727\n",
            "  train_runtime            = 0:14:10.19\n",
            "  train_samples            =       4631\n",
            "  train_samples_per_second =      54.47\n",
            "  train_steps_per_second   =      1.705\n",
            "07/26/2022 08:38:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:230] 2022-07-26 08:38:22,122 >> The following columns in the evaluation set  don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2394] 2022-07-26 08:38:22,124 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2396] 2022-07-26 08:38:22,124 >>   Num examples = 739\n",
            "[INFO|trainer.py:2399] 2022-07-26 08:38:22,124 >>   Batch size = 8\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 676, in <module>\n",
            "    main()\n",
            "  File \"/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters_new.py\", line 614, in main\n",
            "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2269, in evaluate\n",
            "    metric_key_prefix=metric_key_prefix,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2436, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2646, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1991, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/models/bert.py\", line 85, in forward\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 745, in forward_head\n",
            "    return head_module(all_outputs, cls_output, attention_mask, return_dict, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapters/heads/base.py\", line 125, in forward\n",
            "    logits = super().forward(cls_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 06 : Monolingual MirrorWiC BERT for one-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "NIyNmo_PL_RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'cambridgeltl/mirrorwic-bert-base-uncased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 15 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "id": "zsTpdpNmL-RC",
        "outputId": "e0a4dc33-a4df-490b-f284-3ceb3b5ee5fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/26/2022 06:35:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/26/2022 06:35:54 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Jul26_06-35-54_5a915ffe101d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/26/2022 06:35:54 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "07/26/2022 06:35:54 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "07/26/2022 06:35:55 - WARNING - datasets.builder -   Using custom data configuration default-ca5a028a8281ff16\n",
            "07/26/2022 06:35:55 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-ca5a028a8281ff16/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "100% 2/2 [00:00<00:00, 123.84it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:35:56,309 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:35:56,317 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:35:58,109 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:35:58,110 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:36:03,508 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/462202e3879c0bc38b0f112d6513bd775a86e228e46ca8ceb6b7cc271fa96fe1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:36:03,508 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:36:03,508 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:36:03,508 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/eddf333a0b59fecb3d9a870823738ea744160a6e2c333ab92ce97e7149e09cd8.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-07-26 06:36:03,508 >> loading file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d18bed018631fcadd197f6c150bccd823012f9747d6c14b19fff54b269a8d6d7.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:36:04,403 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:36:04,404 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-07-26 06:36:05,338 >> loading configuration file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/be9e4262bebf2afbb29f07d0dd1c5dab2481d67324ee4d0eeb8638947d014944.44e527e12d02eb654de75268caec6a7218471a45d75ed404e7baaef8b13ece0f\n",
            "[INFO|configuration_utils.py:708] 2022-07-26 06:36:05,339 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"cambridgeltl/mirrorwic-bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2107] 2022-07-26 06:36:06,369 >> loading weights file https://huggingface.co/cambridgeltl/mirrorwic-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/d57b0eb5c893ede0ea4088e156c4c06f1157a296783e8e23dca56a9f3c68237b.ec3a6b253565b365acd2fadaaac221a38539339839ed69cbd7c22ca292ee9066\n",
            "[INFO|modeling_utils.py:2483] 2022-07-26 06:36:09,594 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:2486] 2022-07-26 06:36:09,594 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cambridgeltl/mirrorwic-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  8.15ba/s]\n",
            "100% 1/1 [00:00<00:00, 15.35ba/s]\n",
            "07/26/2022 06:36:10 - INFO - __main__ -   Sample 550 of the training set: {'label': 0, 'sentence1': 'When in a current bull market it can be difficult to ask the question: when will the bull market end?', 'sentence2': 'bull market', 'input_ids': [101, 2043, 1999, 1037, 2783, 7087, 3006, 2009, 2064, 2022, 3697, 2000, 3198, 1996, 3160, 1024, 2043, 2097, 1996, 7087, 3006, 2203, 1029, 102, 7087, 3006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 06:36:10 - INFO - __main__ -   Sample 2331 of the training set: {'label': 1, 'sentence1': 'The Pain Killer consists of light and dark rums, pineapple and orange juices, cream of coconut and nutmeg — it’s blended, making it the ideal drink to sip on the beach at the lakefront resort.', 'sentence2': 'pain killer', 'input_ids': [101, 1996, 3255, 6359, 3774, 1997, 2422, 1998, 2601, 19379, 2015, 1010, 7222, 23804, 1998, 4589, 10869, 2015, 1010, 6949, 1997, 16027, 1998, 17490, 4168, 2290, 1517, 2009, 1521, 1055, 19803, 1010, 2437, 2009, 1996, 7812, 4392, 2000, 10668, 2006, 1996, 3509, 2012, 1996, 2697, 12792, 7001, 1012, 102, 3255, 6359, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "07/26/2022 06:36:10 - INFO - __main__ -   Sample 3286 of the training set: {'label': 1, 'sentence1': 'Renovations are scheduled to begin Monday for the P4 parking lot in Colonial Williamsburg, located behind the Merchants Square bus stop on South Henry Street, which will reopen as paid parking.', 'sentence2': 'parking lot', 'input_ids': [101, 15576, 2024, 5115, 2000, 4088, 6928, 2005, 1996, 1052, 2549, 5581, 2843, 1999, 5336, 26366, 1010, 2284, 2369, 1996, 10310, 2675, 3902, 2644, 2006, 2148, 2888, 2395, 1010, 2029, 2097, 2128, 26915, 2004, 3825, 5581, 1012, 102, 5581, 2843, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:1803] 2022-07-26 06:36:12,147 >> Loading model from models/OneShot/1/checkpoint-963.\n",
            "[INFO|trainer.py:662] 2022-07-26 06:36:12,488 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-26 06:36:13,109 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-26 06:36:13,109 >>   Num examples = 3414\n",
            "[INFO|trainer.py:1518] 2022-07-26 06:36:13,109 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1519] 2022-07-26 06:36:13,109 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1520] 2022-07-26 06:36:13,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-26 06:36:13,109 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-26 06:36:13,109 >>   Total optimization steps = 1605\n",
            "[INFO|trainer.py:1542] 2022-07-26 06:36:13,110 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1543] 2022-07-26 06:36:13,110 >>   Continuing training from epoch 9\n",
            "[INFO|trainer.py:1544] 2022-07-26 06:36:13,110 >>   Continuing training from global step 963\n",
            "[INFO|trainer.py:1547] 2022-07-26 06:36:13,110 >>   Will skip the first 9 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches: : 0it [00:00, ?it/s]\n",
            "Skipping the first batches: : 0it [00:00, ?it/s]\n",
            "\n",
            " 60% 964/1605 [00:01<00:00, 694.83it/s]\u001b[A\n",
            " 61% 981/1605 [00:12<00:00, 694.83it/s]\u001b[A\n",
            " 61% 982/1605 [00:12<00:10, 58.06it/s] \u001b[A\n",
            " 61% 983/1605 [00:12<00:11, 54.18it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0014, 'learning_rate': 7.538940809968847e-06, 'epoch': 9.35}\n",
            "\n",
            " 62% 1000/1605 [00:23<00:11, 54.18it/s]\u001b[A\n",
            " 63% 1012/1605 [00:30<00:38, 15.29it/s]\u001b[A\n",
            " 63% 1013/1605 [00:31<00:40, 14.80it/s]\u001b[A\n",
            " 64% 1029/1605 [00:41<01:05,  8.84it/s]\u001b[A\n",
            " 64% 1030/1605 [00:42<01:07,  8.56it/s]\u001b[A\n",
            " 65% 1040/1605 [00:48<01:32,  6.10it/s]\u001b[A\n",
            " 65% 1047/1605 [00:52<01:53,  4.91it/s]\u001b[A\n",
            " 66% 1052/1605 [00:55<02:11,  4.21it/s]\u001b[A\n",
            " 66% 1055/1605 [00:57<02:24,  3.80it/s]\u001b[A\n",
            " 66% 1057/1605 [00:59<02:35,  3.52it/s]\u001b[A\n",
            " 66% 1059/1605 [01:00<02:49,  3.22it/s]\u001b[A\n",
            " 66% 1061/1605 [01:01<03:06,  2.91it/s]\u001b[A\n",
            " 66% 1062/1605 [01:02<03:16,  2.76it/s]\u001b[A\n",
            " 66% 1063/1605 [01:02<03:29,  2.59it/s]\u001b[A\n",
            " 66% 1064/1605 [01:03<03:43,  2.42it/s]\u001b[A\n",
            " 66% 1065/1605 [01:04<03:59,  2.25it/s]\u001b[A\n",
            " 66% 1066/1605 [01:04<04:16,  2.10it/s]\u001b[A\n",
            " 66% 1067/1605 [01:05<04:33,  1.97it/s]\u001b[A\n",
            " 67% 1068/1605 [01:06<04:48,  1.86it/s]\u001b[A\n",
            " 67% 1069/1605 [01:06<05:01,  1.78it/s]\u001b[A\n",
            " 67% 1070/1605 [01:07<04:47,  1.86it/s]\u001b[A[INFO|trainer.py:662] 2022-07-26 06:37:20,406 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:37:20,408 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:37:20,408 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:37:20,408 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1616473197937012, 'eval_accuracy': 0.7982832789421082, 'eval_f1': 0.795537714712472, 'eval_runtime': 3.5637, 'eval_samples_per_second': 130.763, 'eval_steps_per_second': 16.556, 'epoch': 10.0}\n",
            "100% 59/59 [00:03<00:00, 16.46it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:37:23,973 >> Saving model checkpoint to models/OneShot/1/checkpoint-1070\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:37:23,975 >> Configuration saved in models/OneShot/1/checkpoint-1070/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:37:25,066 >> Model weights saved in models/OneShot/1/checkpoint-1070/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:37:25,067 >> tokenizer config file saved in models/OneShot/1/checkpoint-1070/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:37:25,067 >> Special tokens file saved in models/OneShot/1/checkpoint-1070/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:37:28,664 >> Deleting older checkpoint [models/OneShot/1/checkpoint-963] due to args.save_total_limit\n",
            "\n",
            " 67% 1071/1605 [01:16<24:45,  2.78s/it]\u001b[A\n",
            " 67% 1072/1605 [01:17<19:31,  2.20s/it]\u001b[A\n",
            " 67% 1073/1605 [01:17<15:37,  1.76s/it]\u001b[A\n",
            " 67% 1074/1605 [01:18<12:44,  1.44s/it]\u001b[A\n",
            " 67% 1075/1605 [01:18<10:41,  1.21s/it]\u001b[A\n",
            " 67% 1076/1605 [01:19<09:13,  1.05s/it]\u001b[A\n",
            " 67% 1077/1605 [01:20<08:10,  1.08it/s]\u001b[A\n",
            " 67% 1078/1605 [01:20<07:25,  1.18it/s]\u001b[A\n",
            " 67% 1079/1605 [01:21<06:54,  1.27it/s]\u001b[A\n",
            " 67% 1080/1605 [01:22<06:32,  1.34it/s]\u001b[A\n",
            " 67% 1081/1605 [01:22<06:16,  1.39it/s]\u001b[A\n",
            " 67% 1082/1605 [01:23<06:04,  1.43it/s]\u001b[A\n",
            " 67% 1083/1605 [01:24<05:56,  1.46it/s]\u001b[A\n",
            " 68% 1084/1605 [01:24<05:50,  1.48it/s]\u001b[A\n",
            " 68% 1085/1605 [01:25<05:46,  1.50it/s]\u001b[A\n",
            " 68% 1086/1605 [01:26<05:43,  1.51it/s]\u001b[A\n",
            " 68% 1087/1605 [01:26<05:40,  1.52it/s]\u001b[A\n",
            " 68% 1088/1605 [01:27<05:38,  1.53it/s]\u001b[A\n",
            " 68% 1089/1605 [01:28<05:37,  1.53it/s]\u001b[A\n",
            " 68% 1090/1605 [01:28<05:35,  1.53it/s]\u001b[A\n",
            " 68% 1091/1605 [01:29<05:35,  1.53it/s]\u001b[A\n",
            " 68% 1092/1605 [01:29<05:33,  1.54it/s]\u001b[A\n",
            " 68% 1093/1605 [01:30<05:32,  1.54it/s]\u001b[A\n",
            " 68% 1094/1605 [01:31<05:31,  1.54it/s]\u001b[A\n",
            " 68% 1095/1605 [01:31<05:30,  1.54it/s]\u001b[A\n",
            " 68% 1096/1605 [01:32<05:30,  1.54it/s]\u001b[A\n",
            " 68% 1097/1605 [01:33<05:30,  1.54it/s]\u001b[A\n",
            " 68% 1098/1605 [01:33<05:28,  1.54it/s]\u001b[A\n",
            " 68% 1099/1605 [01:34<05:28,  1.54it/s]\u001b[A\n",
            " 69% 1100/1605 [01:35<05:27,  1.54it/s]\u001b[A\n",
            " 69% 1101/1605 [01:35<05:27,  1.54it/s]\u001b[A\n",
            " 69% 1102/1605 [01:36<05:26,  1.54it/s]\u001b[A\n",
            " 69% 1103/1605 [01:37<05:25,  1.54it/s]\u001b[A\n",
            " 69% 1104/1605 [01:37<05:24,  1.54it/s]\u001b[A\n",
            " 69% 1105/1605 [01:38<05:24,  1.54it/s]\u001b[A\n",
            " 69% 1106/1605 [01:39<05:23,  1.54it/s]\u001b[A\n",
            " 69% 1107/1605 [01:39<05:22,  1.54it/s]\u001b[A\n",
            " 69% 1108/1605 [01:40<05:22,  1.54it/s]\u001b[A\n",
            " 69% 1109/1605 [01:41<05:21,  1.54it/s]\u001b[A\n",
            " 69% 1110/1605 [01:41<05:21,  1.54it/s]\u001b[A\n",
            " 69% 1111/1605 [01:42<05:20,  1.54it/s]\u001b[A\n",
            " 69% 1112/1605 [01:42<05:21,  1.53it/s]\u001b[A\n",
            " 69% 1113/1605 [01:43<05:22,  1.53it/s]\u001b[A\n",
            " 69% 1114/1605 [01:44<05:23,  1.52it/s]\u001b[A\n",
            " 69% 1115/1605 [01:44<05:21,  1.52it/s]\u001b[A\n",
            " 70% 1116/1605 [01:45<05:21,  1.52it/s]\u001b[A\n",
            " 70% 1117/1605 [01:46<05:19,  1.53it/s]\u001b[A\n",
            " 70% 1118/1605 [01:46<05:17,  1.53it/s]\u001b[A\n",
            " 70% 1119/1605 [01:47<05:16,  1.53it/s]\u001b[A\n",
            " 70% 1120/1605 [01:48<05:15,  1.54it/s]\u001b[A\n",
            " 70% 1121/1605 [01:48<05:14,  1.54it/s]\u001b[A\n",
            " 70% 1122/1605 [01:49<05:13,  1.54it/s]\u001b[A\n",
            " 70% 1123/1605 [01:50<05:13,  1.54it/s]\u001b[A\n",
            " 70% 1124/1605 [01:50<05:12,  1.54it/s]\u001b[A\n",
            " 70% 1125/1605 [01:51<05:11,  1.54it/s]\u001b[A\n",
            " 70% 1126/1605 [01:52<05:10,  1.54it/s]\u001b[A\n",
            " 70% 1127/1605 [01:52<05:10,  1.54it/s]\u001b[A\n",
            " 70% 1128/1605 [01:53<05:10,  1.54it/s]\u001b[A\n",
            " 70% 1129/1605 [01:54<05:09,  1.54it/s]\u001b[A\n",
            " 70% 1130/1605 [01:54<05:09,  1.54it/s]\u001b[A\n",
            " 70% 1131/1605 [01:55<05:09,  1.53it/s]\u001b[A\n",
            " 71% 1132/1605 [01:56<05:08,  1.53it/s]\u001b[A\n",
            " 71% 1133/1605 [01:56<05:07,  1.54it/s]\u001b[A\n",
            " 71% 1134/1605 [01:57<05:06,  1.54it/s]\u001b[A\n",
            " 71% 1135/1605 [01:57<05:05,  1.54it/s]\u001b[A\n",
            " 71% 1136/1605 [01:58<05:05,  1.54it/s]\u001b[A\n",
            " 71% 1137/1605 [01:59<05:04,  1.54it/s]\u001b[A\n",
            " 71% 1138/1605 [01:59<05:04,  1.53it/s]\u001b[A\n",
            " 71% 1139/1605 [02:00<05:04,  1.53it/s]\u001b[A\n",
            " 71% 1140/1605 [02:01<05:03,  1.53it/s]\u001b[A\n",
            " 71% 1141/1605 [02:01<05:03,  1.53it/s]\u001b[A\n",
            " 71% 1142/1605 [02:02<05:03,  1.53it/s]\u001b[A\n",
            " 71% 1143/1605 [02:03<05:02,  1.53it/s]\u001b[A\n",
            " 71% 1144/1605 [02:03<05:02,  1.53it/s]\u001b[A\n",
            " 71% 1145/1605 [02:04<05:01,  1.53it/s]\u001b[A\n",
            " 71% 1146/1605 [02:05<05:00,  1.53it/s]\u001b[A\n",
            " 71% 1147/1605 [02:05<04:59,  1.53it/s]\u001b[A\n",
            " 72% 1148/1605 [02:06<04:59,  1.53it/s]\u001b[A\n",
            " 72% 1149/1605 [02:07<04:58,  1.53it/s]\u001b[A\n",
            " 72% 1150/1605 [02:07<04:57,  1.53it/s]\u001b[A\n",
            " 72% 1151/1605 [02:08<04:55,  1.54it/s]\u001b[A\n",
            " 72% 1152/1605 [02:09<04:54,  1.54it/s]\u001b[A\n",
            " 72% 1153/1605 [02:09<04:54,  1.54it/s]\u001b[A\n",
            " 72% 1154/1605 [02:10<04:53,  1.53it/s]\u001b[A\n",
            " 72% 1155/1605 [02:11<04:54,  1.53it/s]\u001b[A\n",
            " 72% 1156/1605 [02:11<04:53,  1.53it/s]\u001b[A\n",
            " 72% 1157/1605 [02:12<04:52,  1.53it/s]\u001b[A\n",
            " 72% 1158/1605 [02:12<04:52,  1.53it/s]\u001b[A\n",
            " 72% 1159/1605 [02:13<04:52,  1.53it/s]\u001b[A\n",
            " 72% 1160/1605 [02:14<04:51,  1.53it/s]\u001b[A\n",
            " 72% 1161/1605 [02:14<04:51,  1.52it/s]\u001b[A\n",
            " 72% 1162/1605 [02:15<04:50,  1.53it/s]\u001b[A\n",
            " 72% 1163/1605 [02:16<04:49,  1.53it/s]\u001b[A\n",
            " 73% 1164/1605 [02:16<04:49,  1.52it/s]\u001b[A\n",
            " 73% 1165/1605 [02:17<04:48,  1.52it/s]\u001b[A\n",
            " 73% 1166/1605 [02:18<04:48,  1.52it/s]\u001b[A\n",
            " 73% 1167/1605 [02:18<04:48,  1.52it/s]\u001b[A\n",
            " 73% 1168/1605 [02:19<04:48,  1.52it/s]\u001b[A\n",
            " 73% 1169/1605 [02:20<04:46,  1.52it/s]\u001b[A\n",
            " 73% 1170/1605 [02:20<04:47,  1.52it/s]\u001b[A\n",
            " 73% 1171/1605 [02:21<04:46,  1.52it/s]\u001b[A\n",
            " 73% 1172/1605 [02:22<04:46,  1.51it/s]\u001b[A\n",
            " 73% 1173/1605 [02:22<04:46,  1.51it/s]\u001b[A\n",
            " 73% 1174/1605 [02:23<04:46,  1.51it/s]\u001b[A\n",
            " 73% 1175/1605 [02:24<04:44,  1.51it/s]\u001b[A\n",
            " 73% 1176/1605 [02:24<04:45,  1.50it/s]\u001b[A\n",
            " 73% 1177/1605 [02:25<04:21,  1.64it/s]\u001b[A[INFO|trainer.py:662] 2022-07-26 06:38:38,488 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:38:38,490 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:38:38,490 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:38:38,490 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1488301753997803, 'eval_accuracy': 0.7918455004692078, 'eval_f1': 0.7860027363926013, 'eval_runtime': 3.8038, 'eval_samples_per_second': 122.51, 'eval_steps_per_second': 15.511, 'epoch': 11.0}\n",
            "100% 59/59 [00:03<00:00, 15.14it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:38:42,296 >> Saving model checkpoint to models/OneShot/1/checkpoint-1177\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:38:42,297 >> Configuration saved in models/OneShot/1/checkpoint-1177/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:38:43,486 >> Model weights saved in models/OneShot/1/checkpoint-1177/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:38:43,487 >> tokenizer config file saved in models/OneShot/1/checkpoint-1177/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:38:43,487 >> Special tokens file saved in models/OneShot/1/checkpoint-1177/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:38:46,809 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1070] due to args.save_total_limit\n",
            "\n",
            " 73% 1178/1605 [02:34<22:42,  3.19s/it]\u001b[A\n",
            " 73% 1179/1605 [02:35<17:16,  2.43s/it]\u001b[A\n",
            " 74% 1180/1605 [02:35<13:28,  1.90s/it]\u001b[A\n",
            " 74% 1181/1605 [02:36<10:51,  1.54s/it]\u001b[A\n",
            " 74% 1182/1605 [02:37<08:59,  1.27s/it]\u001b[A\n",
            " 74% 1183/1605 [02:37<07:41,  1.09s/it]\u001b[A\n",
            " 74% 1184/1605 [02:38<06:47,  1.03it/s]\u001b[A\n",
            " 74% 1185/1605 [02:39<06:09,  1.14it/s]\u001b[A\n",
            " 74% 1186/1605 [02:39<05:43,  1.22it/s]\u001b[A\n",
            " 74% 1187/1605 [02:40<05:24,  1.29it/s]\u001b[A\n",
            " 74% 1188/1605 [02:41<05:10,  1.34it/s]\u001b[A\n",
            " 74% 1189/1605 [02:41<05:02,  1.38it/s]\u001b[A\n",
            " 74% 1190/1605 [02:42<04:55,  1.41it/s]\u001b[A\n",
            " 74% 1191/1605 [02:43<04:50,  1.43it/s]\u001b[A\n",
            " 74% 1192/1605 [02:44<04:46,  1.44it/s]\u001b[A\n",
            " 74% 1193/1605 [02:44<04:44,  1.45it/s]\u001b[A\n",
            " 74% 1194/1605 [02:45<04:42,  1.46it/s]\u001b[A\n",
            " 74% 1195/1605 [02:46<04:41,  1.46it/s]\u001b[A\n",
            " 75% 1196/1605 [02:46<04:39,  1.46it/s]\u001b[A\n",
            " 75% 1197/1605 [02:47<04:38,  1.47it/s]\u001b[A\n",
            " 75% 1198/1605 [02:48<04:37,  1.47it/s]\u001b[A\n",
            " 75% 1199/1605 [02:48<04:37,  1.46it/s]\u001b[A\n",
            " 75% 1200/1605 [02:49<04:36,  1.46it/s]\u001b[A\n",
            " 75% 1201/1605 [02:50<04:36,  1.46it/s]\u001b[A\n",
            " 75% 1202/1605 [02:50<04:36,  1.46it/s]\u001b[A\n",
            " 75% 1203/1605 [02:51<04:36,  1.46it/s]\u001b[A\n",
            " 75% 1204/1605 [02:52<04:35,  1.45it/s]\u001b[A\n",
            " 75% 1205/1605 [02:52<04:35,  1.45it/s]\u001b[A\n",
            " 75% 1206/1605 [02:53<04:34,  1.45it/s]\u001b[A\n",
            " 75% 1207/1605 [02:54<04:33,  1.46it/s]\u001b[A\n",
            " 75% 1208/1605 [02:54<04:32,  1.46it/s]\u001b[A\n",
            " 75% 1209/1605 [02:55<04:31,  1.46it/s]\u001b[A\n",
            " 75% 1210/1605 [02:56<04:31,  1.45it/s]\u001b[A\n",
            " 75% 1211/1605 [02:57<04:31,  1.45it/s]\u001b[A\n",
            " 76% 1212/1605 [02:57<04:30,  1.45it/s]\u001b[A\n",
            " 76% 1213/1605 [02:58<04:30,  1.45it/s]\u001b[A\n",
            " 76% 1214/1605 [02:59<04:29,  1.45it/s]\u001b[A\n",
            " 76% 1215/1605 [02:59<04:28,  1.45it/s]\u001b[A\n",
            " 76% 1216/1605 [03:00<04:27,  1.45it/s]\u001b[A\n",
            " 76% 1217/1605 [03:01<04:27,  1.45it/s]\u001b[A\n",
            " 76% 1218/1605 [03:01<04:26,  1.45it/s]\u001b[A\n",
            " 76% 1219/1605 [03:02<04:25,  1.45it/s]\u001b[A\n",
            " 76% 1220/1605 [03:03<04:24,  1.45it/s]\u001b[A\n",
            " 76% 1221/1605 [03:03<04:24,  1.45it/s]\u001b[A\n",
            " 76% 1222/1605 [03:04<04:23,  1.45it/s]\u001b[A\n",
            " 76% 1223/1605 [03:05<04:22,  1.46it/s]\u001b[A\n",
            " 76% 1224/1605 [03:05<04:21,  1.45it/s]\u001b[A\n",
            " 76% 1225/1605 [03:06<04:20,  1.46it/s]\u001b[A\n",
            " 76% 1226/1605 [03:07<04:20,  1.46it/s]\u001b[A\n",
            " 76% 1227/1605 [03:08<04:18,  1.46it/s]\u001b[A\n",
            " 77% 1228/1605 [03:08<04:17,  1.46it/s]\u001b[A\n",
            " 77% 1229/1605 [03:09<04:17,  1.46it/s]\u001b[A\n",
            " 77% 1230/1605 [03:10<04:16,  1.46it/s]\u001b[A\n",
            " 77% 1231/1605 [03:10<04:14,  1.47it/s]\u001b[A\n",
            " 77% 1232/1605 [03:11<04:13,  1.47it/s]\u001b[A\n",
            " 77% 1233/1605 [03:12<04:12,  1.47it/s]\u001b[A\n",
            " 77% 1234/1605 [03:12<04:11,  1.47it/s]\u001b[A\n",
            " 77% 1235/1605 [03:13<04:11,  1.47it/s]\u001b[A\n",
            " 77% 1236/1605 [03:14<04:10,  1.47it/s]\u001b[A\n",
            " 77% 1237/1605 [03:14<04:09,  1.47it/s]\u001b[A\n",
            " 77% 1238/1605 [03:15<04:09,  1.47it/s]\u001b[A\n",
            " 77% 1239/1605 [03:16<04:08,  1.47it/s]\u001b[A\n",
            " 77% 1240/1605 [03:16<04:07,  1.47it/s]\u001b[A\n",
            " 77% 1241/1605 [03:17<04:06,  1.48it/s]\u001b[A\n",
            " 77% 1242/1605 [03:18<04:05,  1.48it/s]\u001b[A\n",
            " 77% 1243/1605 [03:18<04:04,  1.48it/s]\u001b[A\n",
            " 78% 1244/1605 [03:19<04:02,  1.49it/s]\u001b[A\n",
            " 78% 1245/1605 [03:20<04:02,  1.49it/s]\u001b[A\n",
            " 78% 1246/1605 [03:20<04:02,  1.48it/s]\u001b[A\n",
            " 78% 1247/1605 [03:21<04:01,  1.48it/s]\u001b[A\n",
            " 78% 1248/1605 [03:22<04:00,  1.48it/s]\u001b[A\n",
            " 78% 1249/1605 [03:22<04:00,  1.48it/s]\u001b[A\n",
            " 78% 1250/1605 [03:23<03:59,  1.48it/s]\u001b[A\n",
            " 78% 1251/1605 [03:24<03:58,  1.48it/s]\u001b[A\n",
            " 78% 1252/1605 [03:24<03:57,  1.49it/s]\u001b[A\n",
            " 78% 1253/1605 [03:25<03:56,  1.49it/s]\u001b[A\n",
            " 78% 1254/1605 [03:26<03:55,  1.49it/s]\u001b[A\n",
            " 78% 1255/1605 [03:26<03:55,  1.49it/s]\u001b[A\n",
            " 78% 1256/1605 [03:27<03:54,  1.49it/s]\u001b[A\n",
            " 78% 1257/1605 [03:28<03:53,  1.49it/s]\u001b[A\n",
            " 78% 1258/1605 [03:28<03:53,  1.49it/s]\u001b[A\n",
            " 78% 1259/1605 [03:29<03:52,  1.49it/s]\u001b[A\n",
            " 79% 1260/1605 [03:30<03:51,  1.49it/s]\u001b[A\n",
            " 79% 1261/1605 [03:30<03:50,  1.49it/s]\u001b[A\n",
            " 79% 1262/1605 [03:31<03:50,  1.49it/s]\u001b[A\n",
            " 79% 1263/1605 [03:32<03:49,  1.49it/s]\u001b[A\n",
            " 79% 1264/1605 [03:32<03:48,  1.49it/s]\u001b[A\n",
            " 79% 1265/1605 [03:33<03:47,  1.49it/s]\u001b[A\n",
            " 79% 1266/1605 [03:34<03:47,  1.49it/s]\u001b[A\n",
            " 79% 1267/1605 [03:35<03:46,  1.49it/s]\u001b[A\n",
            " 79% 1268/1605 [03:35<03:46,  1.49it/s]\u001b[A\n",
            " 79% 1269/1605 [03:36<03:45,  1.49it/s]\u001b[A\n",
            " 79% 1270/1605 [03:37<03:44,  1.49it/s]\u001b[A\n",
            " 79% 1271/1605 [03:37<03:43,  1.49it/s]\u001b[A\n",
            " 79% 1272/1605 [03:38<03:42,  1.49it/s]\u001b[A\n",
            " 79% 1273/1605 [03:39<03:42,  1.49it/s]\u001b[A\n",
            " 79% 1274/1605 [03:39<03:41,  1.49it/s]\u001b[A\n",
            " 79% 1275/1605 [03:40<03:40,  1.49it/s]\u001b[A\n",
            " 80% 1276/1605 [03:41<03:40,  1.49it/s]\u001b[A\n",
            " 80% 1277/1605 [03:41<03:39,  1.49it/s]\u001b[A\n",
            " 80% 1278/1605 [03:42<03:39,  1.49it/s]\u001b[A\n",
            " 80% 1279/1605 [03:43<03:38,  1.49it/s]\u001b[A\n",
            " 80% 1280/1605 [03:43<03:37,  1.49it/s]\u001b[A\n",
            " 80% 1281/1605 [03:44<03:37,  1.49it/s]\u001b[A\n",
            " 80% 1282/1605 [03:45<03:36,  1.49it/s]\u001b[A\n",
            " 80% 1283/1605 [03:45<03:35,  1.49it/s]\u001b[A\n",
            " 80% 1284/1605 [03:46<03:17,  1.63it/s]\u001b[A[INFO|trainer.py:662] 2022-07-26 06:39:59,330 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:39:59,332 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:39:59,332 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:39:59,332 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.135042667388916, 'eval_accuracy': 0.8133047223091125, 'eval_f1': 0.8033192484390994, 'eval_runtime': 3.8131, 'eval_samples_per_second': 122.211, 'eval_steps_per_second': 15.473, 'epoch': 12.0}\n",
            "100% 59/59 [00:03<00:00, 15.18it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:40:03,147 >> Saving model checkpoint to models/OneShot/1/checkpoint-1284\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:40:03,147 >> Configuration saved in models/OneShot/1/checkpoint-1284/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:40:04,187 >> Model weights saved in models/OneShot/1/checkpoint-1284/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:40:04,187 >> tokenizer config file saved in models/OneShot/1/checkpoint-1284/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:40:04,188 >> Special tokens file saved in models/OneShot/1/checkpoint-1284/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:40:07,668 >> Deleting older checkpoint [models/OneShot/1/checkpoint-856] due to args.save_total_limit\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:40:07,688 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1177] due to args.save_total_limit\n",
            "\n",
            " 80% 1285/1605 [03:55<17:05,  3.21s/it]\u001b[A\n",
            " 80% 1286/1605 [03:56<12:59,  2.44s/it]\u001b[A\n",
            " 80% 1287/1605 [03:56<10:07,  1.91s/it]\u001b[A\n",
            " 80% 1288/1605 [03:57<08:08,  1.54s/it]\u001b[A\n",
            " 80% 1289/1605 [03:58<06:44,  1.28s/it]\u001b[A\n",
            " 80% 1290/1605 [03:58<05:45,  1.10s/it]\u001b[A\n",
            " 80% 1291/1605 [03:59<05:04,  1.03it/s]\u001b[A\n",
            " 80% 1292/1605 [04:00<04:34,  1.14it/s]\u001b[A\n",
            " 81% 1293/1605 [04:00<04:13,  1.23it/s]\u001b[A\n",
            " 81% 1294/1605 [04:01<03:59,  1.30it/s]\u001b[A\n",
            " 81% 1295/1605 [04:02<03:49,  1.35it/s]\u001b[A\n",
            " 81% 1296/1605 [04:02<03:41,  1.39it/s]\u001b[A\n",
            " 81% 1297/1605 [04:03<03:36,  1.42it/s]\u001b[A\n",
            " 81% 1298/1605 [04:04<03:33,  1.44it/s]\u001b[A\n",
            " 81% 1299/1605 [04:04<03:29,  1.46it/s]\u001b[A\n",
            " 81% 1300/1605 [04:05<03:27,  1.47it/s]\u001b[A\n",
            " 81% 1301/1605 [04:06<03:25,  1.48it/s]\u001b[A\n",
            " 81% 1302/1605 [04:06<03:24,  1.48it/s]\u001b[A\n",
            " 81% 1303/1605 [04:07<03:23,  1.48it/s]\u001b[A\n",
            " 81% 1304/1605 [04:08<03:22,  1.49it/s]\u001b[A\n",
            " 81% 1305/1605 [04:08<03:21,  1.49it/s]\u001b[A\n",
            " 81% 1306/1605 [04:09<03:20,  1.49it/s]\u001b[A\n",
            " 81% 1307/1605 [04:10<03:19,  1.49it/s]\u001b[A\n",
            " 81% 1308/1605 [04:10<03:18,  1.49it/s]\u001b[A\n",
            " 82% 1309/1605 [04:11<03:18,  1.49it/s]\u001b[A\n",
            " 82% 1310/1605 [04:12<03:18,  1.48it/s]\u001b[A\n",
            " 82% 1311/1605 [04:12<03:18,  1.48it/s]\u001b[A\n",
            " 82% 1312/1605 [04:13<03:17,  1.48it/s]\u001b[A\n",
            " 82% 1313/1605 [04:14<03:17,  1.48it/s]\u001b[A\n",
            " 82% 1314/1605 [04:14<03:16,  1.48it/s]\u001b[A\n",
            " 82% 1315/1605 [04:15<03:15,  1.48it/s]\u001b[A\n",
            " 82% 1316/1605 [04:16<03:16,  1.47it/s]\u001b[A\n",
            " 82% 1317/1605 [04:16<03:15,  1.47it/s]\u001b[A\n",
            " 82% 1318/1605 [04:17<03:15,  1.47it/s]\u001b[A\n",
            " 82% 1319/1605 [04:18<03:14,  1.47it/s]\u001b[A\n",
            " 82% 1320/1605 [04:18<03:14,  1.47it/s]\u001b[A\n",
            " 82% 1321/1605 [04:19<03:13,  1.47it/s]\u001b[A\n",
            " 82% 1322/1605 [04:20<03:12,  1.47it/s]\u001b[A\n",
            " 82% 1323/1605 [04:21<03:12,  1.46it/s]\u001b[A\n",
            " 82% 1324/1605 [04:21<03:12,  1.46it/s]\u001b[A\n",
            " 83% 1325/1605 [04:22<03:11,  1.46it/s]\u001b[A\n",
            " 83% 1326/1605 [04:23<03:11,  1.46it/s]\u001b[A\n",
            " 83% 1327/1605 [04:23<03:10,  1.46it/s]\u001b[A\n",
            " 83% 1328/1605 [04:24<03:09,  1.46it/s]\u001b[A\n",
            " 83% 1329/1605 [04:25<03:09,  1.46it/s]\u001b[A\n",
            " 83% 1330/1605 [04:25<03:08,  1.46it/s]\u001b[A\n",
            " 83% 1331/1605 [04:26<03:08,  1.45it/s]\u001b[A\n",
            " 83% 1332/1605 [04:27<03:07,  1.46it/s]\u001b[A\n",
            " 83% 1333/1605 [04:27<03:06,  1.46it/s]\u001b[A\n",
            " 83% 1334/1605 [04:28<03:05,  1.46it/s]\u001b[A\n",
            " 83% 1335/1605 [04:29<03:04,  1.46it/s]\u001b[A\n",
            " 83% 1336/1605 [04:29<03:04,  1.46it/s]\u001b[A\n",
            " 83% 1337/1605 [04:30<03:03,  1.46it/s]\u001b[A\n",
            " 83% 1338/1605 [04:31<03:02,  1.46it/s]\u001b[A\n",
            " 83% 1339/1605 [04:32<03:01,  1.46it/s]\u001b[A\n",
            " 83% 1340/1605 [04:32<03:01,  1.46it/s]\u001b[A\n",
            " 84% 1341/1605 [04:33<03:00,  1.47it/s]\u001b[A\n",
            " 84% 1342/1605 [04:34<02:58,  1.47it/s]\u001b[A\n",
            " 84% 1343/1605 [04:34<02:58,  1.47it/s]\u001b[A\n",
            " 84% 1344/1605 [04:35<02:57,  1.47it/s]\u001b[A\n",
            " 84% 1345/1605 [04:36<02:56,  1.47it/s]\u001b[A\n",
            " 84% 1346/1605 [04:36<02:56,  1.47it/s]\u001b[A\n",
            " 84% 1347/1605 [04:37<02:55,  1.47it/s]\u001b[A\n",
            " 84% 1348/1605 [04:38<02:54,  1.47it/s]\u001b[A\n",
            " 84% 1349/1605 [04:38<02:54,  1.47it/s]\u001b[A\n",
            " 84% 1350/1605 [04:39<02:53,  1.47it/s]\u001b[A\n",
            " 84% 1351/1605 [04:40<02:52,  1.47it/s]\u001b[A\n",
            " 84% 1352/1605 [04:40<02:51,  1.47it/s]\u001b[A\n",
            " 84% 1353/1605 [04:41<02:51,  1.47it/s]\u001b[A\n",
            " 84% 1354/1605 [04:42<02:50,  1.47it/s]\u001b[A\n",
            " 84% 1355/1605 [04:42<02:49,  1.47it/s]\u001b[A\n",
            " 84% 1356/1605 [04:43<02:49,  1.47it/s]\u001b[A\n",
            " 85% 1357/1605 [04:44<02:48,  1.47it/s]\u001b[A\n",
            " 85% 1358/1605 [04:44<02:47,  1.47it/s]\u001b[A\n",
            " 85% 1359/1605 [04:45<02:46,  1.48it/s]\u001b[A\n",
            " 85% 1360/1605 [04:46<02:46,  1.47it/s]\u001b[A\n",
            " 85% 1361/1605 [04:46<02:45,  1.48it/s]\u001b[A\n",
            " 85% 1362/1605 [04:47<02:44,  1.48it/s]\u001b[A\n",
            " 85% 1363/1605 [04:48<02:43,  1.48it/s]\u001b[A\n",
            " 85% 1364/1605 [04:48<02:42,  1.48it/s]\u001b[A\n",
            " 85% 1365/1605 [04:49<02:42,  1.48it/s]\u001b[A\n",
            " 85% 1366/1605 [04:50<02:41,  1.48it/s]\u001b[A\n",
            " 85% 1367/1605 [04:51<02:40,  1.48it/s]\u001b[A\n",
            " 85% 1368/1605 [04:51<02:39,  1.48it/s]\u001b[A\n",
            " 85% 1369/1605 [04:52<02:39,  1.48it/s]\u001b[A\n",
            " 85% 1370/1605 [04:53<02:38,  1.48it/s]\u001b[A\n",
            " 85% 1371/1605 [04:53<02:38,  1.48it/s]\u001b[A\n",
            " 85% 1372/1605 [04:54<02:37,  1.48it/s]\u001b[A\n",
            " 86% 1373/1605 [04:55<02:36,  1.48it/s]\u001b[A\n",
            " 86% 1374/1605 [04:55<02:35,  1.48it/s]\u001b[A\n",
            " 86% 1375/1605 [04:56<02:34,  1.49it/s]\u001b[A\n",
            " 86% 1376/1605 [04:57<02:34,  1.48it/s]\u001b[A\n",
            " 86% 1377/1605 [04:57<02:33,  1.48it/s]\u001b[A\n",
            " 86% 1378/1605 [04:58<02:32,  1.49it/s]\u001b[A\n",
            " 86% 1379/1605 [04:59<02:32,  1.48it/s]\u001b[A\n",
            " 86% 1380/1605 [04:59<02:31,  1.49it/s]\u001b[A\n",
            " 86% 1381/1605 [05:00<02:30,  1.48it/s]\u001b[A\n",
            " 86% 1382/1605 [05:01<02:29,  1.49it/s]\u001b[A\n",
            " 86% 1383/1605 [05:01<02:29,  1.49it/s]\u001b[A\n",
            " 86% 1384/1605 [05:02<02:28,  1.49it/s]\u001b[A\n",
            " 86% 1385/1605 [05:03<02:27,  1.49it/s]\u001b[A\n",
            " 86% 1386/1605 [05:03<02:26,  1.49it/s]\u001b[A\n",
            " 86% 1387/1605 [05:04<02:26,  1.49it/s]\u001b[A\n",
            " 86% 1388/1605 [05:05<02:25,  1.49it/s]\u001b[A\n",
            " 87% 1389/1605 [05:05<02:24,  1.50it/s]\u001b[A\n",
            " 87% 1390/1605 [05:06<02:23,  1.49it/s]\u001b[A\n",
            " 87% 1391/1605 [05:06<02:11,  1.62it/s]\u001b[A[INFO|trainer.py:662] 2022-07-26 06:41:20,076 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:41:20,078 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:41:20,078 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:41:20,078 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.177408218383789, 'eval_accuracy': 0.7982832789421082, 'eval_f1': 0.7908238305350219, 'eval_runtime': 3.8243, 'eval_samples_per_second': 121.851, 'eval_steps_per_second': 15.427, 'epoch': 13.0}\n",
            "100% 59/59 [00:03<00:00, 15.23it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:41:23,904 >> Saving model checkpoint to models/OneShot/1/checkpoint-1391\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:41:23,905 >> Configuration saved in models/OneShot/1/checkpoint-1391/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:41:24,941 >> Model weights saved in models/OneShot/1/checkpoint-1391/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:41:24,942 >> tokenizer config file saved in models/OneShot/1/checkpoint-1391/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:41:24,942 >> Special tokens file saved in models/OneShot/1/checkpoint-1391/special_tokens_map.json\n",
            "\n",
            " 87% 1392/1605 [05:16<11:28,  3.23s/it]\u001b[A\n",
            " 87% 1393/1605 [05:16<08:41,  2.46s/it]\u001b[A\n",
            " 87% 1394/1605 [05:17<06:45,  1.92s/it]\u001b[A\n",
            " 87% 1395/1605 [05:18<05:24,  1.54s/it]\u001b[A\n",
            " 87% 1396/1605 [05:18<04:27,  1.28s/it]\u001b[A\n",
            " 87% 1397/1605 [05:19<03:47,  1.09s/it]\u001b[A\n",
            " 87% 1398/1605 [05:20<03:20,  1.03it/s]\u001b[A\n",
            " 87% 1399/1605 [05:20<03:00,  1.14it/s]\u001b[A\n",
            " 87% 1400/1605 [05:21<02:47,  1.23it/s]\u001b[A\n",
            " 87% 1401/1605 [05:22<02:37,  1.30it/s]\u001b[A\n",
            " 87% 1402/1605 [05:22<02:30,  1.35it/s]\u001b[A\n",
            " 87% 1403/1605 [05:23<02:25,  1.39it/s]\u001b[A\n",
            " 87% 1404/1605 [05:24<02:21,  1.42it/s]\u001b[A\n",
            " 88% 1405/1605 [05:24<02:18,  1.44it/s]\u001b[A\n",
            " 88% 1406/1605 [05:25<02:16,  1.46it/s]\u001b[A\n",
            " 88% 1407/1605 [05:26<02:14,  1.47it/s]\u001b[A\n",
            " 88% 1408/1605 [05:26<02:13,  1.48it/s]\u001b[A\n",
            " 88% 1409/1605 [05:27<02:12,  1.48it/s]\u001b[A\n",
            " 88% 1410/1605 [05:28<02:11,  1.49it/s]\u001b[A\n",
            " 88% 1411/1605 [05:28<02:10,  1.49it/s]\u001b[A\n",
            " 88% 1412/1605 [05:29<02:09,  1.49it/s]\u001b[A\n",
            " 88% 1413/1605 [05:30<02:08,  1.49it/s]\u001b[A\n",
            " 88% 1414/1605 [05:30<02:08,  1.49it/s]\u001b[A\n",
            " 88% 1415/1605 [05:31<02:08,  1.48it/s]\u001b[A\n",
            " 88% 1416/1605 [05:32<02:07,  1.48it/s]\u001b[A\n",
            " 88% 1417/1605 [05:33<02:06,  1.48it/s]\u001b[A\n",
            " 88% 1418/1605 [05:33<02:06,  1.48it/s]\u001b[A\n",
            " 88% 1419/1605 [05:34<02:05,  1.48it/s]\u001b[A\n",
            " 88% 1420/1605 [05:35<02:04,  1.48it/s]\u001b[A\n",
            " 89% 1421/1605 [05:35<02:04,  1.48it/s]\u001b[A\n",
            " 89% 1422/1605 [05:36<02:03,  1.48it/s]\u001b[A\n",
            " 89% 1423/1605 [05:37<02:02,  1.48it/s]\u001b[A\n",
            " 89% 1424/1605 [05:37<02:02,  1.48it/s]\u001b[A\n",
            " 89% 1425/1605 [05:38<02:01,  1.48it/s]\u001b[A\n",
            " 89% 1426/1605 [05:39<02:01,  1.47it/s]\u001b[A\n",
            " 89% 1427/1605 [05:39<02:00,  1.47it/s]\u001b[A\n",
            " 89% 1428/1605 [05:40<02:00,  1.47it/s]\u001b[A\n",
            " 89% 1429/1605 [05:41<01:59,  1.47it/s]\u001b[A\n",
            " 89% 1430/1605 [05:41<01:58,  1.47it/s]\u001b[A\n",
            " 89% 1431/1605 [05:42<01:58,  1.47it/s]\u001b[A\n",
            " 89% 1432/1605 [05:43<01:57,  1.47it/s]\u001b[A\n",
            " 89% 1433/1605 [05:43<01:57,  1.47it/s]\u001b[A\n",
            " 89% 1434/1605 [05:44<01:56,  1.47it/s]\u001b[A\n",
            " 89% 1435/1605 [05:45<01:56,  1.46it/s]\u001b[A\n",
            " 89% 1436/1605 [05:45<01:55,  1.46it/s]\u001b[A\n",
            " 90% 1437/1605 [05:46<01:54,  1.46it/s]\u001b[A\n",
            " 90% 1438/1605 [05:47<01:54,  1.46it/s]\u001b[A\n",
            " 90% 1439/1605 [05:47<01:53,  1.46it/s]\u001b[A\n",
            " 90% 1440/1605 [05:48<01:52,  1.46it/s]\u001b[A\n",
            " 90% 1441/1605 [05:49<01:51,  1.47it/s]\u001b[A\n",
            " 90% 1442/1605 [05:50<01:51,  1.47it/s]\u001b[A\n",
            " 90% 1443/1605 [05:50<01:50,  1.47it/s]\u001b[A\n",
            " 90% 1444/1605 [05:51<01:49,  1.47it/s]\u001b[A\n",
            " 90% 1445/1605 [05:52<01:48,  1.47it/s]\u001b[A\n",
            " 90% 1446/1605 [05:52<01:48,  1.47it/s]\u001b[A\n",
            " 90% 1447/1605 [05:53<01:47,  1.47it/s]\u001b[A\n",
            " 90% 1448/1605 [05:54<01:46,  1.47it/s]\u001b[A\n",
            " 90% 1449/1605 [05:54<01:46,  1.47it/s]\u001b[A\n",
            " 90% 1450/1605 [05:55<01:45,  1.47it/s]\u001b[A\n",
            " 90% 1451/1605 [05:56<01:44,  1.47it/s]\u001b[A\n",
            " 90% 1452/1605 [05:56<01:43,  1.47it/s]\u001b[A\n",
            " 91% 1453/1605 [05:57<01:43,  1.47it/s]\u001b[A\n",
            " 91% 1454/1605 [05:58<01:42,  1.47it/s]\u001b[A\n",
            " 91% 1455/1605 [05:58<01:42,  1.47it/s]\u001b[A\n",
            " 91% 1456/1605 [05:59<01:41,  1.47it/s]\u001b[A\n",
            " 91% 1457/1605 [06:00<01:40,  1.47it/s]\u001b[A\n",
            " 91% 1458/1605 [06:00<01:39,  1.47it/s]\u001b[A\n",
            " 91% 1459/1605 [06:01<01:39,  1.47it/s]\u001b[A\n",
            " 91% 1460/1605 [06:02<01:38,  1.48it/s]\u001b[A\n",
            " 91% 1461/1605 [06:02<01:37,  1.47it/s]\u001b[A\n",
            " 91% 1462/1605 [06:03<01:37,  1.47it/s]\u001b[A\n",
            " 91% 1463/1605 [06:04<01:36,  1.47it/s]\u001b[A\n",
            " 91% 1464/1605 [06:04<01:35,  1.47it/s]\u001b[A\n",
            " 91% 1465/1605 [06:05<01:35,  1.47it/s]\u001b[A\n",
            " 91% 1466/1605 [06:06<01:34,  1.47it/s]\u001b[A\n",
            " 91% 1467/1605 [06:06<01:33,  1.47it/s]\u001b[A\n",
            " 91% 1468/1605 [06:07<01:32,  1.48it/s]\u001b[A\n",
            " 92% 1469/1605 [06:08<01:32,  1.47it/s]\u001b[A\n",
            " 92% 1470/1605 [06:09<01:31,  1.47it/s]\u001b[A\n",
            " 92% 1471/1605 [06:09<01:30,  1.48it/s]\u001b[A\n",
            " 92% 1472/1605 [06:10<01:30,  1.48it/s]\u001b[A\n",
            " 92% 1473/1605 [06:11<01:29,  1.47it/s]\u001b[A\n",
            " 92% 1474/1605 [06:11<01:29,  1.47it/s]\u001b[A\n",
            " 92% 1475/1605 [06:12<01:28,  1.46it/s]\u001b[A\n",
            " 92% 1476/1605 [06:13<01:28,  1.45it/s]\u001b[A\n",
            " 92% 1477/1605 [06:13<01:27,  1.46it/s]\u001b[A\n",
            " 92% 1478/1605 [06:14<01:26,  1.46it/s]\u001b[A\n",
            " 92% 1479/1605 [06:15<01:25,  1.47it/s]\u001b[A\n",
            " 92% 1480/1605 [06:15<01:24,  1.47it/s]\u001b[A\n",
            " 92% 1481/1605 [06:16<01:23,  1.48it/s]\u001b[A\n",
            " 92% 1482/1605 [06:17<01:23,  1.48it/s]\u001b[A\n",
            " 92% 1483/1605 [06:17<01:22,  1.48it/s]\u001b[A\n",
            " 92% 1484/1605 [06:18<01:21,  1.48it/s]\u001b[A\n",
            " 93% 1485/1605 [06:19<01:20,  1.48it/s]\u001b[A\n",
            " 93% 1486/1605 [06:19<01:20,  1.48it/s]\u001b[A\n",
            " 93% 1487/1605 [06:20<01:19,  1.49it/s]\u001b[A\n",
            " 93% 1488/1605 [06:21<01:18,  1.49it/s]\u001b[A\n",
            " 93% 1489/1605 [06:21<01:18,  1.48it/s]\u001b[A\n",
            " 93% 1490/1605 [06:22<01:17,  1.49it/s]\u001b[A\n",
            " 93% 1491/1605 [06:23<01:16,  1.48it/s]\u001b[A\n",
            " 93% 1492/1605 [06:23<01:16,  1.48it/s]\u001b[A\n",
            " 93% 1493/1605 [06:24<01:15,  1.49it/s]\u001b[A\n",
            " 93% 1494/1605 [06:25<01:14,  1.49it/s]\u001b[A\n",
            " 93% 1495/1605 [06:25<01:14,  1.49it/s]\u001b[A\n",
            " 93% 1496/1605 [06:26<01:13,  1.49it/s]\u001b[A\n",
            " 93% 1497/1605 [06:27<01:12,  1.49it/s]\u001b[A\n",
            " 93% 1498/1605 [06:27<01:05,  1.62it/s]\u001b[A[INFO|trainer.py:662] 2022-07-26 06:42:40,894 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:42:40,896 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:42:40,896 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:42:40,896 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1914654970169067, 'eval_accuracy': 0.8004291653633118, 'eval_f1': 0.7928740411499033, 'eval_runtime': 3.8313, 'eval_samples_per_second': 121.629, 'eval_steps_per_second': 15.399, 'epoch': 14.0}\n",
            "100% 59/59 [00:03<00:00, 15.16it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:42:44,729 >> Saving model checkpoint to models/OneShot/1/checkpoint-1498\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:42:44,730 >> Configuration saved in models/OneShot/1/checkpoint-1498/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:42:45,875 >> Model weights saved in models/OneShot/1/checkpoint-1498/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:42:45,875 >> tokenizer config file saved in models/OneShot/1/checkpoint-1498/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:42:45,876 >> Special tokens file saved in models/OneShot/1/checkpoint-1498/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:42:49,332 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1391] due to args.save_total_limit\n",
            "\n",
            " 93% 1499/1605 [06:37<05:42,  3.23s/it]\u001b[A\n",
            " 93% 1500/1605 [06:37<04:18,  2.46s/it]\u001b[A\n",
            "\u001b[A{'loss': 0.0019, 'learning_rate': 1.308411214953271e-06, 'epoch': 14.02}\n",
            "\n",
            " 93% 1500/1605 [06:37<04:18,  2.46s/it]\u001b[A\n",
            " 94% 1501/1605 [06:38<03:19,  1.92s/it]\u001b[A\n",
            " 94% 1502/1605 [06:39<02:39,  1.55s/it]\u001b[A\n",
            " 94% 1503/1605 [06:39<02:10,  1.28s/it]\u001b[A\n",
            " 94% 1504/1605 [06:40<01:50,  1.10s/it]\u001b[A\n",
            " 94% 1505/1605 [06:41<01:37,  1.03it/s]\u001b[A\n",
            " 94% 1506/1605 [06:41<01:27,  1.14it/s]\u001b[A\n",
            " 94% 1507/1605 [06:42<01:20,  1.22it/s]\u001b[A\n",
            " 94% 1508/1605 [06:43<01:14,  1.30it/s]\u001b[A\n",
            " 94% 1509/1605 [06:43<01:11,  1.35it/s]\u001b[A\n",
            " 94% 1510/1605 [06:44<01:08,  1.39it/s]\u001b[A\n",
            " 94% 1511/1605 [06:45<01:06,  1.42it/s]\u001b[A\n",
            " 94% 1512/1605 [06:45<01:04,  1.44it/s]\u001b[A\n",
            " 94% 1513/1605 [06:46<01:03,  1.46it/s]\u001b[A\n",
            " 94% 1514/1605 [06:47<01:02,  1.47it/s]\u001b[A\n",
            " 94% 1515/1605 [06:47<01:01,  1.47it/s]\u001b[A\n",
            " 94% 1516/1605 [06:48<01:00,  1.48it/s]\u001b[A\n",
            " 95% 1517/1605 [06:49<00:59,  1.48it/s]\u001b[A\n",
            " 95% 1518/1605 [06:49<00:58,  1.49it/s]\u001b[A\n",
            " 95% 1519/1605 [06:50<00:57,  1.49it/s]\u001b[A\n",
            " 95% 1520/1605 [06:51<00:56,  1.49it/s]\u001b[A\n",
            " 95% 1521/1605 [06:51<00:56,  1.49it/s]\u001b[A\n",
            " 95% 1522/1605 [06:52<00:55,  1.49it/s]\u001b[A\n",
            " 95% 1523/1605 [06:53<00:55,  1.49it/s]\u001b[A\n",
            " 95% 1524/1605 [06:53<00:54,  1.49it/s]\u001b[A\n",
            " 95% 1525/1605 [06:54<00:53,  1.49it/s]\u001b[A\n",
            " 95% 1526/1605 [06:55<00:53,  1.48it/s]\u001b[A\n",
            " 95% 1527/1605 [06:55<00:52,  1.48it/s]\u001b[A\n",
            " 95% 1528/1605 [06:56<00:51,  1.48it/s]\u001b[A\n",
            " 95% 1529/1605 [06:57<00:51,  1.48it/s]\u001b[A\n",
            " 95% 1530/1605 [06:57<00:50,  1.48it/s]\u001b[A\n",
            " 95% 1531/1605 [06:58<00:50,  1.48it/s]\u001b[A\n",
            " 95% 1532/1605 [06:59<00:49,  1.48it/s]\u001b[A\n",
            " 96% 1533/1605 [06:59<00:48,  1.48it/s]\u001b[A\n",
            " 96% 1534/1605 [07:00<00:48,  1.48it/s]\u001b[A\n",
            " 96% 1535/1605 [07:01<00:47,  1.47it/s]\u001b[A\n",
            " 96% 1536/1605 [07:01<00:46,  1.48it/s]\u001b[A\n",
            " 96% 1537/1605 [07:02<00:46,  1.47it/s]\u001b[A\n",
            " 96% 1538/1605 [07:03<00:45,  1.47it/s]\u001b[A\n",
            " 96% 1539/1605 [07:03<00:44,  1.47it/s]\u001b[A\n",
            " 96% 1540/1605 [07:04<00:44,  1.47it/s]\u001b[A\n",
            " 96% 1541/1605 [07:05<00:43,  1.47it/s]\u001b[A\n",
            " 96% 1542/1605 [07:06<00:42,  1.47it/s]\u001b[A\n",
            " 96% 1543/1605 [07:06<00:42,  1.47it/s]\u001b[A\n",
            " 96% 1544/1605 [07:07<00:41,  1.47it/s]\u001b[A\n",
            " 96% 1545/1605 [07:08<00:40,  1.47it/s]\u001b[A\n",
            " 96% 1546/1605 [07:08<00:40,  1.47it/s]\u001b[A\n",
            " 96% 1547/1605 [07:09<00:39,  1.47it/s]\u001b[A\n",
            " 96% 1548/1605 [07:10<00:38,  1.47it/s]\u001b[A\n",
            " 97% 1549/1605 [07:10<00:38,  1.46it/s]\u001b[A\n",
            " 97% 1550/1605 [07:11<00:37,  1.46it/s]\u001b[A\n",
            " 97% 1551/1605 [07:12<00:36,  1.47it/s]\u001b[A\n",
            " 97% 1552/1605 [07:12<00:36,  1.47it/s]\u001b[A\n",
            " 97% 1553/1605 [07:13<00:35,  1.47it/s]\u001b[A\n",
            " 97% 1554/1605 [07:14<00:34,  1.47it/s]\u001b[A\n",
            " 97% 1555/1605 [07:14<00:33,  1.47it/s]\u001b[A\n",
            " 97% 1556/1605 [07:15<00:33,  1.47it/s]\u001b[A\n",
            " 97% 1557/1605 [07:16<00:32,  1.47it/s]\u001b[A\n",
            " 97% 1558/1605 [07:16<00:31,  1.47it/s]\u001b[A\n",
            " 97% 1559/1605 [07:17<00:31,  1.47it/s]\u001b[A\n",
            " 97% 1560/1605 [07:18<00:30,  1.47it/s]\u001b[A\n",
            " 97% 1561/1605 [07:18<00:29,  1.47it/s]\u001b[A\n",
            " 97% 1562/1605 [07:19<00:29,  1.47it/s]\u001b[A\n",
            " 97% 1563/1605 [07:20<00:28,  1.47it/s]\u001b[A\n",
            " 97% 1564/1605 [07:20<00:27,  1.47it/s]\u001b[A\n",
            " 98% 1565/1605 [07:21<00:27,  1.47it/s]\u001b[A\n",
            " 98% 1566/1605 [07:22<00:26,  1.47it/s]\u001b[A\n",
            " 98% 1567/1605 [07:23<00:25,  1.47it/s]\u001b[A\n",
            " 98% 1568/1605 [07:23<00:25,  1.47it/s]\u001b[A\n",
            " 98% 1569/1605 [07:24<00:24,  1.47it/s]\u001b[A\n",
            " 98% 1570/1605 [07:25<00:23,  1.47it/s]\u001b[A\n",
            " 98% 1571/1605 [07:25<00:23,  1.47it/s]\u001b[A\n",
            " 98% 1572/1605 [07:26<00:22,  1.47it/s]\u001b[A\n",
            " 98% 1573/1605 [07:27<00:21,  1.48it/s]\u001b[A\n",
            " 98% 1574/1605 [07:27<00:21,  1.47it/s]\u001b[A\n",
            " 98% 1575/1605 [07:28<00:20,  1.48it/s]\u001b[A\n",
            " 98% 1576/1605 [07:29<00:19,  1.47it/s]\u001b[A\n",
            " 98% 1577/1605 [07:29<00:18,  1.48it/s]\u001b[A\n",
            " 98% 1578/1605 [07:30<00:18,  1.47it/s]\u001b[A\n",
            " 98% 1579/1605 [07:31<00:17,  1.47it/s]\u001b[A\n",
            " 98% 1580/1605 [07:31<00:16,  1.47it/s]\u001b[A\n",
            " 99% 1581/1605 [07:32<00:16,  1.48it/s]\u001b[A\n",
            " 99% 1582/1605 [07:33<00:15,  1.48it/s]\u001b[A\n",
            " 99% 1583/1605 [07:33<00:14,  1.48it/s]\u001b[A\n",
            " 99% 1584/1605 [07:34<00:14,  1.48it/s]\u001b[A\n",
            " 99% 1585/1605 [07:35<00:13,  1.48it/s]\u001b[A\n",
            " 99% 1586/1605 [07:35<00:12,  1.48it/s]\u001b[A\n",
            " 99% 1587/1605 [07:36<00:12,  1.48it/s]\u001b[A\n",
            " 99% 1588/1605 [07:37<00:11,  1.48it/s]\u001b[A\n",
            " 99% 1589/1605 [07:37<00:10,  1.48it/s]\u001b[A\n",
            " 99% 1590/1605 [07:38<00:10,  1.48it/s]\u001b[A\n",
            " 99% 1591/1605 [07:39<00:09,  1.48it/s]\u001b[A\n",
            " 99% 1592/1605 [07:39<00:08,  1.48it/s]\u001b[A\n",
            " 99% 1593/1605 [07:40<00:08,  1.49it/s]\u001b[A\n",
            " 99% 1594/1605 [07:41<00:07,  1.48it/s]\u001b[A\n",
            " 99% 1595/1605 [07:41<00:06,  1.48it/s]\u001b[A\n",
            " 99% 1596/1605 [07:42<00:06,  1.49it/s]\u001b[A\n",
            "100% 1597/1605 [07:43<00:05,  1.49it/s]\u001b[A\n",
            "100% 1598/1605 [07:43<00:04,  1.49it/s]\u001b[A\n",
            "100% 1599/1605 [07:44<00:04,  1.49it/s]\u001b[A\n",
            "100% 1600/1605 [07:45<00:03,  1.49it/s]\u001b[A\n",
            "100% 1601/1605 [07:45<00:02,  1.49it/s]\u001b[A\n",
            "100% 1602/1605 [07:46<00:02,  1.49it/s]\u001b[A\n",
            "100% 1603/1605 [07:47<00:01,  1.49it/s]\u001b[A\n",
            "100% 1604/1605 [07:48<00:00,  1.49it/s]\u001b[A\n",
            "100% 1605/1605 [07:48<00:00,  1.62it/s]\u001b[A[INFO|trainer.py:662] 2022-07-26 06:44:01,627 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:44:01,629 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:44:01,629 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:44:01,629 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1937546730041504, 'eval_accuracy': 0.8004291653633118, 'eval_f1': 0.7928740411499033, 'eval_runtime': 3.8294, 'eval_samples_per_second': 121.689, 'eval_steps_per_second': 15.407, 'epoch': 15.0}\n",
            "100% 59/59 [00:03<00:00, 15.15it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:44:05,460 >> Saving model checkpoint to models/OneShot/1/checkpoint-1605\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:44:05,461 >> Configuration saved in models/OneShot/1/checkpoint-1605/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:44:06,490 >> Model weights saved in models/OneShot/1/checkpoint-1605/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:44:06,490 >> tokenizer config file saved in models/OneShot/1/checkpoint-1605/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:44:06,490 >> Special tokens file saved in models/OneShot/1/checkpoint-1605/special_tokens_map.json\n",
            "[INFO|trainer.py:2581] 2022-07-26 06:44:10,072 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1498] due to args.save_total_limit\n",
            "[INFO|trainer.py:1761] 2022-07-26 06:44:10,280 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1834] 2022-07-26 06:44:10,281 >> Loading best model from models/OneShot/1/checkpoint-1284 (score: 0.8033192484390994).\n",
            "\n",
            "\u001b[A{'train_runtime': 477.7248, 'train_samples_per_second': 107.196, 'train_steps_per_second': 3.36, 'train_loss': 0.000632183585675706, 'epoch': 15.0}\n",
            "\n",
            "100% 1605/1605 [07:57<00:00,  3.36it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-26 06:44:10,836 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:446] 2022-07-26 06:44:10,837 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-26 06:44:12,163 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-26 06:44:12,164 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-26 06:44:12,164 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.0006\n",
            "  train_runtime            = 0:07:57.72\n",
            "  train_samples            =       3414\n",
            "  train_samples_per_second =    107.196\n",
            "  train_steps_per_second   =       3.36\n",
            "07/26/2022 06:44:12 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:662] 2022-07-26 06:44:12,209 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2753] 2022-07-26 06:44:12,211 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-26 06:44:12,211 >>   Num examples = 466\n",
            "[INFO|trainer.py:2758] 2022-07-26 06:44:12,211 >>   Batch size = 8\n",
            "100% 59/59 [00:03<00:00, 16.11it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       15.0\n",
            "  eval_accuracy           =     0.8133\n",
            "  eval_f1                 =     0.8033\n",
            "  eval_loss               =      1.135\n",
            "  eval_runtime            = 0:00:03.74\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    124.593\n",
            "  eval_steps_per_second   =     15.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW8stSsnIKWo",
        "outputId": "a797d11f-5b2f-4854-883f-fa1d592dafe1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0uO16BfcIur"
      },
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/OneShot/1/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}