{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adapter Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "do-TXGBemGgH",
        "LfqCJteZxeXM",
        "skzktsJWo29H",
        "VErwYC4h0kQQ",
        "wZJLePe_xqR_",
        "DMIhGZVs1SXr",
        "nTJPiYN2LAeK",
        "WY7Irn_YvIni",
        "Np_fganSL6oi",
        "HlsdRUjAGAMP",
        "LgoRNxSD2ygG",
        "AzC-WNAbREEv",
        "smQkFLCLEV_h"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedYaseen97/idiomaticity-detection/blob/main/adapter_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9b_p85gxaGc"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook provides a baseline for each setting in [Subtask A of SemEval 2022 Task 2](https://sites.google.com/view/semeval2022task2-idiomaticity#h.qq7eefmehqf9). In addition this provides some helpful pre-processing scripts that you are free to use with your experiments. \n",
        "\n",
        "Please start by stepping through this notebook so you have a clear idea as to what is expected of the task and what you need to submit. \n",
        "\n",
        "These baselines are based on the results described in the paper “[AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models](https://arxiv.org/abs/2109.04413)”. \n",
        "\n",
        "## Zero-shot setting: Methodology \n",
        "\n",
        "Note that in the zero-shot setting you are NOT allowed to train the model using the one-shot data. \n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”). This is based on the results presented in the dataset paper. \n",
        "\n",
        "We use Multilingual BERT for this setting.\n",
        "\n",
        "## One-shot setting: Methodology\n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”. Again, this is based on the results presented in the dataset paper. \n",
        "\n",
        "We also use Multilingual BERT for this setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivT35iewtNnV",
        "outputId": "49246490-b606-4f13-9cc9-aa38f5dcc790"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-TXGBemGgH"
      },
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WsITUAnzvFl"
      },
      "source": [
        "Download the Task data and evaluation scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3qhQdpl-1-",
        "outputId": "eb58aea4-6c63-4dcf-d3ea-7d07b97cf416"
      },
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval_2022_Task2-idiomaticity'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 123 (delta 48), reused 61 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 2.50 MiB | 10.77 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-0POB9tzfNx"
      },
      "source": [
        "Download the “AStitchInLanguageModels” code which we make use of. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affNQCRktdx4",
        "outputId": "616e97d2-4c8e-4cb2-e095-bf42d23844c6"
      },
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AStitchInLanguageModels'...\n",
            "remote: Enumerating objects: 1030, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/17)\u001b[K\rremote: Counting objects:  11% (2/17)\u001b[K\rremote: Counting objects:  17% (3/17)\u001b[K\rremote: Counting objects:  23% (4/17)\u001b[K\rremote: Counting objects:  29% (5/17)\u001b[K\rremote: Counting objects:  35% (6/17)\u001b[K\rremote: Counting objects:  41% (7/17)\u001b[K\rremote: Counting objects:  47% (8/17)\u001b[K\rremote: Counting objects:  52% (9/17)\u001b[K\rremote: Counting objects:  58% (10/17)\u001b[K\rremote: Counting objects:  64% (11/17)\u001b[K\rremote: Counting objects:  70% (12/17)\u001b[K\rremote: Counting objects:  76% (13/17)\u001b[K\rremote: Counting objects:  82% (14/17)\u001b[K\rremote: Counting objects:  88% (15/17)\u001b[K\rremote: Counting objects:  94% (16/17)\u001b[K\rremote: Counting objects: 100% (17/17)\u001b[K\rremote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 1030 (delta 11), reused 4 (delta 4), pack-reused 1013\u001b[K\n",
            "Receiving objects: 100% (1030/1030), 79.59 MiB | 45.20 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60w-An2vzikk"
      },
      "source": [
        "Download and install an editable version of huggingfaces transformers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8BhcLYcmVvd",
        "outputId": "717b00ee-dcdd-477c-f94c-a95014526a25"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!pip install --editable .\n",
        "%cd /content/ "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 106376, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 106376 (delta 96), reused 144 (delta 46), pack-reused 106148\u001b[K\n",
            "Receiving objects: 100% (106376/106376), 98.62 MiB | 29.42 MiB/s, done.\n",
            "Resolving deltas: 100% (78523/78523), done.\n",
            "/content/transformers\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.8.1\n",
            "  Downloading huggingface_hub-0.9.0-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.8.1->transformers==4.22.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.22.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.22.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.9.0 tokenizers-0.12.1 transformers\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVMnwTSzmjJ"
      },
      "source": [
        "Required for run_glue ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tsWits5tw1t",
        "outputId": "85805bd7-51df-4f1d-eef2-fc1492aaf41e"
      },
      "source": [
        "## run_glue needs this. \n",
        "!pip install datasets"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 28.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 63.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.4.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-igYdTTgzp9e"
      },
      "source": [
        "Editable install requires runtime restart unless we do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOuKplBmmbeB"
      },
      "source": [
        "import site\n",
        "site.main()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvC8kAGNnKk_"
      },
      "source": [
        "# Imports and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOw3MaG7nN77"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzDtW9eXnOhG"
      },
      "source": [
        "def load_csv( path, delimiter=',' ) : \n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile, delimiter=delimiter ) \n",
        "    for row in reader : \n",
        "      if header is None : \n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row ) \n",
        "  return header, data\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwtDsdtAnSZu"
      },
      "source": [
        "def write_csv( data, location ) : \n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile ) \n",
        "    writer.writerows( data ) \n",
        "  print( \"Wrote {}\".format( location ) ) \n",
        "  return\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44LyZ-OXmgQW"
      },
      "source": [
        "# Pre-process: Create train and dev and evaluation data in required format\n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”). \n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-3ymBcEmxaV"
      },
      "source": [
        "## Functions for pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MthVK7EQm6m_"
      },
      "source": [
        "### _get_train_data\n",
        "\n",
        "This function generates training data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPGq-Y1Jmvv5"
      },
      "source": [
        "def _get_train_data( data_location, file_name, include_context, include_idiom ) :\n",
        "    \n",
        "    file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "        \n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem in data :\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem[ header.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ header.index( 'Previous' ) ], elem[ header.index( 'Target' ) ], elem[ header.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ header.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        out_data.append( this_row )\n",
        "        assert len( out_header ) == len( this_row )\n",
        "    return [ out_header ] + out_data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytociCB3WZM"
      },
      "source": [
        "### _get_dev_eval_data\n",
        "\n",
        "This function generates training dev and eval data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters. \n",
        "\n",
        "Additionally, if there is no gold label provides (as in the case of eval) it will generate a file that can be used to generate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe4YQJ9Sm-B2"
      },
      "source": [
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None : \n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "    \n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None : \n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "            \n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "            \n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        assert len( out_header ) == len( this_row ) \n",
        "        out_data.append( this_row )\n",
        "        \n",
        "\n",
        "    return [ out_header ] + out_data\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIbyTnn3fHP"
      },
      "source": [
        "### create_data\n",
        "\n",
        "This function generates the training, development and evaluation data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1tr-zNvnBCV"
      },
      "source": [
        "\"\"\"\n",
        "Based on the results presented in `AStitchInLanguageModels' we work with not including the idiom for the zero shot setting and including it in the one shot setting.\n",
        "\"\"\"\n",
        "def create_data( input_location, output_location, lang=None) :\n",
        "\n",
        "    \n",
        "    ## Zero shot data\n",
        "    train_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv' if lang is None else 'train_zero_shot_'+lang.lower()+'.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False\n",
        "    )\n",
        "    if lang:\n",
        "      write_csv( train_data, os.path.join( output_location, 'ZeroShot', lang, 'train.csv' ) )\n",
        "    else:  \n",
        "      write_csv( train_data, os.path.join( output_location, 'ZeroShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv' if lang is None else 'dev_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = 'dev_gold.csv' if lang is None else 'dev_gold_'+lang.lower()+'.csv', \n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    ) \n",
        "    if lang:\n",
        "      write_csv( dev_data, os.path.join( output_location, 'ZeroShot', lang, 'dev.csv' ) )\n",
        "    else:  \n",
        "      write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv' if lang is None else 'eval_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    if lang:\n",
        "      write_csv( eval_data, os.path.join( output_location, 'ZeroShot', lang, 'eval.csv' ) )\n",
        "    else:  \n",
        "      write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "\n",
        "    ## OneShot Data (combine both for training)\n",
        "    train_zero_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv' if lang is None else 'train_zero_shot_'+lang.lower()+'.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "    train_one_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot.csv' if lang is None else 'train_one_shot_'+lang.lower()+'.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "\n",
        "    assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "    train_data = train_one_data + train_zero_data[1:]\n",
        "    if lang:\n",
        "      write_csv( train_data, os.path.join( output_location, 'OneShot', lang, 'train.csv' ) )\n",
        "    else:  \n",
        "      write_csv( train_data, os.path.join( output_location, 'OneShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv' if lang is None else 'dev_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = 'dev_gold.csv' if lang is None else 'dev_gold_'+lang.lower()+'.csv', \n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )        \n",
        "    if lang:\n",
        "      write_csv( dev_data, os.path.join( output_location, 'OneShot', lang, 'dev.csv' ) )\n",
        "    else:  \n",
        "      write_csv( dev_data, os.path.join( output_location, 'OneShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv' if lang is None else 'eval_'+lang.lower()+'.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\n",
        "    if lang:\n",
        "      write_csv( eval_data, os.path.join( output_location, 'OneShot', lang, 'eval.csv' ) )\n",
        "    else:  \n",
        "      write_csv( eval_data, os.path.join( output_location, 'OneShot', 'eval.csv' ) )\n",
        "\n",
        "    return"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmQfvym8ndKH"
      },
      "source": [
        "## Setup and Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxCgaHlKnpMR",
        "outputId": "86c44f49-cc94-4336-a0d3-a4f76ed8de1a"
      },
      "source": [
        "!ls "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AStitchInLanguageModels  SemEval_2022_Task2-idiomaticity\n",
            "sample_data\t\t transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_lang( data_location, file_name, lang ) :\n",
        "  # Filter csv files by language\n",
        "    \n",
        "  file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "  header, data = load_csv( file_name )\n",
        "\n",
        "  # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "  out_data = list()\n",
        "  for elem in data :\n",
        "      if elem[ header.index( 'Language'  ) ] == lang:\n",
        "        out_data.append( elem )\n",
        "  # write_csv( eval_data, os.path.join( data_location, file_name+\"_\"+lang.tolower()+\".csv\" ) )\n",
        "  return [header] + out_data"
      ],
      "metadata": {
        "id": "OYbvrA_98voB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lang_input(file_location, lang):\n",
        "  train0_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'train_zero_shot.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( train0_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"train_zero_shot_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  train1_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'train_one_shot.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( train1_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"train_one_shot_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  dev_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'dev.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( dev_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"dev_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  dev_gold_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'dev_gold.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( dev_gold_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"dev_gold_\"+lang.lower()+\".csv\" ) )\n",
        "\n",
        "  eval_lang_data = _get_lang(\n",
        "      data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "      file_name  = 'eval.csv',\n",
        "      lang=lang\n",
        "  )\n",
        "  write_csv( eval_lang_data, os.path.join( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', \"eval_\"+lang.lower()+\".csv\" ) )"
      ],
      "metadata": {
        "id": "e3sgi86tO5Xe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'EN'"
      ],
      "metadata": {
        "id": "Q35MAQgVuYKa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_lang_input('SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', lang=language)"
      ],
      "metadata": {
        "id": "Ve6NW5FdTsv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1de1326-60fb-4506-8a2b-2f39194aa1c8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote SemEval_2022_Task2-idiomaticity/SubTaskA/Data/train_zero_shot_en.csv\n",
            "Wrote SemEval_2022_Task2-idiomaticity/SubTaskA/Data/train_one_shot_en.csv\n",
            "Wrote SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_en.csv\n",
            "Wrote SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_gold_en.csv\n",
            "Wrote SemEval_2022_Task2-idiomaticity/SubTaskA/Data/eval_en.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkeKLg-Hngs4",
        "outputId": "ab3cc580-4266-4c3a-8411-4a4d856dcb69"
      },
      "source": [
        "outpath = 'Data'\n",
        "\n",
        "if language:    \n",
        "  Path( os.path.join( outpath, 'ZeroShot', language ) ).mkdir(parents=True, exist_ok=True)\n",
        "  Path( os.path.join( outpath, 'OneShot', language ) ).mkdir(parents=True, exist_ok=True)\n",
        "else:    \n",
        "  Path( os.path.join( outpath, 'ZeroShot') ).mkdir(parents=True, exist_ok=True)\n",
        "  Path( os.path.join( outpath, 'OneShot') ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', outpath, lang=language)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/EN/train.csv\n",
            "Wrote Data/ZeroShot/EN/dev.csv\n",
            "Wrote Data/ZeroShot/EN/eval.csv\n",
            "Wrote Data/OneShot/EN/train.csv\n",
            "Wrote Data/OneShot/EN/dev.csv\n",
            "Wrote Data/OneShot/EN/eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-Ol7hfoC8a"
      },
      "source": [
        "# Zero Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline EN : Monolingual BERT for zero-shot english idiomaticity detection "
      ],
      "metadata": {
        "id": "LfqCJteZxeXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPRUmDjokHx_",
        "outputId": "2fcf82ab-a340-4848-9d5c-336291899d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k0BwA0uoKAu",
        "outputId": "6d08908c-63b1-4dc0-d325-84d632c00ac4"
      },
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 25 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/24/2022 23:38:57 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/24/2022 23:38:57 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Aug24_23-38-57_93ca22c7e237,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/24/2022 23:38:57 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "08/24/2022 23:38:57 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "08/24/2022 23:38:57 - WARNING - datasets.builder -   Using custom data configuration default-435889a51633a83c\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-435889a51633a83c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 10318.09it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1453.58it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-435889a51633a83c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 631.39it/s]\n",
            "[INFO|hub.py:600] 2022-08-24 23:38:58,099 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu8rme95b\n",
            "Downloading config.json: 100% 570/570 [00:00<00:00, 957kB/s]\n",
            "[INFO|hub.py:613] 2022-08-24 23:38:58,223 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|hub.py:621] 2022-08-24 23:38:58,223 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:681] 2022-08-24 23:38:58,224 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-24 23:38:58,226 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2022-08-24 23:38:58,366 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptrgkqz2r\n",
            "Downloading tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 55.1kB/s]\n",
            "[INFO|hub.py:613] 2022-08-24 23:38:58,498 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|hub.py:621] 2022-08-24 23:38:58,498 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-24 23:38:58,626 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-24 23:38:58,627 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2022-08-24 23:38:58,897 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphe1hcne4\n",
            "Downloading vocab.txt: 100% 208k/208k [00:00<00:00, 1.88MB/s]\n",
            "[INFO|hub.py:613] 2022-08-24 23:38:59,146 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|hub.py:621] 2022-08-24 23:38:59,146 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|hub.py:600] 2022-08-24 23:38:59,276 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7zca8uzi\n",
            "Downloading tokenizer.json: 100% 426k/426k [00:00<00:00, 3.05MB/s]\n",
            "[INFO|hub.py:613] 2022-08-24 23:38:59,558 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|hub.py:621] 2022-08-24 23:38:59,558 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-24 23:38:59,951 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-24 23:38:59,951 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-24 23:38:59,951 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-24 23:38:59,951 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-24 23:38:59,951 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-24 23:39:00,070 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-24 23:39:00,071 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2022-08-24 23:39:00,248 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxwgy0xoq\n",
            "Downloading pytorch_model.bin: 100% 416M/416M [00:15<00:00, 28.6MB/s]\n",
            "[INFO|hub.py:613] 2022-08-24 23:39:15,589 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|hub.py:621] 2022-08-24 23:39:15,590 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|modeling_utils.py:2041] 2022-08-24 23:39:15,590 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:2426] 2022-08-24 23:39:17,164 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2438] 2022-08-24 23:39:17,164 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  4.36ba/s]\n",
            "100% 1/1 [00:00<00:00, 10.12ba/s]\n",
            "08/24/2022 23:39:18 - INFO - __main__ -   Sample 1577 of the training set: {'label': 1, 'sentence1': 'Where do I stream Stag Night online? Stag Night is available to watch and stream, download, buy on demand at Amazon Prime, Amazon, Vudu, Google Play, iTunes, YouTube VOD online. Some platforms allow you to rent Stag Night for a limited time or purchase the movie and download it to your device.', 'input_ids': [101, 2777, 1202, 146, 5118, 1457, 8517, 3259, 3294, 136, 1457, 8517, 3259, 1110, 1907, 1106, 2824, 1105, 5118, 117, 9133, 117, 4417, 1113, 4555, 1120, 9786, 3460, 117, 9786, 117, 159, 4867, 1358, 117, 7986, 6060, 117, 12145, 117, 7673, 159, 15609, 3294, 119, 1789, 6833, 2621, 1128, 1106, 9795, 1457, 8517, 3259, 1111, 170, 2609, 1159, 1137, 4779, 1103, 2523, 1105, 9133, 1122, 1106, 1240, 4442, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/24/2022 23:39:18 - INFO - __main__ -   Sample 3104 of the training set: {'label': 1, 'sentence1': 'Did Biden think for himself before he reversed direction on the Keystone Pipeline or did he do it just because the last Democrat president, under whom he served, was against it? I applaud Biden’s decisions to rejoin the World Health Organization and to call a world conference on climate change because, for whatever reason, the climate is changing. The atmosphere is getting warmer and the storms are getting bigger, as evidenced by the one in California earlier this week.', 'input_ids': [101, 2966, 139, 26859, 1341, 1111, 1471, 1196, 1119, 11802, 2447, 1113, 1103, 17142, 4793, 21902, 10522, 2042, 1137, 1225, 1119, 1202, 1122, 1198, 1272, 1103, 1314, 7319, 2084, 117, 1223, 2292, 1119, 1462, 117, 1108, 1222, 1122, 136, 146, 12647, 15554, 1181, 139, 26859, 787, 188, 6134, 1106, 1231, 25665, 1103, 1291, 3225, 6534, 1105, 1106, 1840, 170, 1362, 3511, 1113, 4530, 1849, 1272, 117, 1111, 3451, 2255, 117, 1103, 4530, 1110, 4787, 119, 1109, 6814, 1110, 2033, 18153, 1105, 1103, 14041, 1132, 2033, 6706, 117, 1112, 23526, 1118, 1103, 1141, 1107, 1756, 2206, 1142, 1989, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/24/2022 23:39:18 - INFO - __main__ -   Sample 1722 of the training set: {'label': 1, 'sentence1': 'Mank leads the Critics Choice Awards 2021 nominations Coronavirus latest: Covid national research project will study effects of emerging mutations Jared Kushner and Ivanka Trump made up to $640 million while working in White House, report finds', 'input_ids': [101, 2268, 1377, 4501, 1103, 10320, 10373, 2763, 17881, 1475, 10394, 3291, 15789, 27608, 6270, 131, 3291, 18312, 1569, 1844, 1933, 1209, 2025, 3154, 1104, 8999, 17157, 7927, 23209, 2737, 2511, 1105, 7062, 1968, 8499, 1189, 1146, 1106, 109, 21451, 1550, 1229, 1684, 1107, 2061, 1585, 117, 2592, 4090, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:723] 2022-08-24 23:39:22,260 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-08-24 23:39:22,266 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-08-24 23:39:22,266 >>   Num examples = 3327\n",
            "[INFO|trainer.py:1607] 2022-08-24 23:39:22,266 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1608] 2022-08-24 23:39:22,266 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1609] 2022-08-24 23:39:22,266 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1610] 2022-08-24 23:39:22,266 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-08-24 23:39:22,266 >>   Total optimization steps = 2600\n",
            "  4% 104/2600 [01:06<27:11,  1.53it/s][INFO|trainer.py:723] 2022-08-24 23:40:28,585 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:40:28,587 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:40:28,587 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:40:28,587 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 23.35it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 18.41it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:02, 17.76it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:02, 17.11it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 16.97it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 16.72it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 16.29it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 16.31it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 16.10it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 16.08it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 16.18it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 16.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:01, 16.22it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 16.24it/s]\u001b[A\n",
            " 54% 32/59 [00:01<00:01, 16.28it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 16.39it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 16.39it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 16.30it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 16.42it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 16.30it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 16.29it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 16.13it/s]\u001b[A\n",
            " 81% 48/59 [00:02<00:00, 16.02it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.97it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 16.01it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.95it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 16.14it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6372513771057129, 'eval_accuracy': 0.6630901098251343, 'eval_f1': 0.6627406871309311, 'eval_runtime': 3.5983, 'eval_samples_per_second': 129.506, 'eval_steps_per_second': 16.397, 'epoch': 1.0}\n",
            "  4% 104/2600 [01:09<27:11,  1.53it/s]\n",
            "100% 59/59 [00:03<00:00, 16.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:40:32,186 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-104\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:40:32,187 >> Configuration saved in models/ZeroShot/0/checkpoint-104/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:40:33,330 >> Model weights saved in models/ZeroShot/0/checkpoint-104/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:40:33,330 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-104/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:40:33,331 >> Special tokens file saved in models/ZeroShot/0/checkpoint-104/special_tokens_map.json\n",
            "  8% 208/2600 [02:25<27:51,  1.43it/s][INFO|trainer.py:723] 2022-08-24 23:41:48,056 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:41:48,058 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:41:48,058 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:41:48,058 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.02it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.97it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 15.94it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.49it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.36it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.08it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.00it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 14.93it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 14.96it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 14.98it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.98it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.10it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 14.99it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 14.94it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 14.93it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 14.87it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 14.88it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.86it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.91it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 15.03it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 14.94it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.03it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.00it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 14.95it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9293813705444336, 'eval_accuracy': 0.6351931095123291, 'eval_f1': 0.6338712957314255, 'eval_runtime': 3.91, 'eval_samples_per_second': 119.183, 'eval_steps_per_second': 15.09, 'epoch': 2.0}\n",
            "  8% 208/2600 [02:29<27:51,  1.43it/s]\n",
            "100% 59/59 [00:03<00:00, 14.86it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:41:51,970 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-208\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:41:51,970 >> Configuration saved in models/ZeroShot/0/checkpoint-208/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:41:53,002 >> Model weights saved in models/ZeroShot/0/checkpoint-208/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:41:53,003 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-208/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:41:53,003 >> Special tokens file saved in models/ZeroShot/0/checkpoint-208/special_tokens_map.json\n",
            " 12% 312/2600 [03:46<26:18,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:43:08,542 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:43:08,544 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:43:08,544 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:43:08,544 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.27it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.12it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.14it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.66it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.49it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.28it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.00it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 14.99it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.99it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 14.99it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 14.98it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.13it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 14.97it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.02it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.01it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.109946846961975, 'eval_accuracy': 0.6201716661453247, 'eval_f1': 0.6182572998190377, 'eval_runtime': 3.8808, 'eval_samples_per_second': 120.078, 'eval_steps_per_second': 15.203, 'epoch': 3.0}\n",
            " 12% 312/2600 [03:50<26:18,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 14.97it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:43:12,426 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-312\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:43:12,427 >> Configuration saved in models/ZeroShot/0/checkpoint-312/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:43:13,526 >> Model weights saved in models/ZeroShot/0/checkpoint-312/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:43:13,527 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:43:13,527 >> Special tokens file saved in models/ZeroShot/0/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:43:16,910 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-208] due to args.save_total_limit\n",
            " 16% 416/2600 [05:07<25:07,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:44:29,517 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:44:29,519 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:44:29,519 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:44:29,519 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.17it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.42it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.43it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.72it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.43it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.32it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.05it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.04it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.01it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.04it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 14.94it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 14.95it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.06it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 14.92it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2672836780548096, 'eval_accuracy': 0.6995708346366882, 'eval_f1': 0.693795175068056, 'eval_runtime': 3.8802, 'eval_samples_per_second': 120.097, 'eval_steps_per_second': 15.205, 'epoch': 4.0}\n",
            " 16% 416/2600 [05:11<25:07,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 14.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:44:33,401 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-416\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:44:33,402 >> Configuration saved in models/ZeroShot/0/checkpoint-416/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:44:34,451 >> Model weights saved in models/ZeroShot/0/checkpoint-416/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:44:34,451 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-416/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:44:34,452 >> Special tokens file saved in models/ZeroShot/0/checkpoint-416/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:44:37,960 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-104] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:44:38,035 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-312] due to args.save_total_limit\n",
            "{'loss': 0.2418, 'learning_rate': 1.6153846153846154e-05, 'epoch': 4.81}\n",
            " 20% 520/2600 [06:28<23:57,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:45:50,817 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:45:50,819 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:45:50,819 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:45:50,819 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.25it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.59it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.36it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.85it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.67it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.30it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.46it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.16it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.06it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.3772106170654297, 'eval_accuracy': 0.721030056476593, 'eval_f1': 0.6966935693829731, 'eval_runtime': 3.8497, 'eval_samples_per_second': 121.048, 'eval_steps_per_second': 15.326, 'epoch': 5.0}\n",
            " 20% 520/2600 [06:32<23:57,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:45:54,670 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-520\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:45:54,671 >> Configuration saved in models/ZeroShot/0/checkpoint-520/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:45:55,686 >> Model weights saved in models/ZeroShot/0/checkpoint-520/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:45:55,687 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:45:55,687 >> Special tokens file saved in models/ZeroShot/0/checkpoint-520/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:45:59,396 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-416] due to args.save_total_limit\n",
            " 24% 624/2600 [07:49<22:41,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:47:12,222 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:47:12,224 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:47:12,224 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:47:12,224 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.41it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.14it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.35it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.81it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.75it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.42it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.99it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.03it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.5218534469604492, 'eval_accuracy': 0.729613721370697, 'eval_f1': 0.7214029493822239, 'eval_runtime': 3.8592, 'eval_samples_per_second': 120.75, 'eval_steps_per_second': 15.288, 'epoch': 6.0}\n",
            " 24% 624/2600 [07:53<22:41,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:47:16,085 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-624\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:47:16,085 >> Configuration saved in models/ZeroShot/0/checkpoint-624/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:47:17,095 >> Model weights saved in models/ZeroShot/0/checkpoint-624/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:47:17,096 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:47:17,096 >> Special tokens file saved in models/ZeroShot/0/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:47:20,576 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-520] due to args.save_total_limit\n",
            " 28% 728/2600 [09:11<21:25,  1.46it/s][INFO|trainer.py:723] 2022-08-24 23:48:33,496 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:48:33,498 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:48:33,498 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:48:33,498 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.95it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.52it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.57it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.03it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.76it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.43it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.59it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.58it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.49it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.41it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.07it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.04it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.04it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.00it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.32it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.6465857028961182, 'eval_accuracy': 0.7038626670837402, 'eval_f1': 0.6877039627039626, 'eval_runtime': 3.8456, 'eval_samples_per_second': 121.177, 'eval_steps_per_second': 15.342, 'epoch': 7.0}\n",
            " 28% 728/2600 [09:15<21:25,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:48:37,345 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-728\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:48:37,347 >> Configuration saved in models/ZeroShot/0/checkpoint-728/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:48:38,361 >> Model weights saved in models/ZeroShot/0/checkpoint-728/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:48:38,362 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-728/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:48:38,362 >> Special tokens file saved in models/ZeroShot/0/checkpoint-728/special_tokens_map.json\n",
            " 32% 832/2600 [10:32<20:14,  1.46it/s][INFO|trainer.py:723] 2022-08-24 23:49:54,619 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:49:54,621 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:49:54,621 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:49:54,622 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.01it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.42it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.51it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.84it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.76it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.27it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.11it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.37it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.35it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 14.89it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.97it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.36it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.33it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.41it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.718380331993103, 'eval_accuracy': 0.7103004455566406, 'eval_f1': 0.6998382533267807, 'eval_runtime': 3.8363, 'eval_samples_per_second': 121.473, 'eval_steps_per_second': 15.38, 'epoch': 8.0}\n",
            " 32% 832/2600 [10:36<20:14,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:49:58,459 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-832\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:49:58,460 >> Configuration saved in models/ZeroShot/0/checkpoint-832/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:49:59,648 >> Model weights saved in models/ZeroShot/0/checkpoint-832/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:49:59,648 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-832/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:49:59,648 >> Special tokens file saved in models/ZeroShot/0/checkpoint-832/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:50:03,126 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-728] due to args.save_total_limit\n",
            " 36% 936/2600 [11:53<19:01,  1.46it/s][INFO|trainer.py:723] 2022-08-24 23:51:15,896 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:51:15,898 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:51:15,898 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:51:15,898 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.34it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.30it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.31it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.75it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.80it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.70it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.64it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.29it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.26it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.40it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8871948719024658, 'eval_accuracy': 0.7038626670837402, 'eval_f1': 0.6988705538594093, 'eval_runtime': 3.8319, 'eval_samples_per_second': 121.609, 'eval_steps_per_second': 15.397, 'epoch': 9.0}\n",
            " 36% 936/2600 [11:57<19:01,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:51:19,731 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-936\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:51:19,732 >> Configuration saved in models/ZeroShot/0/checkpoint-936/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:51:20,799 >> Model weights saved in models/ZeroShot/0/checkpoint-936/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:51:20,800 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:51:20,800 >> Special tokens file saved in models/ZeroShot/0/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:51:24,297 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-832] due to args.save_total_limit\n",
            "{'loss': 0.014, 'learning_rate': 1.230769230769231e-05, 'epoch': 9.62}\n",
            " 40% 1040/2600 [13:14<17:48,  1.46it/s][INFO|trainer.py:723] 2022-08-24 23:52:37,085 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:52:37,086 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:52:37,086 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:52:37,086 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.68it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.25it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.27it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.93it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.57it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.26it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.41it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.19it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.963460087776184, 'eval_accuracy': 0.6909871101379395, 'eval_f1': 0.688207415667689, 'eval_runtime': 3.8432, 'eval_samples_per_second': 121.254, 'eval_steps_per_second': 15.352, 'epoch': 10.0}\n",
            " 40% 1040/2600 [13:18<17:48,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:52:40,931 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1040\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:52:40,932 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:52:41,903 >> Model weights saved in models/ZeroShot/0/checkpoint-1040/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:52:41,904 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1040/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:52:41,904 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1040/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:52:45,583 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-936] due to args.save_total_limit\n",
            " 44% 1144/2600 [14:36<16:43,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:53:58,359 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:53:58,361 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:53:58,361 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:53:58,361 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.00it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.35it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.41it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.93it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.81it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.38it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.52it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.13it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.06it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.03it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.8020488023757935, 'eval_accuracy': 0.721030056476593, 'eval_f1': 0.7125585985689613, 'eval_runtime': 3.8433, 'eval_samples_per_second': 121.25, 'eval_steps_per_second': 15.351, 'epoch': 11.0}\n",
            " 44% 1144/2600 [14:39<16:43,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:54:02,206 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1144\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:54:02,207 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:54:03,343 >> Model weights saved in models/ZeroShot/0/checkpoint-1144/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:54:03,344 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1144/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:54:03,344 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1144/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:54:06,875 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1040] due to args.save_total_limit\n",
            " 48% 1248/2600 [15:57<15:33,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:55:19,683 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:55:19,685 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:55:19,685 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:55:19,685 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.69it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.30it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.47it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.88it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.85it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.31it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.16it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.25it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.8919192552566528, 'eval_accuracy': 0.7231759428977966, 'eval_f1': 0.7083699200993543, 'eval_runtime': 3.8486, 'eval_samples_per_second': 121.083, 'eval_steps_per_second': 15.33, 'epoch': 12.0}\n",
            " 48% 1248/2600 [16:01<15:33,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 14.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:55:23,535 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1248\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:55:23,536 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:55:24,526 >> Model weights saved in models/ZeroShot/0/checkpoint-1248/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:55:24,526 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1248/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:55:24,526 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1248/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:55:28,105 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1144] due to args.save_total_limit\n",
            " 52% 1352/2600 [17:18<14:23,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:56:40,931 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:56:40,933 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:56:40,933 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:56:40,933 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.64it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 17.73it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.55it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.11it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.71it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.49it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.51it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.21it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.16it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.06it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.0028178691864014, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6942127301996439, 'eval_runtime': 3.8454, 'eval_samples_per_second': 121.184, 'eval_steps_per_second': 15.343, 'epoch': 13.0}\n",
            " 52% 1352/2600 [17:22<14:23,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:56:44,780 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1352\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:56:44,781 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:56:46,083 >> Model weights saved in models/ZeroShot/0/checkpoint-1352/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:56:46,084 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1352/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:56:46,085 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1352/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:56:49,420 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1248] due to args.save_total_limit\n",
            " 56% 1456/2600 [18:40<13:05,  1.46it/s][INFO|trainer.py:723] 2022-08-24 23:58:02,313 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:58:02,315 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:58:02,315 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:58:02,315 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.46it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.53it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.40it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.92it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.78it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.69it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.58it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.54it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.24it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.37it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.18it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.34it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.039782762527466, 'eval_accuracy': 0.7038626670837402, 'eval_f1': 0.6939171822941457, 'eval_runtime': 3.826, 'eval_samples_per_second': 121.798, 'eval_steps_per_second': 15.421, 'epoch': 14.0}\n",
            " 56% 1456/2600 [18:43<13:05,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:58:06,143 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1456\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:58:06,143 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:58:07,218 >> Model weights saved in models/ZeroShot/0/checkpoint-1456/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:58:07,218 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1456/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:58:07,219 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1456/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:58:10,665 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1352] due to args.save_total_limit\n",
            "{'loss': 0.0043, 'learning_rate': 8.461538461538462e-06, 'epoch': 14.42}\n",
            " 60% 1560/2600 [20:01<11:55,  1.45it/s][INFO|trainer.py:723] 2022-08-24 23:59:23,560 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-24 23:59:23,562 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-24 23:59:23,562 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-24 23:59:23,562 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.81it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.18it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.17it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.80it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.56it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.44it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.59it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.05it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.18it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.17it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.07it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.079385280609131, 'eval_accuracy': 0.7081544995307922, 'eval_f1': 0.6973621377953507, 'eval_runtime': 3.8459, 'eval_samples_per_second': 121.169, 'eval_steps_per_second': 15.341, 'epoch': 15.0}\n",
            " 60% 1560/2600 [20:05<11:55,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-24 23:59:27,409 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1560\n",
            "[INFO|configuration_utils.py:451] 2022-08-24 23:59:27,410 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-24 23:59:28,528 >> Model weights saved in models/ZeroShot/0/checkpoint-1560/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-24 23:59:28,528 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-24 23:59:28,529 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1560/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-24 23:59:32,041 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1456] due to args.save_total_limit\n",
            " 64% 1664/2600 [21:22<10:47,  1.45it/s][INFO|trainer.py:723] 2022-08-25 00:00:44,743 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:00:44,744 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:00:44,744 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:00:44,745 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.40it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.18it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.15it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.80it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.56it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.41it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.36it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.48it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.03it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.09it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.20it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1342406272888184, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6946279141031988, 'eval_runtime': 3.8467, 'eval_samples_per_second': 121.142, 'eval_steps_per_second': 15.338, 'epoch': 16.0}\n",
            " 64% 1664/2600 [21:26<10:47,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:00:48,593 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1664\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:00:48,593 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:00:49,579 >> Model weights saved in models/ZeroShot/0/checkpoint-1664/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:00:49,579 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1664/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:00:49,580 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1664/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:00:53,131 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1560] due to args.save_total_limit\n",
            " 68% 1768/2600 [22:43<09:32,  1.45it/s][INFO|trainer.py:723] 2022-08-25 00:02:05,873 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:02:05,874 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:02:05,875 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:02:05,875 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.30it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.14it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.29it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.82it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.48it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.40it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.15it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.00it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 14.96it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.04it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.02it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.24it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.24it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.08it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1815288066864014, 'eval_accuracy': 0.7060086131095886, 'eval_f1': 0.6943530819868342, 'eval_runtime': 3.8672, 'eval_samples_per_second': 120.5, 'eval_steps_per_second': 15.256, 'epoch': 17.0}\n",
            " 68% 1768/2600 [22:47<09:32,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 14.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:02:09,743 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1768\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:02:09,744 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:02:10,844 >> Model weights saved in models/ZeroShot/0/checkpoint-1768/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:02:10,845 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1768/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:02:10,845 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1768/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:02:14,359 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1664] due to args.save_total_limit\n",
            " 72% 1872/2600 [24:04<08:19,  1.46it/s][INFO|trainer.py:723] 2022-08-25 00:03:27,135 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:03:27,137 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:03:27,137 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:03:27,137 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.44it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.27it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.37it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.95it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.63it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.08it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.20it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.02it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.23it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.34it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.2066121101379395, 'eval_accuracy': 0.7081544995307922, 'eval_f1': 0.6940663075170403, 'eval_runtime': 3.8405, 'eval_samples_per_second': 121.338, 'eval_steps_per_second': 15.363, 'epoch': 18.0}\n",
            " 72% 1872/2600 [24:08<08:19,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:03:30,979 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1872\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:03:30,980 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:03:31,967 >> Model weights saved in models/ZeroShot/0/checkpoint-1872/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:03:31,967 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1872/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:03:31,968 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1872/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:03:35,528 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1768] due to args.save_total_limit\n",
            " 76% 1976/2600 [25:26<07:11,  1.45it/s][INFO|trainer.py:723] 2022-08-25 00:04:48,389 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:04:48,391 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:04:48,391 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:04:48,391 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.48it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.62it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.13it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.57it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.38it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.21it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 14.97it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.03it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.97it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.07it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.30it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.00it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.96it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.10it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.32it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.05it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.241943836212158, 'eval_accuracy': 0.7188841104507446, 'eval_f1': 0.7019747601728219, 'eval_runtime': 3.8701, 'eval_samples_per_second': 120.41, 'eval_steps_per_second': 15.245, 'epoch': 19.0}\n",
            " 76% 1976/2600 [25:29<07:11,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:04:52,263 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1976\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:04:52,263 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:04:53,333 >> Model weights saved in models/ZeroShot/0/checkpoint-1976/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:04:53,334 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1976/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:04:53,334 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1976/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:04:56,995 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1872] due to args.save_total_limit\n",
            "{'loss': 0.0008, 'learning_rate': 4.615384615384616e-06, 'epoch': 19.23}\n",
            " 80% 2080/2600 [26:47<05:59,  1.45it/s][INFO|trainer.py:723] 2022-08-25 00:06:09,658 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:06:09,660 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:06:09,660 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:06:09,660 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.53it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.20it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.26it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.85it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.55it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.10it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.01it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.02it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.00it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 14.91it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 14.93it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 14.85it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 14.84it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 14.92it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 14.88it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.00it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.95it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 14.97it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 14.99it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 14.94it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 14.96it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 14.87it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 14.87it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.258415460586548, 'eval_accuracy': 0.7145922780036926, 'eval_f1': 0.7074121806946234, 'eval_runtime': 3.8973, 'eval_samples_per_second': 119.569, 'eval_steps_per_second': 15.139, 'epoch': 20.0}\n",
            " 80% 2080/2600 [26:51<05:59,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 14.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:06:13,558 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2080\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:06:13,559 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:06:14,572 >> Model weights saved in models/ZeroShot/0/checkpoint-2080/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:06:14,573 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2080/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:06:14,573 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2080/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:06:18,079 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1976] due to args.save_total_limit\n",
            " 84% 2184/2600 [28:08<04:46,  1.45it/s][INFO|trainer.py:723] 2022-08-25 00:07:30,638 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:07:30,640 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:07:30,640 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:07:30,640 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.36it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.07it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.15it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.66it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.61it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.19it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.05it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.00it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 14.97it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 14.96it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.98it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.04it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.02it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.00it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.04it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.22it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.07it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.07it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.2698893547058105, 'eval_accuracy': 0.7081544995307922, 'eval_f1': 0.6997422680412371, 'eval_runtime': 3.8736, 'eval_samples_per_second': 120.302, 'eval_steps_per_second': 15.231, 'epoch': 21.0}\n",
            " 84% 2184/2600 [28:12<04:46,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:07:34,515 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2184\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:07:34,516 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:07:35,646 >> Model weights saved in models/ZeroShot/0/checkpoint-2184/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:07:35,647 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2184/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:07:35,647 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2184/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:07:39,176 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2080] due to args.save_total_limit\n",
            " 88% 2288/2600 [29:29<03:35,  1.45it/s][INFO|trainer.py:723] 2022-08-25 00:08:51,905 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:08:51,906 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:08:51,907 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:08:51,907 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.50it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.15it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.37it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.98it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.75it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.32it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.01it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.03it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.08it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.05it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.16it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.07it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.286106586456299, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6919440520861571, 'eval_runtime': 3.8552, 'eval_samples_per_second': 120.875, 'eval_steps_per_second': 15.304, 'epoch': 22.0}\n",
            " 88% 2288/2600 [29:33<03:35,  1.45it/s]\n",
            "100% 59/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:08:55,763 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2288\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:08:55,764 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:08:56,755 >> Model weights saved in models/ZeroShot/0/checkpoint-2288/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:08:56,756 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2288/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:08:56,756 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2288/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:09:00,285 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2184] due to args.save_total_limit\n",
            " 92% 2392/2600 [30:50<02:22,  1.46it/s][INFO|trainer.py:723] 2022-08-25 00:10:13,055 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:10:13,057 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:10:13,057 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:10:13,057 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.78it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.40it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.43it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.88it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.81it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.26it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.08it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.03it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.11it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.30it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.39it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.04it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.02it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.04it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.296081781387329, 'eval_accuracy': 0.6995708346366882, 'eval_f1': 0.6894811994288434, 'eval_runtime': 3.8453, 'eval_samples_per_second': 121.187, 'eval_steps_per_second': 15.343, 'epoch': 23.0}\n",
            " 92% 2392/2600 [30:54<02:22,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:10:16,904 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2392\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:10:16,905 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:10:17,970 >> Model weights saved in models/ZeroShot/0/checkpoint-2392/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:10:17,970 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2392/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:10:17,971 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2392/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:10:21,526 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2288] due to args.save_total_limit\n",
            " 96% 2496/2600 [32:11<01:11,  1.46it/s][INFO|trainer.py:723] 2022-08-25 00:11:34,243 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:11:34,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:11:34,245 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:11:34,245 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.65it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.41it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.27it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.75it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.68it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.29it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.46it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.16it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.10it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.13it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.300567388534546, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6914510005668529, 'eval_runtime': 3.8486, 'eval_samples_per_second': 121.082, 'eval_steps_per_second': 15.33, 'epoch': 24.0}\n",
            " 96% 2496/2600 [32:15<01:11,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:11:38,095 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2496\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:11:38,096 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:11:39,087 >> Model weights saved in models/ZeroShot/0/checkpoint-2496/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:11:39,087 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2496/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:11:39,087 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2496/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:11:42,644 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2392] due to args.save_total_limit\n",
            "{'loss': 0.0001, 'learning_rate': 7.692307692307694e-07, 'epoch': 24.04}\n",
            "100% 2600/2600 [33:33<00:00,  1.46it/s][INFO|trainer.py:723] 2022-08-25 00:12:55,624 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:12:55,626 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:12:55,627 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:12:55,627 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.83it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.60it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.80it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.11it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.94it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.36it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.34it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.17it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.08it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.01it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.32it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.3025243282318115, 'eval_accuracy': 0.7017167210578918, 'eval_f1': 0.6914510005668529, 'eval_runtime': 3.8253, 'eval_samples_per_second': 121.82, 'eval_steps_per_second': 15.424, 'epoch': 25.0}\n",
            "100% 2600/2600 [33:37<00:00,  1.46it/s]\n",
            "100% 59/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:12:59,453 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2600\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:12:59,454 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:13:00,499 >> Model weights saved in models/ZeroShot/0/checkpoint-2600/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:13:00,499 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:13:00,499 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2600/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:13:04,077 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2496] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2022-08-25 00:13:04,226 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1942] 2022-08-25 00:13:04,227 >> Loading best model from models/ZeroShot/0/checkpoint-624 (score: 0.7214029493822239).\n",
            "{'train_runtime': 2024.236, 'train_samples_per_second': 41.09, 'train_steps_per_second': 1.284, 'train_loss': 0.05019686394854664, 'epoch': 25.0}\n",
            "100% 2600/2600 [33:44<00:00,  1.28it/s]\n",
            "[INFO|trainer.py:2640] 2022-08-25 00:13:06,504 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:13:06,505 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:13:07,733 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:13:07,734 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:13:07,734 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0502\n",
            "  train_runtime            = 0:33:44.23\n",
            "  train_samples            =       3327\n",
            "  train_samples_per_second =      41.09\n",
            "  train_steps_per_second   =      1.284\n",
            "08/25/2022 00:13:07 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:723] 2022-08-25 00:13:07,777 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:13:07,779 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:13:07,779 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:13:07,779 >>   Batch size = 8\n",
            "100% 59/59 [00:03<00:00, 16.00it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.7296\n",
            "  eval_f1                 =     0.7214\n",
            "  eval_loss               =     1.5219\n",
            "  eval_runtime            = 0:00:03.76\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    123.733\n",
            "  eval_steps_per_second   =     15.666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline PT : Monolingual BERT for zero-shot portuguese idiomaticity detection "
      ],
      "metadata": {
        "id": "skzktsJWo29H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Portuguese language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 25 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXANz-sfXIPc",
        "outputId": "8871c310-1a60-44d4-c116-fa2572fe309f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 00:51:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 00:51:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Aug25_00-51-23_93ca22c7e237,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 00:51:23 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "08/25/2022 00:51:23 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "08/25/2022 00:51:24 - WARNING - datasets.builder -   Using custom data configuration default-45eea5039653c272\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-45eea5039653c272/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10525.23it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 354.10it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-45eea5039653c272/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1077.67it/s]\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 00:51:24,253 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 00:51:24,259 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 00:51:24,519 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 00:51:24,520 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:51:25,290 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:51:25,291 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:51:25,291 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:51:25,291 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:51:25,291 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 00:51:25,421 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 00:51:25,422 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2022-08-25 00:51:25,659 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:2426] 2022-08-25 00:51:28,458 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2438] 2022-08-25 00:51:28,459 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 2/2 [00:00<00:00,  5.40ba/s]\n",
            "100% 1/1 [00:00<00:00, 14.78ba/s]\n",
            "08/25/2022 00:51:29 - INFO - __main__ -   Sample 788 of the training set: {'label': 1, 'sentence1': 'Diálogo, harmonia e transparência foram as palavras destacadas por ele. Ao final, se dirigiu até cada um dos parlamentares para fazer o cumprimento com a mão fechada, tradicional em meio à pandemia. Mesmo em clima aparentemente harmonioso, o tucano não deixou de avisar:   — Não há como nos omitirmos quanto aos ajustes, as pautas e reformas sensíveis que porventura entrarão em discussão — afirmou durante a fala oficial de abertura do ano legislativo.', 'input_ids': [101, 12120, 5589, 13791, 1186, 117, 7031, 11357, 174, 14715, 17482, 24559, 22313, 1111, 2312, 1112, 185, 5971, 1964, 7297, 3532, 1777, 2599, 9028, 185, 1766, 8468, 1162, 119, 138, 1186, 1509, 117, 14516, 23155, 24874, 1358, 1120, 2744, 11019, 1810, 15276, 18463, 14247, 7609, 3452, 18828, 18311, 175, 10961, 1200, 184, 16040, 1643, 10205, 3452, 1186, 3254, 170, 182, 9290, 175, 11252, 7971, 117, 189, 9871, 27989, 24059, 9712, 1143, 2660, 246, 13316, 2007, 8191, 119, 2508, 26358, 9712, 172, 24891, 1161, 170, 17482, 23771, 1880, 1162, 7031, 11153, 22354, 117, 184, 189, 23315, 2728, 183, 9290, 21357, 1775, 6094, 1260, 170, 9356, 1813, 131, 783, 151, 9290, 177, 5589, 3254, 1186, 1185, 1116, 184, 9084, 3161, 11828, 186, 27280, 1186, 170, 2155, 170, 9380, 13894, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "08/25/2022 00:51:29 - INFO - __main__ -   Sample 861 of the training set: {'label': 1, 'sentence1': 'Rudôlf Pikhóia, doutor em História e ex-arquivista russo, argumenta, em seu artigo \"Por que a União Soviética caiu?\", que a principal característica do Estado soviético era a unidade dos órgãos governamentais e do Partido Comunista.  A Constituição Soviética de 1977 definiu o Partido como “o núcleo do sistema político”. Seguindo esta linha, Lênin afirmava que o Soviete - ou seja, os órgãos eleitos de autogestão local - era uma forma de democracia direta e, assim, não havia necessidade de parlamento ou de separação de poderes (legislativo, executivo e judiciário).', 'input_ids': [101, 155, 4867, 28206, 9654, 21902, 9862, 7774, 1465, 117, 1202, 21017, 1197, 9712, 1230, 1204, 7774, 3464, 174, 4252, 118, 170, 1197, 18276, 18295, 1161, 187, 13356, 1186, 117, 6171, 1161, 117, 9712, 14516, 1358, 1893, 11466, 107, 18959, 1197, 15027, 170, 12118, 1182, 9290, 1573, 5086, 2744, 11761, 11019, 19009, 136, 107, 117, 15027, 170, 3981, 1610, 11179, 1200, 16928, 11761, 1202, 142, 18735, 1186, 1177, 5086, 2744, 2941, 1186, 3386, 170, 8362, 6859, 2007, 18463, 265, 10805, 9290, 1116, 23633, 11462, 15837, 174, 1202, 4539, 12894, 3291, 13601, 22158, 1161, 119, 138, 16752, 2050, 2875, 6592, 18052, 1573, 5086, 2744, 11761, 1260, 2449, 19353, 4729, 1358, 184, 4539, 12894, 3254, 1186, 789, 184, 183, 12643, 10536, 1186, 1202, 27466, 13894, 1918, 185, 4063, 6212, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "08/25/2022 00:51:29 - INFO - __main__ -   Sample 82 of the training set: {'label': 1, 'sentence1': 'Antes de tudo, preaqueça o forno a 180º C e unte uma assadeira grande. Então, para dar início e colocar de vez a mão na massa, comece misturando a aveia e a farinha integral. Igualmente, adicione à mistura o fermento e o açúcar.', 'input_ids': [101, 1760, 3052, 1260, 189, 18109, 117, 3073, 23911, 15331, 184, 1111, 2728, 170, 7967, 28174, 140, 174, 8362, 1566, 15276, 1161, 3919, 6397, 5132, 5372, 1162, 119, 13832, 1204, 9290, 117, 18311, 5358, 1197, 1107, 6212, 8174, 174, 1884, 27089, 1813, 1260, 1396, 1584, 170, 182, 9290, 9468, 3367, 1161, 117, 1435, 2093, 12791, 4084, 13645, 170, 170, 2707, 1465, 174, 170, 1677, 1394, 2328, 10226, 119, 146, 13855, 13505, 23771, 117, 8050, 27989, 4798, 246, 12791, 4084, 184, 175, 1200, 26173, 174, 184, 170, 28201, 12643, 8766, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:723] 2022-08-25 00:51:32,959 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-08-25 00:51:32,968 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-08-25 00:51:32,968 >>   Num examples = 1164\n",
            "[INFO|trainer.py:1607] 2022-08-25 00:51:32,968 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1608] 2022-08-25 00:51:32,968 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1609] 2022-08-25 00:51:32,968 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1610] 2022-08-25 00:51:32,968 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-08-25 00:51:32,968 >>   Total optimization steps = 925\n",
            "  4% 37/925 [00:24<07:59,  1.85it/s][INFO|trainer.py:723] 2022-08-25 00:51:57,307 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:51:57,310 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:51:57,310 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:51:57,310 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 24.59it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 18.81it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 17.94it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 17.13it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 16.77it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 16.80it/s]\u001b[A\n",
            " 46% 16/35 [00:00<00:01, 16.50it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 16.69it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 16.66it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 16.66it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 16.60it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 16.32it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 16.29it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 16.32it/s]\u001b[A\n",
            " 91% 32/35 [00:01<00:00, 16.35it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 16.52it/s]\u001b[A\n",
            "{'eval_loss': 0.7562416195869446, 'eval_accuracy': 0.5641025900840759, 'eval_f1': 0.36065573770491804, 'eval_runtime': 2.0846, 'eval_samples_per_second': 130.962, 'eval_steps_per_second': 16.79, 'epoch': 1.0}\n",
            "\n",
            "  4% 37/925 [00:26<07:59,  1.85it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:51:59,396 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-37\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:51:59,396 >> Configuration saved in models/ZeroShot/0/checkpoint-37/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:52:00,513 >> Model weights saved in models/ZeroShot/0/checkpoint-37/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:52:00,513 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-37/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:52:00,513 >> Special tokens file saved in models/ZeroShot/0/checkpoint-37/special_tokens_map.json\n",
            "  8% 74/925 [00:55<08:00,  1.77it/s][INFO|trainer.py:723] 2022-08-25 00:52:28,418 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:52:28,419 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:52:28,419 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:52:28,419 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 23.62it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 18.53it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 17.62it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 16.63it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 16.45it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 16.18it/s]\u001b[A\n",
            " 46% 16/35 [00:00<00:01, 15.95it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.84it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.91it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.98it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 16.13it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 15.84it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 15.79it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 15.74it/s]\u001b[A\n",
            " 91% 32/35 [00:01<00:00, 15.68it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6852218508720398, 'eval_accuracy': 0.5641025900840759, 'eval_f1': 0.36065573770491804, 'eval_runtime': 2.1622, 'eval_samples_per_second': 126.259, 'eval_steps_per_second': 16.187, 'epoch': 2.0}\n",
            "  8% 74/925 [00:57<08:00,  1.77it/s]\n",
            "100% 35/35 [00:02<00:00, 15.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:52:30,583 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-74\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:52:30,584 >> Configuration saved in models/ZeroShot/0/checkpoint-74/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:52:31,656 >> Model weights saved in models/ZeroShot/0/checkpoint-74/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:52:31,656 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-74/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:52:31,657 >> Special tokens file saved in models/ZeroShot/0/checkpoint-74/special_tokens_map.json\n",
            " 12% 111/925 [01:27<07:49,  1.73it/s][INFO|trainer.py:723] 2022-08-25 00:53:00,301 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:53:00,303 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:53:00,303 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:53:00,303 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 22.02it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.32it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.64it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 16.07it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.95it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.58it/s]\u001b[A\n",
            " 46% 16/35 [00:00<00:01, 15.60it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.81it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.84it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.60it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.53it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 15.43it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 15.29it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 15.15it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 15.17it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7295523881912231, 'eval_accuracy': 0.5824176073074341, 'eval_f1': 0.4524630541871921, 'eval_runtime': 2.2305, 'eval_samples_per_second': 122.393, 'eval_steps_per_second': 15.691, 'epoch': 3.0}\n",
            " 12% 111/925 [01:29<07:49,  1.73it/s]\n",
            "100% 35/35 [00:02<00:00, 15.14it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:53:02,535 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-111\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:53:02,536 >> Configuration saved in models/ZeroShot/0/checkpoint-111/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:53:03,631 >> Model weights saved in models/ZeroShot/0/checkpoint-111/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:53:03,632 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-111/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:53:03,632 >> Special tokens file saved in models/ZeroShot/0/checkpoint-111/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:53:07,019 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-37] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:53:07,108 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-74] due to args.save_total_limit\n",
            " 16% 148/925 [02:00<07:46,  1.67it/s][INFO|trainer.py:723] 2022-08-25 00:53:33,346 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:53:33,348 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:53:33,348 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:53:33,348 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.87it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.99it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.09it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.50it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.28it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.87it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.84it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.84it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.74it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.69it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.51it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.59it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.50it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.60it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.59it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7973518371582031, 'eval_accuracy': 0.6153846383094788, 'eval_f1': 0.5112781954887218, 'eval_runtime': 2.3388, 'eval_samples_per_second': 116.725, 'eval_steps_per_second': 14.965, 'epoch': 4.0}\n",
            " 16% 148/925 [02:02<07:46,  1.67it/s]\n",
            "100% 35/35 [00:02<00:00, 14.65it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:53:35,689 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-148\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:53:35,690 >> Configuration saved in models/ZeroShot/0/checkpoint-148/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:53:36,947 >> Model weights saved in models/ZeroShot/0/checkpoint-148/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:53:36,948 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-148/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:53:36,948 >> Special tokens file saved in models/ZeroShot/0/checkpoint-148/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:53:40,215 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-111] due to args.save_total_limit\n",
            " 20% 185/925 [02:33<07:16,  1.69it/s][INFO|trainer.py:723] 2022-08-25 00:54:06,322 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:54:06,324 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:54:06,324 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:54:06,324 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.89it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.30it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.42it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.61it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.36it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.85it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.05it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.07it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.08it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.94it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.98it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.71it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.99it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 15.01it/s]\u001b[A\n",
            "{'eval_loss': 0.855400562286377, 'eval_accuracy': 0.6190476417541504, 'eval_f1': 0.5802483737433471, 'eval_runtime': 2.2908, 'eval_samples_per_second': 119.172, 'eval_steps_per_second': 15.279, 'epoch': 5.0}\n",
            "\n",
            " 20% 185/925 [02:35<07:16,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:54:08,617 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-185\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:54:08,617 >> Configuration saved in models/ZeroShot/0/checkpoint-185/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:54:09,643 >> Model weights saved in models/ZeroShot/0/checkpoint-185/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:54:09,644 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-185/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:54:09,644 >> Special tokens file saved in models/ZeroShot/0/checkpoint-185/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:54:13,101 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-148] due to args.save_total_limit\n",
            " 24% 222/925 [03:06<06:58,  1.68it/s][INFO|trainer.py:723] 2022-08-25 00:54:39,128 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:54:39,129 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:54:39,129 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:54:39,129 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.72it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.97it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.00it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.47it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.32it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.93it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.11it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.05it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.98it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.84it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.64it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.75it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.88it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.83it/s]\u001b[A\n",
            "{'eval_loss': 1.0325703620910645, 'eval_accuracy': 0.5860806107521057, 'eval_f1': 0.5471972287865667, 'eval_runtime': 2.3135, 'eval_samples_per_second': 118.004, 'eval_steps_per_second': 15.129, 'epoch': 6.0}\n",
            "\n",
            " 24% 222/925 [03:08<06:58,  1.68it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:54:41,445 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-222\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:54:41,446 >> Configuration saved in models/ZeroShot/0/checkpoint-222/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:54:42,445 >> Model weights saved in models/ZeroShot/0/checkpoint-222/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:54:42,445 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:54:42,446 >> Special tokens file saved in models/ZeroShot/0/checkpoint-222/special_tokens_map.json\n",
            " 28% 259/925 [03:39<06:38,  1.67it/s][INFO|trainer.py:723] 2022-08-25 00:55:12,028 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:55:12,030 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:55:12,030 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:55:12,030 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.13it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.01it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.13it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.52it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.25it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.80it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.80it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.94it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.84it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.74it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.69it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.80it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.71it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.83it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.79it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.76it/s]\u001b[A\n",
            "{'eval_loss': 1.27616286277771, 'eval_accuracy': 0.6080586314201355, 'eval_f1': 0.5473198096979652, 'eval_runtime': 2.3226, 'eval_samples_per_second': 117.541, 'eval_steps_per_second': 15.069, 'epoch': 7.0}\n",
            "\n",
            " 28% 259/925 [03:41<06:38,  1.67it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:55:14,354 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-259\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:55:14,355 >> Configuration saved in models/ZeroShot/0/checkpoint-259/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:55:15,580 >> Model weights saved in models/ZeroShot/0/checkpoint-259/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:55:15,581 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-259/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:55:15,581 >> Special tokens file saved in models/ZeroShot/0/checkpoint-259/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:55:19,004 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-222] due to args.save_total_limit\n",
            " 32% 296/925 [04:12<06:13,  1.69it/s][INFO|trainer.py:723] 2022-08-25 00:55:45,167 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:55:45,169 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:55:45,169 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:55:45,169 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.22it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.96it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.10it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.31it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.23it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.06it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.97it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.90it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.67it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.72it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.75it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.93it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.2609621286392212, 'eval_accuracy': 0.5824176073074341, 'eval_f1': 0.5807920258620689, 'eval_runtime': 2.3137, 'eval_samples_per_second': 117.991, 'eval_steps_per_second': 15.127, 'epoch': 8.0}\n",
            " 32% 296/925 [04:14<06:13,  1.69it/s]\n",
            "100% 35/35 [00:02<00:00, 14.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:55:47,484 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-296\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:55:47,485 >> Configuration saved in models/ZeroShot/0/checkpoint-296/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:55:48,459 >> Model weights saved in models/ZeroShot/0/checkpoint-296/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:55:48,460 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-296/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:55:48,460 >> Special tokens file saved in models/ZeroShot/0/checkpoint-296/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:55:52,012 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-185] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:55:52,041 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-259] due to args.save_total_limit\n",
            " 36% 333/925 [04:45<05:51,  1.69it/s][INFO|trainer.py:723] 2022-08-25 00:56:18,347 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:56:18,348 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:56:18,349 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:56:18,349 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 22.29it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.22it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.40it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.63it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.34it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.89it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.91it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.84it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.85it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.86it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.71it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.56it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.78it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.78it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.80it/s]\u001b[A\n",
            "{'eval_loss': 1.4830832481384277, 'eval_accuracy': 0.6300366520881653, 'eval_f1': 0.6040012063938875, 'eval_runtime': 2.3083, 'eval_samples_per_second': 118.269, 'eval_steps_per_second': 15.163, 'epoch': 9.0}\n",
            "\n",
            " 36% 333/925 [04:47<05:51,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:56:20,658 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-333\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:56:20,659 >> Configuration saved in models/ZeroShot/0/checkpoint-333/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:56:21,642 >> Model weights saved in models/ZeroShot/0/checkpoint-333/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:56:21,643 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-333/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:56:21,643 >> Special tokens file saved in models/ZeroShot/0/checkpoint-333/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:56:25,215 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-296] due to args.save_total_limit\n",
            " 40% 370/925 [05:18<05:26,  1.70it/s][INFO|trainer.py:723] 2022-08-25 00:56:51,248 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:56:51,250 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:56:51,250 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:56:51,250 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.71it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.16it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.32it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.55it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.50it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.14it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.34it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.24it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.26it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.10it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.92it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.87it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.70it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.97it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 15.09it/s]\u001b[A\n",
            "{'eval_loss': 1.6840450763702393, 'eval_accuracy': 0.6263736486434937, 'eval_f1': 0.5992689385217592, 'eval_runtime': 2.2827, 'eval_samples_per_second': 119.597, 'eval_steps_per_second': 15.333, 'epoch': 10.0}\n",
            "\n",
            " 40% 370/925 [05:20<05:26,  1.70it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:56:53,534 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-370\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:56:53,535 >> Configuration saved in models/ZeroShot/0/checkpoint-370/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:56:54,579 >> Model weights saved in models/ZeroShot/0/checkpoint-370/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:56:54,579 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-370/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:56:54,580 >> Special tokens file saved in models/ZeroShot/0/checkpoint-370/special_tokens_map.json\n",
            " 44% 407/925 [05:51<05:08,  1.68it/s][INFO|trainer.py:723] 2022-08-25 00:57:24,220 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:57:24,222 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:57:24,222 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:57:24,222 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.66it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.03it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.14it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.53it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.19it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.86it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.89it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.91it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.86it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.99it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.75it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.67it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.64it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.82it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.72it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.66it/s]\u001b[A\n",
            "{'eval_loss': 1.8640676736831665, 'eval_accuracy': 0.622710645198822, 'eval_f1': 0.5789623418432284, 'eval_runtime': 2.3244, 'eval_samples_per_second': 117.448, 'eval_steps_per_second': 15.057, 'epoch': 11.0}\n",
            "\n",
            " 44% 407/925 [05:53<05:08,  1.68it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:57:26,550 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-407\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:57:26,551 >> Configuration saved in models/ZeroShot/0/checkpoint-407/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:57:27,656 >> Model weights saved in models/ZeroShot/0/checkpoint-407/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:57:27,656 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-407/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:57:27,656 >> Special tokens file saved in models/ZeroShot/0/checkpoint-407/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:57:31,104 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-370] due to args.save_total_limit\n",
            " 48% 444/925 [06:24<04:44,  1.69it/s][INFO|trainer.py:723] 2022-08-25 00:57:57,243 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:57:57,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:57:57,245 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:57:57,245 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.24it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.03it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.10it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.47it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.45it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.00it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.97it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.05it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.95it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.03it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.87it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.85it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.90it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.88it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.92it/s]\u001b[A\n",
            "{'eval_loss': 1.887851357460022, 'eval_accuracy': 0.6153846383094788, 'eval_f1': 0.5984900480439259, 'eval_runtime': 2.3007, 'eval_samples_per_second': 118.659, 'eval_steps_per_second': 15.213, 'epoch': 12.0}\n",
            "\n",
            " 48% 444/925 [06:26<04:44,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:57:59,547 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-444\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:57:59,548 >> Configuration saved in models/ZeroShot/0/checkpoint-444/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:58:00,529 >> Model weights saved in models/ZeroShot/0/checkpoint-444/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:58:00,529 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-444/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:58:00,529 >> Special tokens file saved in models/ZeroShot/0/checkpoint-444/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:58:04,098 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-407] due to args.save_total_limit\n",
            " 52% 481/925 [06:57<04:24,  1.68it/s][INFO|trainer.py:723] 2022-08-25 00:58:30,259 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:58:30,261 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:58:30,261 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:58:30,261 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.98it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.74it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.04it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.42it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.37it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.89it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.91it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.80it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.75it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.70it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.61it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.67it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.83it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.3928656578063965, 'eval_accuracy': 0.6043956279754639, 'eval_f1': 0.49945674317533617, 'eval_runtime': 2.3221, 'eval_samples_per_second': 117.564, 'eval_steps_per_second': 15.072, 'epoch': 13.0}\n",
            " 52% 481/925 [06:59<04:24,  1.68it/s]\n",
            "100% 35/35 [00:02<00:00, 14.75it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:58:32,584 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-481\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:58:32,585 >> Configuration saved in models/ZeroShot/0/checkpoint-481/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:58:33,628 >> Model weights saved in models/ZeroShot/0/checkpoint-481/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:58:33,629 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-481/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:58:33,629 >> Special tokens file saved in models/ZeroShot/0/checkpoint-481/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:58:37,167 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-444] due to args.save_total_limit\n",
            "{'loss': 0.272, 'learning_rate': 9.189189189189191e-06, 'epoch': 13.51}\n",
            " 56% 518/925 [07:30<04:04,  1.66it/s][INFO|trainer.py:723] 2022-08-25 00:59:03,423 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:59:03,425 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:59:03,425 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:59:03,425 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.94it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.83it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.05it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.33it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.25it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.88it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.70it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.64it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.71it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.55it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.55it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.47it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.60it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.62it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.160318613052368, 'eval_accuracy': 0.6043956279754639, 'eval_f1': 0.5790405482581382, 'eval_runtime': 2.3407, 'eval_samples_per_second': 116.633, 'eval_steps_per_second': 14.953, 'epoch': 14.0}\n",
            " 56% 518/925 [07:32<04:04,  1.66it/s]\n",
            "100% 35/35 [00:02<00:00, 14.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:59:05,767 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-518\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:59:05,768 >> Configuration saved in models/ZeroShot/0/checkpoint-518/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:59:06,767 >> Model weights saved in models/ZeroShot/0/checkpoint-518/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:59:06,767 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-518/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:59:06,768 >> Special tokens file saved in models/ZeroShot/0/checkpoint-518/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:59:10,345 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-481] due to args.save_total_limit\n",
            " 60% 555/925 [08:03<03:39,  1.69it/s][INFO|trainer.py:723] 2022-08-25 00:59:36,521 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:59:36,524 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:59:36,524 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:59:36,524 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.49it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.20it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.42it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.80it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.56it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.07it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.08it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.12it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.05it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.08it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.93it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.86it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.80it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.76it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.93it/s]\u001b[A\n",
            "{'eval_loss': 2.3465447425842285, 'eval_accuracy': 0.6153846383094788, 'eval_f1': 0.5636521394973589, 'eval_runtime': 2.2947, 'eval_samples_per_second': 118.972, 'eval_steps_per_second': 15.253, 'epoch': 15.0}\n",
            "\n",
            " 60% 555/925 [08:05<03:39,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:59:38,820 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-555\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:59:38,821 >> Configuration saved in models/ZeroShot/0/checkpoint-555/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:59:39,857 >> Model weights saved in models/ZeroShot/0/checkpoint-555/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:59:39,857 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-555/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:59:39,857 >> Special tokens file saved in models/ZeroShot/0/checkpoint-555/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:59:43,396 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-518] due to args.save_total_limit\n",
            " 64% 592/925 [08:36<03:20,  1.66it/s][INFO|trainer.py:723] 2022-08-25 01:00:09,581 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:00:09,582 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:00:09,582 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:00:09,582 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.60it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.07it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.98it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.47it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.22it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.85it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.81it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.90it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.69it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.54it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.50it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.55it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.64it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.60it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.62it/s]\u001b[A\n",
            "{'eval_loss': 2.360442876815796, 'eval_accuracy': 0.6080586314201355, 'eval_f1': 0.5602673531138509, 'eval_runtime': 2.3349, 'eval_samples_per_second': 116.922, 'eval_steps_per_second': 14.99, 'epoch': 16.0}\n",
            "\n",
            " 64% 592/925 [08:38<03:20,  1.66it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:00:11,919 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-592\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:00:11,920 >> Configuration saved in models/ZeroShot/0/checkpoint-592/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:00:12,961 >> Model weights saved in models/ZeroShot/0/checkpoint-592/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:00:12,962 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-592/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:00:12,962 >> Special tokens file saved in models/ZeroShot/0/checkpoint-592/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:00:16,482 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-555] due to args.save_total_limit\n",
            " 68% 629/925 [09:09<02:57,  1.67it/s][INFO|trainer.py:723] 2022-08-25 01:00:42,723 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:00:42,725 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:00:42,725 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:00:42,725 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.55it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.04it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.96it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.46it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.46it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.07it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.07it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.06it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.83it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.66it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.80it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.67it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.70it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.86it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.3764750957489014, 'eval_accuracy': 0.6080586314201355, 'eval_f1': 0.5626113648274313, 'eval_runtime': 2.3072, 'eval_samples_per_second': 118.326, 'eval_steps_per_second': 15.17, 'epoch': 17.0}\n",
            " 68% 629/925 [09:12<02:57,  1.67it/s]\n",
            "100% 35/35 [00:02<00:00, 14.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:00:45,033 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-629\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:00:45,034 >> Configuration saved in models/ZeroShot/0/checkpoint-629/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:00:46,087 >> Model weights saved in models/ZeroShot/0/checkpoint-629/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:00:46,088 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-629/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:00:46,088 >> Special tokens file saved in models/ZeroShot/0/checkpoint-629/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:00:49,606 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-592] due to args.save_total_limit\n",
            " 72% 666/925 [09:42<02:34,  1.68it/s][INFO|trainer.py:723] 2022-08-25 01:01:15,748 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:01:15,750 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:01:15,750 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:01:15,750 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.40it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.17it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.36it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.52it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.14it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.82it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.85it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.82it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.95it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.79it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.62it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.58it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.83it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.76it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.82it/s]\u001b[A\n",
            "{'eval_loss': 2.36519455909729, 'eval_accuracy': 0.6043956279754639, 'eval_f1': 0.5662076271186441, 'eval_runtime': 2.3185, 'eval_samples_per_second': 117.75, 'eval_steps_per_second': 15.096, 'epoch': 18.0}\n",
            "\n",
            " 72% 666/925 [09:45<02:34,  1.68it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:01:18,070 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-666\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:01:18,071 >> Configuration saved in models/ZeroShot/0/checkpoint-666/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:01:19,069 >> Model weights saved in models/ZeroShot/0/checkpoint-666/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:01:19,070 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-666/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:01:19,070 >> Special tokens file saved in models/ZeroShot/0/checkpoint-666/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:01:22,607 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-629] due to args.save_total_limit\n",
            " 76% 703/925 [10:15<02:12,  1.68it/s][INFO|trainer.py:723] 2022-08-25 01:01:48,769 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:01:48,771 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:01:48,771 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:01:48,772 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.28it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.96it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.08it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.60it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.35it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.99it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.15it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.07it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.96it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.76it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.69it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.66it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.78it/s]\u001b[A\n",
            "{'eval_loss': 2.4135050773620605, 'eval_accuracy': 0.5970696210861206, 'eval_f1': 0.5538155235944371, 'eval_runtime': 2.3108, 'eval_samples_per_second': 118.143, 'eval_steps_per_second': 15.146, 'epoch': 19.0}\n",
            "\n",
            " 76% 703/925 [10:18<02:12,  1.68it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:01:51,084 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-703\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:01:51,084 >> Configuration saved in models/ZeroShot/0/checkpoint-703/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:01:52,122 >> Model weights saved in models/ZeroShot/0/checkpoint-703/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:01:52,122 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-703/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:01:52,123 >> Special tokens file saved in models/ZeroShot/0/checkpoint-703/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:01:55,674 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-666] due to args.save_total_limit\n",
            " 80% 740/925 [10:48<01:50,  1.67it/s][INFO|trainer.py:723] 2022-08-25 01:02:21,864 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:02:21,866 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:02:21,866 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:02:21,866 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.70it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.83it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.02it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.25it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.12it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.81it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.04it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.02it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.91it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.79it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.63it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.61it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.71it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5060672760009766, 'eval_accuracy': 0.6263736486434937, 'eval_f1': 0.5671247357293869, 'eval_runtime': 2.3183, 'eval_samples_per_second': 117.756, 'eval_steps_per_second': 15.097, 'epoch': 20.0}\n",
            " 80% 740/925 [10:51<01:50,  1.67it/s]\n",
            "100% 35/35 [00:02<00:00, 14.84it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:02:24,186 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-740\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:02:24,186 >> Configuration saved in models/ZeroShot/0/checkpoint-740/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:02:25,238 >> Model weights saved in models/ZeroShot/0/checkpoint-740/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:02:25,239 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-740/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:02:25,239 >> Special tokens file saved in models/ZeroShot/0/checkpoint-740/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:02:28,965 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-703] due to args.save_total_limit\n",
            " 84% 777/925 [11:22<01:28,  1.67it/s][INFO|trainer.py:723] 2022-08-25 01:02:55,124 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:02:55,127 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:02:55,127 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:02:55,127 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.24it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.90it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.02it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.53it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.31it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.81it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.86it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.88it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.70it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.54it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.58it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.79it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.75it/s]\u001b[A\n",
            "{'eval_loss': 2.442176103591919, 'eval_accuracy': 0.6153846383094788, 'eval_f1': 0.573011782580847, 'eval_runtime': 2.3217, 'eval_samples_per_second': 117.586, 'eval_steps_per_second': 15.075, 'epoch': 21.0}\n",
            "\n",
            " 84% 777/925 [11:24<01:28,  1.67it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:02:57,450 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-777\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:02:57,451 >> Configuration saved in models/ZeroShot/0/checkpoint-777/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:02:58,625 >> Model weights saved in models/ZeroShot/0/checkpoint-777/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:02:58,625 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-777/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:02:58,626 >> Special tokens file saved in models/ZeroShot/0/checkpoint-777/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:03:02,078 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-740] due to args.save_total_limit\n",
            " 88% 814/925 [11:55<01:06,  1.68it/s][INFO|trainer.py:723] 2022-08-25 01:03:28,161 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:03:28,162 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:03:28,162 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:03:28,163 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.01it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.89it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.08it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.47it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.44it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.04it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.02it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.99it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.94it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.60it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.55it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.69it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.75it/s]\u001b[A\n",
            "{'eval_loss': 2.4764955043792725, 'eval_accuracy': 0.6190476417541504, 'eval_f1': 0.5665608207132389, 'eval_runtime': 2.3166, 'eval_samples_per_second': 117.846, 'eval_steps_per_second': 15.108, 'epoch': 22.0}\n",
            "\n",
            " 88% 814/925 [11:57<01:06,  1.68it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:03:30,481 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-814\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:03:30,482 >> Configuration saved in models/ZeroShot/0/checkpoint-814/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:03:31,492 >> Model weights saved in models/ZeroShot/0/checkpoint-814/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:03:31,493 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-814/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:03:31,493 >> Special tokens file saved in models/ZeroShot/0/checkpoint-814/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:03:35,016 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-777] due to args.save_total_limit\n",
            " 92% 851/925 [12:28<00:43,  1.68it/s][INFO|trainer.py:723] 2022-08-25 01:04:01,285 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:04:01,287 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:04:01,287 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:04:01,287 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.83it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.93it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.96it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.40it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.39it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.05it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.05it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.88it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.78it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.63it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.86it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 14.87it/s]\u001b[A\n",
            "{'eval_loss': 2.442155361175537, 'eval_accuracy': 0.6080586314201355, 'eval_f1': 0.5670712729536259, 'eval_runtime': 2.3115, 'eval_samples_per_second': 118.105, 'eval_steps_per_second': 15.142, 'epoch': 23.0}\n",
            "\n",
            " 92% 851/925 [12:30<00:43,  1.68it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:04:03,600 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-851\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:04:03,601 >> Configuration saved in models/ZeroShot/0/checkpoint-851/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:04:04,670 >> Model weights saved in models/ZeroShot/0/checkpoint-851/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:04:04,671 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-851/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:04:04,671 >> Special tokens file saved in models/ZeroShot/0/checkpoint-851/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:04:08,194 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-814] due to args.save_total_limit\n",
            " 96% 888/925 [13:01<00:21,  1.69it/s][INFO|trainer.py:723] 2022-08-25 01:04:34,312 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:04:34,313 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:04:34,314 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:04:34,314 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.77it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.20it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.36it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.67it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.42it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.23it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.21it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.26it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.14it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.10it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.98it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.91it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.69it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.82it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.84it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 15.01it/s]\u001b[A\n",
            "{'eval_loss': 2.4621834754943848, 'eval_accuracy': 0.6190476417541504, 'eval_f1': 0.5737448955080471, 'eval_runtime': 2.2884, 'eval_samples_per_second': 119.296, 'eval_steps_per_second': 15.294, 'epoch': 24.0}\n",
            "\n",
            " 96% 888/925 [13:03<00:21,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:04:36,603 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-888\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:04:36,604 >> Configuration saved in models/ZeroShot/0/checkpoint-888/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:04:37,617 >> Model weights saved in models/ZeroShot/0/checkpoint-888/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:04:37,618 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-888/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:04:37,618 >> Special tokens file saved in models/ZeroShot/0/checkpoint-888/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:04:41,159 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-851] due to args.save_total_limit\n",
            "100% 925/925 [13:34<00:00,  1.69it/s][INFO|trainer.py:723] 2022-08-25 01:05:07,397 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:05:07,399 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:05:07,399 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:05:07,399 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.83it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.09it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.04it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.32it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.32it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.98it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.11it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.02it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.04it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.82it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.68it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.91it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.91it/s]\u001b[A\n",
            " 97% 34/35 [00:02<00:00, 15.06it/s]\u001b[A\n",
            "{'eval_loss': 2.4653122425079346, 'eval_accuracy': 0.6190476417541504, 'eval_f1': 0.5737448955080471, 'eval_runtime': 2.2996, 'eval_samples_per_second': 118.714, 'eval_steps_per_second': 15.22, 'epoch': 25.0}\n",
            "\n",
            "100% 925/925 [13:36<00:00,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:05:09,700 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-925\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:05:09,701 >> Configuration saved in models/ZeroShot/0/checkpoint-925/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:05:10,848 >> Model weights saved in models/ZeroShot/0/checkpoint-925/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:05:10,849 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-925/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:05:10,849 >> Special tokens file saved in models/ZeroShot/0/checkpoint-925/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:05:14,391 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-888] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2022-08-25 01:05:14,536 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1942] 2022-08-25 01:05:14,537 >> Loading best model from models/ZeroShot/0/checkpoint-333 (score: 0.6040012063938875).\n",
            "{'train_runtime': 824.1909, 'train_samples_per_second': 35.307, 'train_steps_per_second': 1.122, 'train_loss': 0.1501680394765493, 'epoch': 25.0}\n",
            "100% 925/925 [13:44<00:00,  1.12it/s]\n",
            "[INFO|trainer.py:2640] 2022-08-25 01:05:17,161 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:05:17,162 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:05:18,121 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:05:18,121 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:05:18,122 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.1502\n",
            "  train_runtime            = 0:13:44.19\n",
            "  train_samples            =       1164\n",
            "  train_samples_per_second =     35.307\n",
            "  train_steps_per_second   =      1.122\n",
            "08/25/2022 01:05:18 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:723] 2022-08-25 01:05:18,157 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:05:18,158 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:05:18,158 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:05:18,158 >>   Batch size = 8\n",
            "100% 35/35 [00:02<00:00, 16.18it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =       0.63\n",
            "  eval_f1                 =      0.604\n",
            "  eval_loss               =     1.4831\n",
            "  eval_runtime            = 0:00:02.24\n",
            "  eval_samples            =        273\n",
            "  eval_samples_per_second =    121.439\n",
            "  eval_steps_per_second   =     15.569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Multi : Multilingual BERT for zero-shot multilingual idiomaticity detection"
      ],
      "metadata": {
        "id": "VErwYC4h0kQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 25 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "id": "RDxc4AfhYgO3",
        "outputId": "fdda7e8f-6651-4e69-b858-20aea99b0f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 02:07:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 02:07:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Aug25_02-07-53_93ca22c7e237,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 02:07:53 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "08/25/2022 02:07:53 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "08/25/2022 02:07:53 - WARNING - datasets.builder -   Using custom data configuration default-8a16c0eee14dec66\n",
            "08/25/2022 02:07:53 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-8a16c0eee14dec66/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 945.09it/s]\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 02:07:53,973 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 02:07:53,976 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 02:07:54,237 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 02:07:54,238 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:07:54,997 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:07:54,997 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:07:54,997 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:07:54,997 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:07:54,997 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 02:07:55,121 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 02:07:55,121 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2022-08-25 02:07:55,390 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2426] 2022-08-25 02:07:57,480 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2438] 2022-08-25 02:07:57,480 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "08/25/2022 02:07:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8a16c0eee14dec66/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b8fc9e8f12bf5ebc.arrow\n",
            "100% 1/1 [00:00<00:00,  6.22ba/s]\n",
            "08/25/2022 02:07:57 - INFO - __main__ -   Sample 3155 of the training set: {'label': 0, 'sentence1': 'According to history.com, the leap day was originally discovered by Egyptian astronomers, but this discovery did not reach the western world until Julius Caesar’s reign in 45 BC. Caesar then created the leap year calendar to fix the problem, which was later adapted in accordance with new knowledge about the earth’s orbit by Pope Gregory into the Gregorian calendar that we observe today. Feb. 29 happens every four years, because the earth technically requires 365.25 days to complete its orbit around the sun.', 'input_ids': [101, 14156, 10114, 11486, 119, 10212, 117, 10105, 20169, 10410, 11940, 10134, 15556, 21756, 10155, 34624, 50575, 10901, 117, 10473, 10531, 30419, 12172, 10472, 24278, 10105, 16672, 11356, 11444, 18703, 30159, 100, 187, 38587, 10106, 10827, 19376, 119, 30159, 11059, 13745, 10105, 20169, 10410, 10924, 61637, 10114, 14045, 10686, 10105, 18077, 117, 10319, 10134, 10873, 40851, 10106, 88651, 10169, 10751, 22975, 10978, 10105, 39189, 100, 187, 17090, 10155, 23874, 22392, 10708, 10105, 47723, 11630, 61637, 10189, 11951, 78275, 18745, 119, 21194, 119, 10386, 105315, 14234, 11598, 10855, 117, 12373, 10105, 39189, 29914, 10454, 39575, 25385, 119, 10258, 13990, 10114, 17876, 10474, 17090, 12166, 10105, 42230, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 02:07:57 - INFO - __main__ -   Sample 3445 of the training set: {'label': 1, 'sentence1': 'A saída da crise, segundo Bachelet, depende de ações que garantam renda para os mais pobres e vacina para todos São Paulo – Alta comissária para os Direitos Humanos da Organização das Nações Unidas (ONU), a ex-presidenta do Chile Michelle Bachelet criticou governantes de países pobres e ricos que optaram pela economia em vez de promover a saúde da população. O resultado, segundo ela, foi o aprofundamento das desigualdades sociais causadas pela histórica falta de investimento em áreas sociais, entre elas a saúde.', 'input_ids': [101, 138, 70747, 10143, 34862, 117, 12943, 18965, 41583, 117, 59216, 10104, 77302, 10121, 24457, 14732, 10147, 21367, 10220, 10427, 10614, 64440, 173, 10321, 22849, 10220, 12656, 12114, 13360, 100, 22798, 10212, 47097, 19075, 10220, 10427, 66130, 10107, 87537, 10143, 72154, 27187, 10242, 95322, 49323, 113, 46743, 114, 117, 169, 11419, 118, 107041, 10149, 13218, 27062, 18965, 41583, 82389, 10138, 25574, 25819, 10104, 15395, 64440, 173, 99702, 10121, 10303, 35474, 10147, 11793, 28223, 10266, 11675, 10104, 57833, 169, 54728, 10143, 17857, 119, 152, 20229, 117, 12943, 12593, 117, 10448, 183, 26219, 10567, 55227, 16686, 10242, 10139, 104915, 110285, 55167, 102453, 10107, 11793, 36818, 23821, 10104, 10106, 63996, 11498, 10266, 23571, 55167, 117, 10402, 34338, 169, 54728, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 02:07:57 - INFO - __main__ -   Sample 331 of the training set: {'label': 1, 'sentence1': 'Concept Smoke Screen, in partnership with G4S, have developed a new way of defending cash and guards against attacks when replenishing ATM\\'s. On May 11th at the IFSEC Security Industry Awards 2009, Concept Smoke Screen were honoured with the \"Physical Security Product of the Year\" award, for the Guardian Smoke Screen. The ceremony was conducted at the Birmingham Hilton Metropole.', 'input_ids': [101, 77961, 80677, 41131, 117, 10106, 36944, 10169, 144, 11011, 10731, 117, 10529, 14628, 169, 10751, 13170, 10108, 53730, 52828, 10111, 99024, 11327, 26483, 10841, 76456, 81635, 74062, 92233, 112, 187, 119, 10576, 10725, 34062, 10160, 10105, 25000, 39039, 10858, 20924, 25539, 12357, 10195, 117, 77961, 80677, 41131, 10309, 46948, 10336, 10169, 10105, 107, 33671, 20924, 93218, 10108, 10105, 13567, 107, 17725, 117, 10142, 10105, 20206, 80677, 41131, 119, 10117, 34713, 10134, 23736, 10160, 10105, 22712, 53329, 20640, 30328, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:723] 2022-08-25 02:07:59,803 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-08-25 02:07:59,809 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-08-25 02:07:59,809 >>   Num examples = 4491\n",
            "[INFO|trainer.py:1607] 2022-08-25 02:07:59,809 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1608] 2022-08-25 02:07:59,809 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1609] 2022-08-25 02:07:59,810 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1610] 2022-08-25 02:07:59,810 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-08-25 02:07:59,810 >>   Total optimization steps = 3525\n",
            "  4% 141/3525 [01:36<32:56,  1.71it/s][INFO|trainer.py:723] 2022-08-25 02:09:35,981 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:09:35,983 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:09:35,983 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:09:35,983 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.38it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.03it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.28it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 16.51it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 16.29it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.80it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 15.63it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.79it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.85it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.92it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.73it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.63it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 15.72it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.59it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 15.61it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.67it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.56it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.49it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.61it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.51it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 15.46it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.54it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.40it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.53it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.72it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.59it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.56it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.65it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.52it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:02, 15.49it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.48it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.40it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.57it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.84it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 15.55it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.47it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 15.32it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 15.34it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.48it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.27it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.44it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.69it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.66it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.55it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6862959265708923, 'eval_accuracy': 0.6387009620666504, 'eval_f1': 0.6369031517232623, 'eval_runtime': 5.9379, 'eval_samples_per_second': 124.456, 'eval_steps_per_second': 15.662, 'epoch': 1.0}\n",
            "  4% 141/3525 [01:42<32:56,  1.71it/s]\n",
            "100% 93/93 [00:05<00:00, 15.50it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:09:41,923 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-141\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:09:41,924 >> Configuration saved in models/ZeroShot/0/checkpoint-141/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:09:44,459 >> Model weights saved in models/ZeroShot/0/checkpoint-141/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:09:44,459 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-141/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:09:44,460 >> Special tokens file saved in models/ZeroShot/0/checkpoint-141/special_tokens_map.json\n",
            "  8% 282/3525 [03:32<32:22,  1.67it/s][INFO|trainer.py:723] 2022-08-25 02:11:32,106 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:11:32,108 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:11:32,108 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:11:32,108 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.34it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.14it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.39it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.62it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.37it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.92it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.14it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.22it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.29it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.97it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.87it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.97it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.01it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.22it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.41it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.23it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.26it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.85it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.05it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 15.00it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.20it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.21it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.22it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.14it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.20it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.15it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.24it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.14it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.14it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.18it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.19it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.19it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.10it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8383196592330933, 'eval_accuracy': 0.6197564005851746, 'eval_f1': 0.6190421154400042, 'eval_runtime': 6.1249, 'eval_samples_per_second': 120.654, 'eval_steps_per_second': 15.184, 'epoch': 2.0}\n",
            "  8% 282/3525 [03:38<32:22,  1.67it/s]\n",
            "100% 93/93 [00:06<00:00, 15.11it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:11:38,234 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-282\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:11:38,235 >> Configuration saved in models/ZeroShot/0/checkpoint-282/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:11:40,532 >> Model weights saved in models/ZeroShot/0/checkpoint-282/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:11:40,533 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-282/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:11:40,533 >> Special tokens file saved in models/ZeroShot/0/checkpoint-282/special_tokens_map.json\n",
            " 12% 423/3525 [05:28<31:13,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:13:28,727 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:13:28,729 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:13:28,729 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:13:28,729 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.13it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.26it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.35it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.68it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.15it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.39it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.40it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.38it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.44it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.93it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.83it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.76it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.94it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.34it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.19it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.88it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.73it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.96it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.13it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.76it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.80it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.88it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.81it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.80it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.1420670747756958, 'eval_accuracy': 0.6292287111282349, 'eval_f1': 0.62865683685446, 'eval_runtime': 6.168, 'eval_samples_per_second': 119.812, 'eval_steps_per_second': 15.078, 'epoch': 3.0}\n",
            " 12% 423/3525 [05:35<31:13,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:13:34,899 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-423\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:13:34,900 >> Configuration saved in models/ZeroShot/0/checkpoint-423/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:13:37,192 >> Model weights saved in models/ZeroShot/0/checkpoint-423/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:13:37,193 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-423/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:13:37,193 >> Special tokens file saved in models/ZeroShot/0/checkpoint-423/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:13:42,969 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-282] due to args.save_total_limit\n",
            "{'loss': 0.3281, 'learning_rate': 1.716312056737589e-05, 'epoch': 3.55}\n",
            " 16% 564/3525 [07:25<29:44,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:15:25,733 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:15:25,735 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:15:25,735 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:15:25,735 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.73it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.95it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.31it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.56it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.00it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.09it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.05it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.92it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.97it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.08it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.30it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.19it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.94it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.76it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.82it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.95it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.90it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.93it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.95it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.5816278457641602, 'eval_accuracy': 0.6265223026275635, 'eval_f1': 0.6230428032823243, 'eval_runtime': 6.1778, 'eval_samples_per_second': 119.622, 'eval_steps_per_second': 15.054, 'epoch': 4.0}\n",
            " 16% 564/3525 [07:32<29:44,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:15:31,914 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-564\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:15:31,915 >> Configuration saved in models/ZeroShot/0/checkpoint-564/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:15:34,324 >> Model weights saved in models/ZeroShot/0/checkpoint-564/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:15:34,325 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-564/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:15:34,325 >> Special tokens file saved in models/ZeroShot/0/checkpoint-564/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:15:40,043 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-423] due to args.save_total_limit\n",
            " 20% 705/3525 [09:23<28:20,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:17:22,832 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:17:22,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:17:22,833 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:17:22,834 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.48it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.50it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.47it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.69it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.57it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.04it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.13it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.28it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.34it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.00it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.00it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.81it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.74it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.83it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.97it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.89it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.80it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.96it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.84it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.96it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.98it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.09it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8636274337768555, 'eval_accuracy': 0.6779431700706482, 'eval_f1': 0.6734168659665176, 'eval_runtime': 6.173, 'eval_samples_per_second': 119.716, 'eval_steps_per_second': 15.066, 'epoch': 5.0}\n",
            " 20% 705/3525 [09:29<28:20,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:17:29,008 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-705\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:17:29,009 >> Configuration saved in models/ZeroShot/0/checkpoint-705/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:17:31,402 >> Model weights saved in models/ZeroShot/0/checkpoint-705/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:17:31,402 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-705/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:17:31,403 >> Special tokens file saved in models/ZeroShot/0/checkpoint-705/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:17:37,110 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-141] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:17:37,134 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-564] due to args.save_total_limit\n",
            " 24% 846/3525 [11:20<26:53,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:19:19,935 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:19:19,937 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:19:19,937 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:19:19,937 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.85it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.41it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.59it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.46it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.15it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.22it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.40it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.53it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.33it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.09it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.97it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 15.04it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.16it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.26it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.02it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.82it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.82it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.83it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.91it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.98it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.86it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.3411076068878174, 'eval_accuracy': 0.6346414089202881, 'eval_f1': 0.6265161725067385, 'eval_runtime': 6.1515, 'eval_samples_per_second': 120.134, 'eval_steps_per_second': 15.118, 'epoch': 6.0}\n",
            " 24% 846/3525 [11:26<26:53,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 15.09it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:19:26,090 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-846\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:19:26,091 >> Configuration saved in models/ZeroShot/0/checkpoint-846/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:19:28,530 >> Model weights saved in models/ZeroShot/0/checkpoint-846/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:19:28,530 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-846/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:19:28,531 >> Special tokens file saved in models/ZeroShot/0/checkpoint-846/special_tokens_map.json\n",
            " 28% 987/3525 [13:17<25:29,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:21:16,908 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:21:16,911 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:21:16,912 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:21:16,912 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.99it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.18it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.24it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.41it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.46it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.91it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 14.99it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.29it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.22it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.80it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.79it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.95it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.95it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.94it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.00it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.82it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.68it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.82it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.86it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.10it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.82it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.5684823989868164, 'eval_accuracy': 0.6278755068778992, 'eval_f1': 0.6184750265176049, 'eval_runtime': 6.1762, 'eval_samples_per_second': 119.653, 'eval_steps_per_second': 15.058, 'epoch': 7.0}\n",
            " 28% 987/3525 [13:23<25:29,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:21:23,098 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-987\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:21:23,099 >> Configuration saved in models/ZeroShot/0/checkpoint-987/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:21:25,490 >> Model weights saved in models/ZeroShot/0/checkpoint-987/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:21:25,491 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:21:25,491 >> Special tokens file saved in models/ZeroShot/0/checkpoint-987/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:21:31,147 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-846] due to args.save_total_limit\n",
            "{'loss': 0.0327, 'learning_rate': 1.4326241134751775e-05, 'epoch': 7.09}\n",
            " 32% 1128/3525 [15:14<24:01,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:23:14,021 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:23:14,024 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:23:14,024 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:23:14,024 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.49it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.24it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.53it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.56it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.32it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.88it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.05it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.04it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.84it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.73it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.81it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.88it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 14.83it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.95it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.75it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.81it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.89it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.84it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.86it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.96it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.74it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.95it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.76it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.83it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.3209831714630127, 'eval_accuracy': 0.6617050170898438, 'eval_f1': 0.659955973731484, 'eval_runtime': 6.2, 'eval_samples_per_second': 119.194, 'eval_steps_per_second': 15.0, 'epoch': 8.0}\n",
            " 32% 1128/3525 [15:20<24:01,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:23:20,225 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1128\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:23:20,226 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:23:22,630 >> Model weights saved in models/ZeroShot/0/checkpoint-1128/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:23:22,631 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1128/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:23:22,631 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1128/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:23:28,333 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-987] due to args.save_total_limit\n",
            " 36% 1269/3525 [17:11<22:39,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:25:11,080 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:25:11,081 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:25:11,082 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:25:11,082 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.50it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.10it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.12it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.44it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.31it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.85it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.03it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.42it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.13it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.04it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.83it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.04it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.06it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.79it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.69it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.96it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.21it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.11it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.94it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.04it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.16it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.84it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.87it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.94it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.85it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.85it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.76it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.5184364318847656, 'eval_accuracy': 0.6820027232170105, 'eval_f1': 0.6808713948381524, 'eval_runtime': 6.1767, 'eval_samples_per_second': 119.643, 'eval_steps_per_second': 15.057, 'epoch': 9.0}\n",
            " 36% 1269/3525 [17:17<22:39,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.86it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:25:17,260 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1269\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:25:17,261 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:25:19,695 >> Model weights saved in models/ZeroShot/0/checkpoint-1269/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:25:19,696 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1269/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:25:19,696 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1269/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:25:25,329 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-705] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:25:25,361 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1128] due to args.save_total_limit\n",
            " 40% 1410/3525 [19:08<21:16,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:27:08,133 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:27:08,135 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:27:08,135 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:27:08,135 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.20it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.18it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.31it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.41it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.29it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.82it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 14.96it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.18it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.80it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.84it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.06it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.29it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.20it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.87it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.93it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.10it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.76it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.86it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.76it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.83it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.97it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.79it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.69it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.79it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.547858476638794, 'eval_accuracy': 0.6779431700706482, 'eval_f1': 0.6776309036789396, 'eval_runtime': 6.1846, 'eval_samples_per_second': 119.49, 'eval_steps_per_second': 15.037, 'epoch': 10.0}\n",
            " 40% 1410/3525 [19:14<21:16,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:27:14,321 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1410\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:27:14,322 >> Configuration saved in models/ZeroShot/0/checkpoint-1410/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:27:16,713 >> Model weights saved in models/ZeroShot/0/checkpoint-1410/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:27:16,713 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1410/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:27:16,714 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1410/special_tokens_map.json\n",
            "{'loss': 0.0082, 'learning_rate': 1.1489361702127662e-05, 'epoch': 10.64}\n",
            " 44% 1551/3525 [21:05<19:56,  1.65it/s][INFO|trainer.py:723] 2022-08-25 02:29:05,078 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:29:05,080 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:29:05,080 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:29:05,080 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.20it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.98it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.18it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.52it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.30it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.75it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 14.92it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.17it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.02it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.70it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.79it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.94it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.03it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.93it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.90it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.75it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.87it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.90it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.86it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.92it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.74it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7451229095458984, 'eval_accuracy': 0.6617050170898438, 'eval_f1': 0.6611086653459535, 'eval_runtime': 6.1955, 'eval_samples_per_second': 119.281, 'eval_steps_per_second': 15.011, 'epoch': 11.0}\n",
            " 44% 1551/3525 [21:11<19:56,  1.65it/s]\n",
            "100% 93/93 [00:06<00:00, 14.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:29:11,277 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1551\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:29:11,278 >> Configuration saved in models/ZeroShot/0/checkpoint-1551/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:29:13,680 >> Model weights saved in models/ZeroShot/0/checkpoint-1551/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:29:13,681 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1551/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:29:13,681 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1551/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:29:19,338 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1410] due to args.save_total_limit\n",
            " 48% 1692/3525 [23:02<18:23,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:31:02,038 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:31:02,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:31:02,040 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:31:02,040 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.86it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.27it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.46it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.63it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.61it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.01it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.07it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.17it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.93it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.77it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.84it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.99it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.90it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.83it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.99it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.11it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.73it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.90it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.87it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.85it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7396249771118164, 'eval_accuracy': 0.6630581617355347, 'eval_f1': 0.6594564583738167, 'eval_runtime': 6.1815, 'eval_samples_per_second': 119.55, 'eval_steps_per_second': 15.045, 'epoch': 12.0}\n",
            " 48% 1692/3525 [23:08<18:23,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:31:08,223 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1692\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:31:08,223 >> Configuration saved in models/ZeroShot/0/checkpoint-1692/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:31:10,637 >> Model weights saved in models/ZeroShot/0/checkpoint-1692/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:31:10,638 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1692/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:31:10,638 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1692/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:31:16,327 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1551] due to args.save_total_limit\n",
            " 52% 1833/3525 [24:59<17:00,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:32:59,025 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:32:59,027 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:32:59,027 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:32:59,027 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.51it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.12it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.43it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.68it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.41it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.92it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.22it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.22it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.12it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.96it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.88it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.93it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.01it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.95it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.81it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.85it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.98it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.80it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.78it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.78it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.80it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.81it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.81it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.89it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.821417808532715, 'eval_accuracy': 0.6698240637779236, 'eval_f1': 0.6674069264708054, 'eval_runtime': 6.1849, 'eval_samples_per_second': 119.484, 'eval_steps_per_second': 15.037, 'epoch': 13.0}\n",
            " 52% 1833/3525 [25:05<17:00,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:33:05,214 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1833\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:33:05,215 >> Configuration saved in models/ZeroShot/0/checkpoint-1833/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:33:07,600 >> Model weights saved in models/ZeroShot/0/checkpoint-1833/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:33:07,601 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1833/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:33:07,601 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1833/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:33:13,254 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1692] due to args.save_total_limit\n",
            " 56% 1974/3525 [26:56<15:38,  1.65it/s][INFO|trainer.py:723] 2022-08-25 02:34:56,011 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:34:56,012 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:34:56,012 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:34:56,013 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.92it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.54it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.42it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.07it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.28it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.02it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.74it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.69it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.88it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.03it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.87it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.83it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.19it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.77it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.80it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.86it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.00it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.75it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.62it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.72it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.88it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.79it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.82it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.86433482170105, 'eval_accuracy': 0.6589986681938171, 'eval_f1': 0.6582320295983086, 'eval_runtime': 6.1881, 'eval_samples_per_second': 119.422, 'eval_steps_per_second': 15.029, 'epoch': 14.0}\n",
            " 56% 1974/3525 [27:02<15:38,  1.65it/s]\n",
            "100% 93/93 [00:06<00:00, 14.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:35:02,202 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1974\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:35:02,203 >> Configuration saved in models/ZeroShot/0/checkpoint-1974/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:35:04,594 >> Model weights saved in models/ZeroShot/0/checkpoint-1974/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:35:04,595 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1974/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:35:04,595 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1974/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:35:10,140 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1833] due to args.save_total_limit\n",
            "{'loss': 0.0034, 'learning_rate': 8.652482269503547e-06, 'epoch': 14.18}\n",
            " 60% 2115/3525 [28:53<14:08,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:36:52,861 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:36:52,863 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:36:52,863 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:36:52,863 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.55it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.43it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.44it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.54it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.55it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.05it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.05it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.18it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.00it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.01it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.91it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 14.97it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.91it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.94it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.88it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.85it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.92it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.08it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.88it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.86it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.83it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.8836312294006348, 'eval_accuracy': 0.665764570236206, 'eval_f1': 0.6656445482438065, 'eval_runtime': 6.1868, 'eval_samples_per_second': 119.447, 'eval_steps_per_second': 15.032, 'epoch': 15.0}\n",
            " 60% 2115/3525 [28:59<14:08,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:36:59,051 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2115\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:36:59,052 >> Configuration saved in models/ZeroShot/0/checkpoint-2115/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:37:01,603 >> Model weights saved in models/ZeroShot/0/checkpoint-2115/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:37:01,603 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2115/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:37:01,604 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2115/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:37:07,297 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1974] due to args.save_total_limit\n",
            " 64% 2256/3525 [30:50<12:43,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:38:50,054 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:38:50,057 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:38:50,057 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:38:50,057 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.50it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.05it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.18it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.45it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.35it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.90it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.05it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.15it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.29it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.01it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.98it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.65it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.70it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.03it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.95it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.97it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.95it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.10it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.15it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.14it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.83it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.97it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.20it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.15it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.18it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.96it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.08it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.24it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.06it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.07it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9396493434906006, 'eval_accuracy': 0.6711772680282593, 'eval_f1': 0.6682002361327443, 'eval_runtime': 6.1739, 'eval_samples_per_second': 119.697, 'eval_steps_per_second': 15.063, 'epoch': 16.0}\n",
            " 64% 2256/3525 [30:56<12:43,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:38:56,232 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2256\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:38:56,234 >> Configuration saved in models/ZeroShot/0/checkpoint-2256/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:38:58,655 >> Model weights saved in models/ZeroShot/0/checkpoint-2256/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:38:58,656 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2256/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:38:58,656 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2256/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:39:04,370 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2115] due to args.save_total_limit\n",
            " 68% 2397/3525 [32:47<11:20,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:40:47,078 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:40:47,081 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:40:47,081 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:40:47,081 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.37it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.60it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.61it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.85it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.95it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.10it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.02it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.78it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.74it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.93it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.22it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.74it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.85it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.92it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.12it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.77it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.79it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.82it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.86it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.89it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.11it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.909956455230713, 'eval_accuracy': 0.6644113659858704, 'eval_f1': 0.6644058361654752, 'eval_runtime': 6.1756, 'eval_samples_per_second': 119.664, 'eval_steps_per_second': 15.059, 'epoch': 17.0}\n",
            " 68% 2397/3525 [32:53<11:20,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:40:53,258 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2397\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:40:53,259 >> Configuration saved in models/ZeroShot/0/checkpoint-2397/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:40:55,665 >> Model weights saved in models/ZeroShot/0/checkpoint-2397/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:40:55,666 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2397/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:40:55,666 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2397/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:41:01,343 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2256] due to args.save_total_limit\n",
            "{'loss': 0.0028, 'learning_rate': 5.815602836879432e-06, 'epoch': 17.73}\n",
            " 72% 2538/3525 [34:44<09:57,  1.65it/s][INFO|trainer.py:723] 2022-08-25 02:42:44,167 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:42:44,169 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:42:44,169 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:42:44,169 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.10it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.05it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.27it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.72it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.30it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.88it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.14it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.67it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.70it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.90it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.94it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.78it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.87it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.85it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.97it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.07it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.78it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.72it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.93it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.13it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.20it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.80it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.78it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.94it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.08it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.77it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9211599826812744, 'eval_accuracy': 0.6589986681938171, 'eval_f1': 0.6589680481694722, 'eval_runtime': 6.1929, 'eval_samples_per_second': 119.33, 'eval_steps_per_second': 15.017, 'epoch': 18.0}\n",
            " 72% 2538/3525 [34:50<09:57,  1.65it/s]\n",
            "100% 93/93 [00:06<00:00, 14.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:42:50,363 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2538\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:42:50,364 >> Configuration saved in models/ZeroShot/0/checkpoint-2538/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:42:52,744 >> Model weights saved in models/ZeroShot/0/checkpoint-2538/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:42:52,745 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2538/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:42:52,745 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2538/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:42:58,421 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2397] due to args.save_total_limit\n",
            " 76% 2679/3525 [36:41<08:29,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:44:41,561 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:44:41,563 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:44:41,563 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:44:41,563 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.41it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.23it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.42it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.51it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.35it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.92it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.09it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.02it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.03it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.26it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.83it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.85it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.91it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.08it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.15it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.91it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.98it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.86it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9782819747924805, 'eval_accuracy': 0.6576454639434814, 'eval_f1': 0.6570023316950928, 'eval_runtime': 6.1698, 'eval_samples_per_second': 119.777, 'eval_steps_per_second': 15.073, 'epoch': 19.0}\n",
            " 76% 2679/3525 [36:47<08:29,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:44:47,734 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2679\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:44:47,735 >> Configuration saved in models/ZeroShot/0/checkpoint-2679/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:44:50,127 >> Model weights saved in models/ZeroShot/0/checkpoint-2679/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:44:50,128 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2679/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:44:50,128 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2679/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:44:55,775 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2538] due to args.save_total_limit\n",
            " 80% 2820/3525 [38:38<07:03,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:46:38,431 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:46:38,433 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:46:38,433 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:46:38,433 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.52it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.97it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.12it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.38it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.20it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.78it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 14.99it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.10it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.33it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.97it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.93it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.74it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.81it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.93it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.16it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.28it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.12it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.90it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.93it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.98it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.13it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.94it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.84it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.85it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.05it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.79it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.73it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.80it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.69it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.82it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.80it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.85it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0276496410369873, 'eval_accuracy': 0.6522327661514282, 'eval_f1': 0.6511058000540088, 'eval_runtime': 6.1938, 'eval_samples_per_second': 119.313, 'eval_steps_per_second': 15.015, 'epoch': 20.0}\n",
            " 80% 2820/3525 [38:44<07:03,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:46:44,631 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2820\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:46:44,632 >> Configuration saved in models/ZeroShot/0/checkpoint-2820/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:46:47,001 >> Model weights saved in models/ZeroShot/0/checkpoint-2820/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:46:47,002 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2820/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:46:47,002 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2820/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:46:52,703 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2679] due to args.save_total_limit\n",
            " 84% 2961/3525 [40:35<05:39,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:48:35,510 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:48:35,513 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:48:35,513 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:48:35,513 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.58it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.55it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.39it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.61it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.54it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.05it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.12it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.28it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.13it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.86it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.84it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.63it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.74it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.77it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.95it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.80it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.81it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.93it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.86it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.84it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.07it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.04014253616333, 'eval_accuracy': 0.6535859107971191, 'eval_f1': 0.6520562053998382, 'eval_runtime': 6.188, 'eval_samples_per_second': 119.424, 'eval_steps_per_second': 15.029, 'epoch': 21.0}\n",
            " 84% 2961/3525 [40:41<05:39,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.92it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:48:41,702 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2961\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:48:41,703 >> Configuration saved in models/ZeroShot/0/checkpoint-2961/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:48:44,152 >> Model weights saved in models/ZeroShot/0/checkpoint-2961/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:48:44,153 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2961/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:48:44,153 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2961/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:48:49,834 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2820] due to args.save_total_limit\n",
            "{'loss': 0.0011, 'learning_rate': 2.978723404255319e-06, 'epoch': 21.28}\n",
            " 88% 3102/3525 [42:32<04:14,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:50:32,613 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:50:32,616 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:50:32,617 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:50:32,617 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.09it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 17.48it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.49it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.61it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.19it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.41it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 14.93it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.00it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.84it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.92it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.02it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.90it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.66it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.63it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.79it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.87it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.90it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.81it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.88it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.86it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.98it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.10it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0515213012695312, 'eval_accuracy': 0.6576454639434814, 'eval_f1': 0.656919251695063, 'eval_runtime': 6.1821, 'eval_samples_per_second': 119.539, 'eval_steps_per_second': 15.043, 'epoch': 22.0}\n",
            " 88% 3102/3525 [42:38<04:14,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.97it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:50:38,800 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-3102\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:50:38,801 >> Configuration saved in models/ZeroShot/0/checkpoint-3102/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:50:41,184 >> Model weights saved in models/ZeroShot/0/checkpoint-3102/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:50:41,185 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-3102/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:50:41,185 >> Special tokens file saved in models/ZeroShot/0/checkpoint-3102/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:50:46,835 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2961] due to args.save_total_limit\n",
            " 92% 3243/3525 [44:29<02:49,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:52:29,518 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:52:29,519 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:52:29,519 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:52:29,520 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.43it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.26it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.49it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.60it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.38it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.87it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.16it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.29it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.35it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.17it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.79it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.79it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.98it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.04it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.73it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.76it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.76it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.69it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.79it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.89it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.00it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.85it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.17it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.83it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.76it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.78it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.085827112197876, 'eval_accuracy': 0.6644113659858704, 'eval_f1': 0.6630484343055494, 'eval_runtime': 6.1899, 'eval_samples_per_second': 119.389, 'eval_steps_per_second': 15.025, 'epoch': 23.0}\n",
            " 92% 3243/3525 [44:35<02:49,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.72it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:52:35,711 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-3243\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:52:35,712 >> Configuration saved in models/ZeroShot/0/checkpoint-3243/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:52:38,095 >> Model weights saved in models/ZeroShot/0/checkpoint-3243/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:52:38,095 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-3243/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:52:38,096 >> Special tokens file saved in models/ZeroShot/0/checkpoint-3243/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:52:43,772 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-3102] due to args.save_total_limit\n",
            " 96% 3384/3525 [46:26<01:24,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:54:26,469 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:54:26,471 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:54:26,471 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:54:26,472 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.82it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.19it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.53it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.69it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.38it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.85it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.12it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.18it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.77it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.87it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.95it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.04it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.78it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.90it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.90it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.11it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.10it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.15it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.09it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.17it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.94it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.06it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.79it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.74it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0932223796844482, 'eval_accuracy': 0.6576454639434814, 'eval_f1': 0.6567378464668052, 'eval_runtime': 6.1683, 'eval_samples_per_second': 119.806, 'eval_steps_per_second': 15.077, 'epoch': 24.0}\n",
            " 96% 3384/3525 [46:32<01:24,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:54:32,642 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-3384\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:54:32,643 >> Configuration saved in models/ZeroShot/0/checkpoint-3384/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:54:35,071 >> Model weights saved in models/ZeroShot/0/checkpoint-3384/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:54:35,072 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-3384/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:54:35,072 >> Special tokens file saved in models/ZeroShot/0/checkpoint-3384/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:54:40,698 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-3243] due to args.save_total_limit\n",
            "{'loss': 0.0005, 'learning_rate': 1.4184397163120568e-07, 'epoch': 24.82}\n",
            "100% 3525/3525 [48:23<00:00,  1.66it/s][INFO|trainer.py:723] 2022-08-25 02:56:23,535 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:56:23,537 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:56:23,538 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:56:23,538 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.39it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.18it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.40it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.57it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.38it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.86it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 14.96it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.12it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.01it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.98it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.69it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.72it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.00it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.91it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.84it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.86it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.86it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.03it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.97it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.86it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.82it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.79it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.082810163497925, 'eval_accuracy': 0.6562922596931458, 'eval_f1': 0.6554285252738861, 'eval_runtime': 6.1901, 'eval_samples_per_second': 119.383, 'eval_steps_per_second': 15.024, 'epoch': 25.0}\n",
            "100% 3525/3525 [48:29<00:00,  1.66it/s]\n",
            "100% 93/93 [00:06<00:00, 14.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:56:29,729 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-3525\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:56:29,730 >> Configuration saved in models/ZeroShot/0/checkpoint-3525/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:56:32,118 >> Model weights saved in models/ZeroShot/0/checkpoint-3525/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:56:32,118 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-3525/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:56:32,119 >> Special tokens file saved in models/ZeroShot/0/checkpoint-3525/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 02:56:37,750 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-3384] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2022-08-25 02:56:37,906 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1942] 2022-08-25 02:56:37,907 >> Loading best model from models/ZeroShot/0/checkpoint-1269 (score: 0.6808713948381524).\n",
            "{'train_runtime': 2922.1457, 'train_samples_per_second': 38.422, 'train_steps_per_second': 1.206, 'train_loss': 0.05346214252134376, 'epoch': 25.0}\n",
            "100% 3525/3525 [48:42<00:00,  1.21it/s]\n",
            "[INFO|trainer.py:2640] 2022-08-25 02:56:41,957 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:56:41,958 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:56:44,354 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:56:44,354 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:56:44,355 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0535\n",
            "  train_runtime            = 0:48:42.14\n",
            "  train_samples            =       4491\n",
            "  train_samples_per_second =     38.422\n",
            "  train_steps_per_second   =      1.206\n",
            "08/25/2022 02:56:44 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:723] 2022-08-25 02:56:44,638 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:56:44,640 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:56:44,640 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:56:44,640 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 15.69it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =      0.682\n",
            "  eval_f1                 =     0.6809\n",
            "  eval_loss               =     2.5184\n",
            "  eval_runtime            = 0:00:06.01\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    122.932\n",
            "  eval_steps_per_second   =      15.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for zero-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "wZJLePe_xqR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install adapter-transformers\n",
        "!pip install git+https://github.com/adapter-hub/adapter-transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_Svn1vx3zA",
        "outputId": "b0d644e0-4b41-4aaa-d346-2190d81f7bac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/adapter-hub/adapter-transformers.git\n",
            "  Cloning https://github.com/adapter-hub/adapter-transformers.git to /tmp/pip-req-build-2ssu_vmn\n",
            "  Running command git clone -q https://github.com/adapter-hub/adapter-transformers.git /tmp/pip-req-build-2ssu_vmn\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (4.12.0)\n",
            "Collecting huggingface-hub<0.8.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.8.0,>=0.1.0->adapter-transformers==3.1.0a0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->adapter-transformers==3.1.0a0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers==3.1.0a0) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (2022.6.15)\n",
            "Building wheels for collected packages: adapter-transformers\n",
            "  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adapter-transformers: filename=adapter_transformers-3.1.0a0-py3-none-any.whl size=4268660 sha256=b7881ca8d65c079a29e8f134949ca54d8a72d85f6871da410f03dfe9b7039c20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sy78b6gu/wheels/0e/10/59/2e0642953eade6187066db14e31def5345c4ed7d757d32de99\n",
            "Successfully built adapter-transformers\n",
            "Installing collected packages: huggingface-hub, adapter-transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.9.0\n",
            "    Uninstalling huggingface-hub-0.9.0:\n",
            "      Successfully uninstalled huggingface-hub-0.9.0\n",
            "Successfully installed adapter-transformers-3.1.0a0 huggingface-hub-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/ZeroShot/0/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --seed 0 \\\n",
        "  --train_file      Data/ZeroShot/EN/train.csv \\\n",
        "  --validation_file Data/ZeroShot/EN/dev.csv \\\n",
        "\t--evaluation_strategy \"epoch\" \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x427RCa4x5U9",
        "outputId": "124b2f84-aca3-404b-957f-a4ebe5c5837f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 03:57:02 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 03:57:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Aug25_03-57-02_42ef7f7e098f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 03:57:02 - INFO - __main__ - load a local file for train: Data/ZeroShot/EN/train.csv\n",
            "08/25/2022 03:57:02 - INFO - __main__ - load a local file for validation: Data/ZeroShot/EN/dev.csv\n",
            "08/25/2022 03:57:02 - WARNING - datasets.builder - Using custom data configuration default-e2fb07d32e4e9172\n",
            "08/25/2022 03:57:02 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "08/25/2022 03:57:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-e2fb07d32e4e9172/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
            "08/25/2022 03:57:02 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-e2fb07d32e4e9172/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "08/25/2022 03:57:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-e2fb07d32e4e9172/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
            "100% 2/2 [00:00<00:00, 1030.29it/s]\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:03,077 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkzd1887u\n",
            "Downloading: 100% 625/625 [00:00<00:00, 578kB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:03,209 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:03,209 >> creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 03:57:03,210 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 03:57:03,210 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:03,336 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpezqvacft\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 32.9kB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:03,474 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:03,474 >> creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 03:57:03,607 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 03:57:03,608 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:03,878 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprrbvta1c\n",
            "Downloading: 100% 972k/972k [00:00<00:00, 4.87MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:04,251 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:04,252 >> creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:04,377 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpegd44z9g\n",
            "Downloading: 100% 1.87M/1.87M [00:00<00:00, 9.86MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:04,762 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:04,762 >> creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 03:57:05,161 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 03:57:05,161 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 03:57:05,161 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 03:57:05,161 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 03:57:05,161 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 03:57:05,291 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 03:57:05,291 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:05,550 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn0jhrc2h\n",
            "Downloading: 100% 681M/681M [00:10<00:00, 71.4MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:15,636 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:15,636 >> creating metadata file for /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 03:57:15,636 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 03:57:17,759 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 03:57:17,759 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 03:57:17,769 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 03:57:17,770 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 03:57:18,019 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:18,189 >> https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/v2/index/bert-base-multilingual-cased.json not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmpt3oigiv6\n",
            "Downloading: 24.6kB [00:00, 28.3MB/s]       \n",
            "[INFO|hub.py:595] 2022-08-25 03:57:18,238 >> storing https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/v2/index/bert-base-multilingual-cased.json in cache at ~/.cache/torch/adapters/08d069267cb1d67d820deec1e19b1a91aef2bdf0e64fa8456c440abe23c3ddb4.a7da128024bcd4efcca057684afe69285b8f3279e656c8ec9d8af63e19290817\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:18,238 >> creating metadata file for ~/.cache/torch/adapters/08d069267cb1d67d820deec1e19b1a91aef2bdf0e64fa8456c440abe23c3ddb4.a7da128024bcd4efcca057684afe69285b8f3279e656c8ec9d8af63e19290817\n",
            "[INFO|utils.py:327] 2022-08-25 03:57:18,239 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 03:57:18,370 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:19,157 >> https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmp39mdp609\n",
            "Downloading: 100% 28.2M/28.2M [00:04<00:00, 6.64MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:24,256 >> storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip in cache at ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf.3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:24,256 >> creating metadata file for ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf.3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a\n",
            "[INFO|loading.py:77] 2022-08-25 03:57:24,396 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 03:57:24,396 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 03:57:24,513 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 03:57:24,525 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 03:57:24,525 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 03:57:24,573 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 03:57:24,709 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|hub.py:591] 2022-08-25 03:57:25,226 >> https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmppczjwgth\n",
            "Downloading: 100% 381M/381M [00:17<00:00, 22.6MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 03:57:43,537 >> storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip in cache at ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e.babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c\n",
            "[INFO|hub.py:603] 2022-08-25 03:57:43,537 >> creating metadata file for ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e.babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c\n",
            "[INFO|loading.py:77] 2022-08-25 03:57:46,129 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 03:57:46,130 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 03:57:46,266 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 03:57:46,278 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 03:57:46,278 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 03:57:47,459 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 03:57:47,568 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 03:57:47,637 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with EN language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]08/25/2022 03:57:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e2fb07d32e4e9172/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-3b76165cdb2a47d2.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  4.94ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 03:57:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e2fb07d32e4e9172/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-c5868b596386bdff.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 11.56ba/s]\n",
            "08/25/2022 03:57:48 - INFO - __main__ - Sample 1577 of the training set: {'label': 1, 'sentence1': 'Where do I stream Stag Night online? Stag Night is available to watch and stream, download, buy on demand at Amazon Prime, Amazon, Vudu, Google Play, iTunes, YouTube VOD online. Some platforms allow you to rent Stag Night for a limited time or purchase the movie and download it to your device.', 'input_ids': [101, 23525, 10149, 146, 41878, 10838, 14520, 12703, 13893, 136, 10838, 14520, 12703, 10124, 14579, 10114, 34481, 10111, 41878, 117, 13737, 117, 47715, 10135, 34394, 10160, 27986, 19924, 117, 27986, 117, 100154, 11460, 117, 13888, 17712, 117, 26945, 117, 17665, 159, 77836, 13893, 119, 13885, 51325, 21992, 13028, 10114, 60727, 10838, 14520, 12703, 10142, 169, 19264, 10635, 10345, 37891, 10105, 18379, 10111, 13737, 10271, 10114, 20442, 33091, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 03:57:48 - INFO - __main__ - Sample 3104 of the training set: {'label': 1, 'sentence1': 'Did Biden think for himself before he reversed direction on the Keystone Pipeline or did he do it just because the last Democrat president, under whom he served, was against it? I applaud Biden’s decisions to rejoin the World Health Organization and to call a world conference on climate change because, for whatever reason, the climate is changing. The atmosphere is getting warmer and the storms are getting bigger, as evidenced by the one in California earlier this week.', 'input_ids': [101, 50133, 31156, 10633, 27874, 10142, 14764, 11360, 10261, 60971, 10162, 15599, 10135, 10105, 52286, 20124, 38329, 77558, 10238, 10345, 12172, 10261, 10149, 10271, 12820, 12373, 10105, 12469, 45338, 12931, 117, 10571, 18104, 10261, 12325, 117, 10134, 11327, 10271, 136, 146, 72894, 35166, 10162, 31156, 10633, 100, 187, 48126, 10114, 11639, 11039, 10245, 10105, 10315, 15931, 23929, 10111, 10114, 20575, 169, 11356, 25029, 10135, 15648, 15453, 12373, 117, 10142, 104429, 27949, 117, 10105, 15648, 10124, 43068, 119, 10117, 59043, 10124, 34875, 50089, 10165, 10111, 10105, 31642, 10107, 10301, 34875, 110102, 117, 10146, 18713, 10162, 10155, 10105, 10464, 10106, 11621, 18905, 10531, 16118, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 03:57:48 - INFO - __main__ - Sample 1722 of the training set: {'label': 1, 'sentence1': 'Mank leads the Critics Choice Awards 2021 nominations Coronavirus latest: Covid national research project will study effects of emerging mutations Jared Kushner and Ivanka Trump made up to $640 million while working in White House, report finds', 'input_ids': [101, 11343, 10174, 34868, 10105, 33120, 27844, 12357, 67267, 66408, 32769, 37715, 10251, 50908, 131, 13098, 32194, 11844, 14108, 13920, 11337, 14687, 21274, 10108, 78526, 105083, 77741, 49869, 13264, 11129, 10111, 15631, 10371, 29846, 11019, 10741, 10114, 109, 30127, 12473, 11371, 14616, 10106, 12136, 11545, 117, 17553, 31478, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 03:57:55,395 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 03:57:55,411 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 03:57:55,411 >>   Num examples = 3327\n",
            "[INFO|trainer.py:1421] 2022-08-25 03:57:55,412 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 03:57:55,412 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 03:57:55,412 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 03:57:55,412 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 03:57:55,412 >>   Total optimization steps = 2600\n",
            "  4% 104/2600 [00:58<22:35,  1.84it/s][INFO|trainer.py:623] 2022-08-25 03:58:53,877 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 03:58:53,879 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 03:58:53,879 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 03:58:53,879 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.04it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 15.71it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 14.92it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 14.50it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 14.13it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:03, 13.96it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:03, 13.86it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 13.83it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 13.73it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 13.70it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 13.69it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 13.68it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 13.63it/s]\u001b[A\n",
            " 51% 30/59 [00:02<00:02, 13.67it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 13.70it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 13.68it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 13.68it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 13.61it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 13.61it/s]\u001b[A\n",
            " 71% 42/59 [00:03<00:01, 13.58it/s]\u001b[A\n",
            " 75% 44/59 [00:03<00:01, 13.56it/s]\u001b[A\n",
            " 78% 46/59 [00:03<00:00, 13.58it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 13.61it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 13.58it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 13.60it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 13.59it/s]\u001b[A\n",
            " 95% 56/59 [00:04<00:00, 13.65it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6321374177932739, 'eval_accuracy': 0.6502146124839783, 'eval_f1': 0.6028028342092191, 'eval_runtime': 4.2886, 'eval_samples_per_second': 108.66, 'eval_steps_per_second': 13.757, 'epoch': 1.0}\n",
            "  4% 104/2600 [01:02<22:35,  1.84it/s]\n",
            "100% 59/59 [00:04<00:00, 13.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 03:58:58,169 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-104\n",
            "[INFO|loading.py:60] 2022-08-25 03:58:58,171 >> Configuration saved in models/ZeroShot/0/checkpoint-104/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:58:58,296 >> Module weights saved in models/ZeroShot/0/checkpoint-104/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 03:58:58,297 >> Configuration saved in models/ZeroShot/0/checkpoint-104/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:58:58,305 >> Module weights saved in models/ZeroShot/0/checkpoint-104/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 03:58:58,306 >> Configuration saved in models/ZeroShot/0/checkpoint-104/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:58:58,398 >> Module weights saved in models/ZeroShot/0/checkpoint-104/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 03:58:58,399 >> Configuration saved in models/ZeroShot/0/checkpoint-104/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:58:58,505 >> Module weights saved in models/ZeroShot/0/checkpoint-104/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 03:58:58,506 >> Configuration saved in models/ZeroShot/0/checkpoint-104/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:59:00,253 >> Module weights saved in models/ZeroShot/0/checkpoint-104/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 03:59:00,253 >> Configuration saved in models/ZeroShot/0/checkpoint-104/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:59:00,265 >> Module weights saved in models/ZeroShot/0/checkpoint-104/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 03:59:00,265 >> Configuration saved in models/ZeroShot/0/checkpoint-104/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 03:59:01,707 >> Module weights saved in models/ZeroShot/0/checkpoint-104/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 03:59:01,708 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-104/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 03:59:01,708 >> Special tokens file saved in models/ZeroShot/0/checkpoint-104/special_tokens_map.json\n",
            "  8% 208/2600 [02:05<22:23,  1.78it/s][INFO|trainer.py:623] 2022-08-25 04:00:00,436 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:00:00,438 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:00:00,438 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:00:00,438 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.84it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.77it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.67it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.08it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.82it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.46it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.35it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.28it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 13.25it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.22it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.21it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.24it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.23it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.14it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.13it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.11it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.13it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.13it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.22it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 13.19it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.16it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.08it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.12it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.14it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.22it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.20it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 13.20it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 13.19it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2286087274551392, 'eval_accuracy': 0.6223176121711731, 'eval_f1': 0.6186011904761906, 'eval_runtime': 4.4323, 'eval_samples_per_second': 105.137, 'eval_steps_per_second': 13.311, 'epoch': 2.0}\n",
            "  8% 208/2600 [02:09<22:23,  1.78it/s]\n",
            "100% 59/59 [00:04<00:00, 14.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:00:04,871 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-208\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:04,872 >> Configuration saved in models/ZeroShot/0/checkpoint-208/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:04,961 >> Module weights saved in models/ZeroShot/0/checkpoint-208/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:04,961 >> Configuration saved in models/ZeroShot/0/checkpoint-208/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:04,969 >> Module weights saved in models/ZeroShot/0/checkpoint-208/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:04,969 >> Configuration saved in models/ZeroShot/0/checkpoint-208/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:05,022 >> Module weights saved in models/ZeroShot/0/checkpoint-208/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:05,023 >> Configuration saved in models/ZeroShot/0/checkpoint-208/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:05,080 >> Module weights saved in models/ZeroShot/0/checkpoint-208/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:05,081 >> Configuration saved in models/ZeroShot/0/checkpoint-208/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:06,545 >> Module weights saved in models/ZeroShot/0/checkpoint-208/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:06,554 >> Configuration saved in models/ZeroShot/0/checkpoint-208/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:06,628 >> Module weights saved in models/ZeroShot/0/checkpoint-208/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:00:06,628 >> Configuration saved in models/ZeroShot/0/checkpoint-208/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:00:08,131 >> Module weights saved in models/ZeroShot/0/checkpoint-208/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:00:08,132 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-208/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:00:08,132 >> Special tokens file saved in models/ZeroShot/0/checkpoint-208/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:00:08,598 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-104] due to args.save_total_limit\n",
            " 12% 312/2600 [03:12<21:46,  1.75it/s][INFO|trainer.py:623] 2022-08-25 04:01:07,903 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:01:07,904 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:01:07,904 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:01:07,904 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.50it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.42it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.39it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.84it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.54it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.32it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.20it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.17it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 13.07it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.02it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.07it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.98it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.91it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.94it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.94it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.98it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.97it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.95it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.97it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.95it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.95it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.95it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.90it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.92it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.90it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.88it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.96it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.96it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9970883727073669, 'eval_accuracy': 0.6845493316650391, 'eval_f1': 0.6720430879712747, 'eval_runtime': 4.5044, 'eval_samples_per_second': 103.454, 'eval_steps_per_second': 13.098, 'epoch': 3.0}\n",
            " 12% 312/2600 [03:16<21:46,  1.75it/s]\n",
            "100% 59/59 [00:04<00:00, 14.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:01:12,410 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-312\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:12,411 >> Configuration saved in models/ZeroShot/0/checkpoint-312/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:12,500 >> Module weights saved in models/ZeroShot/0/checkpoint-312/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:12,501 >> Configuration saved in models/ZeroShot/0/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:12,508 >> Module weights saved in models/ZeroShot/0/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:12,509 >> Configuration saved in models/ZeroShot/0/checkpoint-312/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:12,562 >> Module weights saved in models/ZeroShot/0/checkpoint-312/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:12,563 >> Configuration saved in models/ZeroShot/0/checkpoint-312/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:12,627 >> Module weights saved in models/ZeroShot/0/checkpoint-312/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:12,628 >> Configuration saved in models/ZeroShot/0/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:14,067 >> Module weights saved in models/ZeroShot/0/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:14,140 >> Configuration saved in models/ZeroShot/0/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:14,160 >> Module weights saved in models/ZeroShot/0/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:01:14,161 >> Configuration saved in models/ZeroShot/0/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:01:15,631 >> Module weights saved in models/ZeroShot/0/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:01:15,632 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:01:15,632 >> Special tokens file saved in models/ZeroShot/0/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:01:16,126 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-208] due to args.save_total_limit\n",
            " 16% 416/2600 [04:20<21:03,  1.73it/s][INFO|trainer.py:623] 2022-08-25 04:02:16,311 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:02:16,312 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:02:16,313 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:02:16,313 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.80it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.97it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.04it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.59it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.35it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.14it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.86it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.71it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.76it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.77it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.79it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.81it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.83it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.71it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.77it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.77it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.71it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.71it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.79it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.71it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.70it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.77it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.79it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.74it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.77it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.78it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.4200609922409058, 'eval_accuracy': 0.6523604989051819, 'eval_f1': 0.6497801035461783, 'eval_runtime': 4.5831, 'eval_samples_per_second': 101.678, 'eval_steps_per_second': 12.873, 'epoch': 4.0}\n",
            " 16% 416/2600 [04:25<21:03,  1.73it/s]\n",
            "100% 59/59 [00:04<00:00, 14.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:02:20,897 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-416\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:20,897 >> Configuration saved in models/ZeroShot/0/checkpoint-416/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:20,990 >> Module weights saved in models/ZeroShot/0/checkpoint-416/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:20,990 >> Configuration saved in models/ZeroShot/0/checkpoint-416/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:20,998 >> Module weights saved in models/ZeroShot/0/checkpoint-416/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:20,999 >> Configuration saved in models/ZeroShot/0/checkpoint-416/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:21,054 >> Module weights saved in models/ZeroShot/0/checkpoint-416/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:21,054 >> Configuration saved in models/ZeroShot/0/checkpoint-416/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:21,110 >> Module weights saved in models/ZeroShot/0/checkpoint-416/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:21,111 >> Configuration saved in models/ZeroShot/0/checkpoint-416/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:22,577 >> Module weights saved in models/ZeroShot/0/checkpoint-416/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:22,730 >> Configuration saved in models/ZeroShot/0/checkpoint-416/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:22,751 >> Module weights saved in models/ZeroShot/0/checkpoint-416/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:02:22,752 >> Configuration saved in models/ZeroShot/0/checkpoint-416/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:02:24,093 >> Module weights saved in models/ZeroShot/0/checkpoint-416/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:02:24,094 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-416/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:02:24,094 >> Special tokens file saved in models/ZeroShot/0/checkpoint-416/special_tokens_map.json\n",
            "{'loss': 0.2742, 'learning_rate': 8.076923076923078e-05, 'epoch': 4.81}\n",
            " 20% 520/2600 [05:29<20:15,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:03:25,118 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:03:25,120 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:03:25,120 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:03:25,120 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.75it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.03it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.10it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.23it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.92it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.87it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.79it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.77it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.74it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.69it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.70it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.72it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.60it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.65it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.69it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.59it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.63it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2935694456100464, 'eval_accuracy': 0.6609442234039307, 'eval_f1': 0.6559760770021493, 'eval_runtime': 4.6152, 'eval_samples_per_second': 100.97, 'eval_steps_per_second': 12.784, 'epoch': 5.0}\n",
            " 20% 520/2600 [05:34<20:15,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:03:29,736 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-520\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:29,737 >> Configuration saved in models/ZeroShot/0/checkpoint-520/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:29,825 >> Module weights saved in models/ZeroShot/0/checkpoint-520/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:29,826 >> Configuration saved in models/ZeroShot/0/checkpoint-520/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:29,834 >> Module weights saved in models/ZeroShot/0/checkpoint-520/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:29,835 >> Configuration saved in models/ZeroShot/0/checkpoint-520/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:29,886 >> Module weights saved in models/ZeroShot/0/checkpoint-520/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:29,887 >> Configuration saved in models/ZeroShot/0/checkpoint-520/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:29,943 >> Module weights saved in models/ZeroShot/0/checkpoint-520/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:29,943 >> Configuration saved in models/ZeroShot/0/checkpoint-520/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:31,344 >> Module weights saved in models/ZeroShot/0/checkpoint-520/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:31,344 >> Configuration saved in models/ZeroShot/0/checkpoint-520/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:31,355 >> Module weights saved in models/ZeroShot/0/checkpoint-520/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:03:31,356 >> Configuration saved in models/ZeroShot/0/checkpoint-520/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:03:32,800 >> Module weights saved in models/ZeroShot/0/checkpoint-520/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:03:32,800 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:03:32,800 >> Special tokens file saved in models/ZeroShot/0/checkpoint-520/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:03:33,238 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-416] due to args.save_total_limit\n",
            " 24% 624/2600 [06:38<19:22,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:04:34,426 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:04:34,428 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:04:34,428 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:04:34,428 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.85it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.10it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.04it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.43it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.01it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.61it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.56it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.58it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.67it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.62it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.55it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.53it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.46it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.44it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.41it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.42it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.47it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.54it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.54it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.48it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.43it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.40it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.39it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.39it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.6320840120315552, 'eval_accuracy': 0.6695278882980347, 'eval_f1': 0.6561248155176048, 'eval_runtime': 4.6732, 'eval_samples_per_second': 99.717, 'eval_steps_per_second': 12.625, 'epoch': 6.0}\n",
            " 24% 624/2600 [06:43<19:22,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:04:39,102 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-624\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:39,103 >> Configuration saved in models/ZeroShot/0/checkpoint-624/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:39,188 >> Module weights saved in models/ZeroShot/0/checkpoint-624/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:39,189 >> Configuration saved in models/ZeroShot/0/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:39,196 >> Module weights saved in models/ZeroShot/0/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:39,197 >> Configuration saved in models/ZeroShot/0/checkpoint-624/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:39,246 >> Module weights saved in models/ZeroShot/0/checkpoint-624/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:39,246 >> Configuration saved in models/ZeroShot/0/checkpoint-624/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:39,322 >> Module weights saved in models/ZeroShot/0/checkpoint-624/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:39,322 >> Configuration saved in models/ZeroShot/0/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:40,756 >> Module weights saved in models/ZeroShot/0/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:40,792 >> Configuration saved in models/ZeroShot/0/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:40,817 >> Module weights saved in models/ZeroShot/0/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:04:40,818 >> Configuration saved in models/ZeroShot/0/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:04:42,273 >> Module weights saved in models/ZeroShot/0/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:04:42,273 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:04:42,273 >> Special tokens file saved in models/ZeroShot/0/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:04:42,730 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-520] due to args.save_total_limit\n",
            " 28% 728/2600 [07:48<18:16,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:05:44,037 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:05:44,039 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:05:44,039 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:05:44,039 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.71it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.89it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.93it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.40it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.12it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.01it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.93it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.75it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.71it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.62it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.63it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.65it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.60it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.60it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.60it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.49it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.45it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.50it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.54it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.60it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.66it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.69it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.70it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.6146185398101807, 'eval_accuracy': 0.6716738343238831, 'eval_f1': 0.658657091561939, 'eval_runtime': 4.6271, 'eval_samples_per_second': 100.711, 'eval_steps_per_second': 12.751, 'epoch': 7.0}\n",
            " 28% 728/2600 [07:53<18:16,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:05:48,667 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-728\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:48,668 >> Configuration saved in models/ZeroShot/0/checkpoint-728/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:48,755 >> Module weights saved in models/ZeroShot/0/checkpoint-728/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:48,755 >> Configuration saved in models/ZeroShot/0/checkpoint-728/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:48,763 >> Module weights saved in models/ZeroShot/0/checkpoint-728/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:48,763 >> Configuration saved in models/ZeroShot/0/checkpoint-728/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:48,817 >> Module weights saved in models/ZeroShot/0/checkpoint-728/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:48,817 >> Configuration saved in models/ZeroShot/0/checkpoint-728/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:48,874 >> Module weights saved in models/ZeroShot/0/checkpoint-728/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:48,875 >> Configuration saved in models/ZeroShot/0/checkpoint-728/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:50,260 >> Module weights saved in models/ZeroShot/0/checkpoint-728/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:50,418 >> Configuration saved in models/ZeroShot/0/checkpoint-728/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:50,435 >> Module weights saved in models/ZeroShot/0/checkpoint-728/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:05:50,436 >> Configuration saved in models/ZeroShot/0/checkpoint-728/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:05:51,855 >> Module weights saved in models/ZeroShot/0/checkpoint-728/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:05:51,856 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-728/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:05:51,856 >> Special tokens file saved in models/ZeroShot/0/checkpoint-728/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:05:52,345 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-624] due to args.save_total_limit\n",
            " 32% 832/2600 [08:58<17:16,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:06:53,731 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:06:53,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:06:53,732 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:06:53,732 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.04it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.07it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.02it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.39it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.07it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.97it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.92it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.81it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.68it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.59it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.47it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.38it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.40it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.42it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.45it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.53it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.55it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.53it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.51it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.64it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.59it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.56it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.55it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.031683921813965, 'eval_accuracy': 0.6587982773780823, 'eval_f1': 0.6470554610800692, 'eval_runtime': 4.6442, 'eval_samples_per_second': 100.339, 'eval_steps_per_second': 12.704, 'epoch': 8.0}\n",
            " 32% 832/2600 [09:02<17:16,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:06:58,378 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-832\n",
            "[INFO|loading.py:60] 2022-08-25 04:06:58,379 >> Configuration saved in models/ZeroShot/0/checkpoint-832/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:06:58,467 >> Module weights saved in models/ZeroShot/0/checkpoint-832/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:06:58,467 >> Configuration saved in models/ZeroShot/0/checkpoint-832/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:06:58,475 >> Module weights saved in models/ZeroShot/0/checkpoint-832/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:06:58,475 >> Configuration saved in models/ZeroShot/0/checkpoint-832/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:06:58,524 >> Module weights saved in models/ZeroShot/0/checkpoint-832/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:06:58,525 >> Configuration saved in models/ZeroShot/0/checkpoint-832/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:06:58,582 >> Module weights saved in models/ZeroShot/0/checkpoint-832/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:06:58,583 >> Configuration saved in models/ZeroShot/0/checkpoint-832/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:06:59,933 >> Module weights saved in models/ZeroShot/0/checkpoint-832/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:07:00,108 >> Configuration saved in models/ZeroShot/0/checkpoint-832/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:07:00,127 >> Module weights saved in models/ZeroShot/0/checkpoint-832/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:07:00,128 >> Configuration saved in models/ZeroShot/0/checkpoint-832/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:07:01,553 >> Module weights saved in models/ZeroShot/0/checkpoint-832/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:07:01,554 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-832/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:07:01,554 >> Special tokens file saved in models/ZeroShot/0/checkpoint-832/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:07:02,008 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-728] due to args.save_total_limit\n",
            " 36% 936/2600 [10:07<16:13,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:08:03,322 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:08:03,324 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:08:03,324 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:08:03,324 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.66it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.80it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.82it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.34it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.99it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.73it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.60it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.48it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.58it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.60it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.51it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.53it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.58it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.63it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.53it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.44it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.37it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.30it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.39it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.43it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.57it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.065314531326294, 'eval_accuracy': 0.6545064449310303, 'eval_f1': 0.6408090963494915, 'eval_runtime': 4.6707, 'eval_samples_per_second': 99.77, 'eval_steps_per_second': 12.632, 'epoch': 9.0}\n",
            " 36% 936/2600 [10:12<16:13,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.89it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:08:07,996 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-936\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:07,997 >> Configuration saved in models/ZeroShot/0/checkpoint-936/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:08,083 >> Module weights saved in models/ZeroShot/0/checkpoint-936/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:08,083 >> Configuration saved in models/ZeroShot/0/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:08,090 >> Module weights saved in models/ZeroShot/0/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:08,091 >> Configuration saved in models/ZeroShot/0/checkpoint-936/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:08,141 >> Module weights saved in models/ZeroShot/0/checkpoint-936/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:08,142 >> Configuration saved in models/ZeroShot/0/checkpoint-936/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:08,202 >> Module weights saved in models/ZeroShot/0/checkpoint-936/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:08,203 >> Configuration saved in models/ZeroShot/0/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:09,575 >> Module weights saved in models/ZeroShot/0/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:09,627 >> Configuration saved in models/ZeroShot/0/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:09,727 >> Module weights saved in models/ZeroShot/0/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:08:09,728 >> Configuration saved in models/ZeroShot/0/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:08:11,008 >> Module weights saved in models/ZeroShot/0/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:08:11,009 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:08:11,009 >> Special tokens file saved in models/ZeroShot/0/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:08:11,464 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-832] due to args.save_total_limit\n",
            "{'loss': 0.0443, 'learning_rate': 6.153846153846155e-05, 'epoch': 9.62}\n",
            " 40% 1040/2600 [11:17<15:16,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:09:12,748 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:09:12,749 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:09:12,749 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:09:12,749 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.59it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.62it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.78it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.23it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.04it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.92it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.84it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.62it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.51it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.42it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.38it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.28it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.33it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.35it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.28it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.31it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.30it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.34it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.37it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.39it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.42it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.49it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.51it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.047518730163574, 'eval_accuracy': 0.667382001876831, 'eval_f1': 0.6473747162350185, 'eval_runtime': 4.6955, 'eval_samples_per_second': 99.244, 'eval_steps_per_second': 12.565, 'epoch': 10.0}\n",
            " 40% 1040/2600 [11:22<15:16,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:09:17,446 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1040\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:17,447 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:17,533 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:17,534 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:17,541 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:17,541 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:17,597 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:17,597 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:17,663 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:17,663 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:19,030 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:19,162 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:19,222 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:09:19,223 >> Configuration saved in models/ZeroShot/0/checkpoint-1040/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:09:20,596 >> Module weights saved in models/ZeroShot/0/checkpoint-1040/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:09:20,597 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1040/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:09:20,597 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1040/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:09:21,069 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-936] due to args.save_total_limit\n",
            " 44% 1144/2600 [12:26<14:16,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:10:22,299 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:10:22,300 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:10:22,300 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:10:22,300 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.05it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.15it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.05it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.27it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.94it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.67it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.53it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.51it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.54it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.57it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.54it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.55it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.38it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.37it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.38it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.41it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.50it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.51it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.53it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.52it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.49it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.41it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.3665525913238525, 'eval_accuracy': 0.6566523313522339, 'eval_f1': 0.6538404397563512, 'eval_runtime': 4.6811, 'eval_samples_per_second': 99.55, 'eval_steps_per_second': 12.604, 'epoch': 11.0}\n",
            " 44% 1144/2600 [12:31<14:16,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.79it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:10:26,983 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1144\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:26,984 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:27,073 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:27,073 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:27,081 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:27,082 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:27,134 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:27,134 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:27,209 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:27,210 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:28,671 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:28,793 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:28,819 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:10:28,819 >> Configuration saved in models/ZeroShot/0/checkpoint-1144/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:10:30,273 >> Module weights saved in models/ZeroShot/0/checkpoint-1144/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:10:30,273 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1144/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:10:30,273 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1144/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:10:30,733 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1040] due to args.save_total_limit\n",
            " 48% 1248/2600 [13:36<13:11,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:11:31,956 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:11:31,957 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:11:31,957 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:11:31,957 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.09it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.83it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.89it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.37it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.97it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.76it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.48it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.41it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.42it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.50it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.51it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.63it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.58it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.60it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.66it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.64it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.50it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.50it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.6535420417785645, 'eval_accuracy': 0.6523604989051819, 'eval_f1': 0.648311717352415, 'eval_runtime': 4.6627, 'eval_samples_per_second': 99.942, 'eval_steps_per_second': 12.654, 'epoch': 12.0}\n",
            " 48% 1248/2600 [13:41<13:11,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:11:36,621 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1248\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:36,622 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:36,710 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:36,711 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:36,718 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:36,719 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:36,770 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:36,770 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:36,835 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:36,836 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:38,283 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:38,417 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:38,440 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:11:38,441 >> Configuration saved in models/ZeroShot/0/checkpoint-1248/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:11:39,878 >> Module weights saved in models/ZeroShot/0/checkpoint-1248/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:11:39,879 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1248/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:11:39,879 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1248/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:11:40,354 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1144] due to args.save_total_limit\n",
            " 52% 1352/2600 [14:46<12:12,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:12:41,549 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:12:41,551 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:12:41,551 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:12:41,552 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.90it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.90it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.97it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.33it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.93it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.71it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.67it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.69it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.61it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.53it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.53it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.38it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.45it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.53it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.53it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.55it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.52it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.51it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.56it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.57it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.5024335384368896, 'eval_accuracy': 0.6738197207450867, 'eval_f1': 0.6530564263322884, 'eval_runtime': 4.6599, 'eval_samples_per_second': 100.003, 'eval_steps_per_second': 12.661, 'epoch': 13.0}\n",
            " 52% 1352/2600 [14:50<12:12,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:12:46,213 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1352\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:46,214 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:46,301 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:46,301 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:46,308 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:46,309 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:46,361 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:46,361 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:46,426 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:46,426 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:47,861 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:48,041 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:48,055 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:12:48,056 >> Configuration saved in models/ZeroShot/0/checkpoint-1352/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:12:49,409 >> Module weights saved in models/ZeroShot/0/checkpoint-1352/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:12:49,409 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1352/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:12:49,409 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1352/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:12:49,884 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1248] due to args.save_total_limit\n",
            " 56% 1456/2600 [15:55<11:12,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:13:51,091 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:13:51,093 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:13:51,093 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:13:51,093 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.19it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.14it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.04it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.49it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.19it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.00it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.84it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.78it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.63it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.62it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.61it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.61it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.58it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.57it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.58it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.8120551109313965, 'eval_accuracy': 0.6587982773780823, 'eval_f1': 0.6563915876365154, 'eval_runtime': 4.6281, 'eval_samples_per_second': 100.688, 'eval_steps_per_second': 12.748, 'epoch': 14.0}\n",
            " 56% 1456/2600 [16:00<11:12,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 14.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:13:55,722 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1456\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:55,723 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:55,817 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:55,818 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:55,825 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:55,825 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:55,881 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:55,882 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:55,947 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:55,948 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:57,424 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:57,552 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:57,576 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:13:57,576 >> Configuration saved in models/ZeroShot/0/checkpoint-1456/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:13:59,025 >> Module weights saved in models/ZeroShot/0/checkpoint-1456/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:13:59,026 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1456/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:13:59,026 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1456/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:13:59,481 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1352] due to args.save_total_limit\n",
            "{'loss': 0.0102, 'learning_rate': 4.230769230769231e-05, 'epoch': 14.42}\n",
            " 60% 1560/2600 [17:05<10:11,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:15:00,660 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:15:00,661 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:15:00,661 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:15:00,661 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.14it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.24it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.11it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.50it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.15it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.84it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.53it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.43it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.44it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.41it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.38it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.39it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.29it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.30it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.37it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.34it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.33it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.35it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.29it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.687105178833008, 'eval_accuracy': 0.6652360558509827, 'eval_f1': 0.6560885608856089, 'eval_runtime': 4.6906, 'eval_samples_per_second': 99.347, 'eval_steps_per_second': 12.578, 'epoch': 15.0}\n",
            " 60% 1560/2600 [17:09<10:11,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:15:05,354 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1560\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:05,355 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:05,441 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:05,442 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:05,449 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:05,449 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:05,507 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:05,507 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:05,575 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:05,576 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:06,862 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:06,940 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:06,962 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:15:06,963 >> Configuration saved in models/ZeroShot/0/checkpoint-1560/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:15:08,430 >> Module weights saved in models/ZeroShot/0/checkpoint-1560/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:15:08,431 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:15:08,431 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1560/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:15:08,917 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1456] due to args.save_total_limit\n",
            " 64% 1664/2600 [18:14<09:09,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:16:10,093 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:16:10,094 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:16:10,095 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:16:10,095 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.81it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.99it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.99it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.35it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.97it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.63it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.53it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.47it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.53it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.58it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.58it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.59it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.46it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.37it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.34it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.37it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.44it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.47it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.56it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.59it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.53it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.52it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.55it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7305307388305664, 'eval_accuracy': 0.6566523313522339, 'eval_f1': 0.6407647240209682, 'eval_runtime': 4.6693, 'eval_samples_per_second': 99.8, 'eval_steps_per_second': 12.636, 'epoch': 16.0}\n",
            " 64% 1664/2600 [18:19<09:09,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:16:14,765 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1664\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:14,766 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:14,855 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:14,855 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:14,862 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:14,863 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:14,921 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:14,921 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:14,988 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:14,988 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:16,468 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:16,541 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:16,553 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:16:16,554 >> Configuration saved in models/ZeroShot/0/checkpoint-1664/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:16:18,020 >> Module weights saved in models/ZeroShot/0/checkpoint-1664/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:16:18,020 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1664/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:16:18,020 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1664/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:16:18,491 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1560] due to args.save_total_limit\n",
            " 68% 1768/2600 [19:24<08:08,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:17:19,765 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:17:19,767 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:17:19,767 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:17:19,767 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.21it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.17it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.09it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.39it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.09it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.80it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.55it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.46it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.35it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.36it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.49it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.57it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.56it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.60it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.58it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.57it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.56it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.44it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7735579013824463, 'eval_accuracy': 0.667382001876831, 'eval_f1': 0.659926081122437, 'eval_runtime': 4.6715, 'eval_samples_per_second': 99.754, 'eval_steps_per_second': 12.63, 'epoch': 17.0}\n",
            " 68% 1768/2600 [19:29<08:08,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 13.77it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:17:24,440 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1768\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:24,440 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:24,531 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:24,532 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:24,539 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:24,540 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:24,591 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:24,592 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:24,657 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:24,658 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:26,131 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:26,132 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:26,141 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:17:26,142 >> Configuration saved in models/ZeroShot/0/checkpoint-1768/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:17:27,636 >> Module weights saved in models/ZeroShot/0/checkpoint-1768/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:17:27,636 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1768/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:17:27,637 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1768/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:17:28,108 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1664] due to args.save_total_limit\n",
            " 72% 1872/2600 [20:33<07:06,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:18:29,298 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:18:29,300 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:18:29,300 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:18:29,300 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.22it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.18it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.08it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.38it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.93it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.79it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.53it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.56it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.59it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.60it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.63it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.63it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.39it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.39it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.37it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.41it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.49it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.56it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.53it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.50it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.37it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.37it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.34it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9274656772613525, 'eval_accuracy': 0.6459227204322815, 'eval_f1': 0.6443488517310761, 'eval_runtime': 4.6749, 'eval_samples_per_second': 99.681, 'eval_steps_per_second': 12.62, 'epoch': 18.0}\n",
            " 72% 1872/2600 [20:38<07:06,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.79it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:18:33,976 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1872\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:33,977 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:34,064 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:34,064 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:34,072 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:34,073 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:34,135 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:34,136 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:34,206 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:34,206 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:35,675 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:35,676 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:35,686 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:18:35,686 >> Configuration saved in models/ZeroShot/0/checkpoint-1872/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:18:37,214 >> Module weights saved in models/ZeroShot/0/checkpoint-1872/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:18:37,214 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1872/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:18:37,215 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1872/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:18:37,679 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1768] due to args.save_total_limit\n",
            " 76% 1976/2600 [21:43<06:05,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:19:39,019 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:19:39,021 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:19:39,021 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:19:39,021 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.99it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.09it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.05it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.33it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.04it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.73it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.67it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.51it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.45it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.42it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.50it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.69it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.62it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.54it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.54it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.48it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.45it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.42it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.44it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.51it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9208123683929443, 'eval_accuracy': 0.6652360558509827, 'eval_f1': 0.6570490442133867, 'eval_runtime': 4.6546, 'eval_samples_per_second': 100.116, 'eval_steps_per_second': 12.676, 'epoch': 19.0}\n",
            " 76% 1976/2600 [21:48<06:05,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:19:43,677 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1976\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:43,677 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:43,768 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:43,769 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:43,776 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:43,777 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:43,841 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:43,842 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:43,909 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:43,909 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:45,346 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:45,347 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:45,357 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:19:45,358 >> Configuration saved in models/ZeroShot/0/checkpoint-1976/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:19:46,918 >> Module weights saved in models/ZeroShot/0/checkpoint-1976/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:19:46,918 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1976/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:19:46,918 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1976/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:19:47,377 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1872] due to args.save_total_limit\n",
            "{'loss': 0.0043, 'learning_rate': 2.307692307692308e-05, 'epoch': 19.23}\n",
            " 80% 2080/2600 [22:53<05:04,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:20:48,527 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:20:48,528 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:20:48,528 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:20:48,528 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.91it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.01it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.96it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.23it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.97it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.70it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.55it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.60it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.61it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.59it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.51it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.40it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.37it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.39it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.37it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.39it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.38it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.47it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.56it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.62it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.69it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.69it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9662115573883057, 'eval_accuracy': 0.6716738343238831, 'eval_f1': 0.6638710133653913, 'eval_runtime': 4.6662, 'eval_samples_per_second': 99.868, 'eval_steps_per_second': 12.644, 'epoch': 20.0}\n",
            " 80% 2080/2600 [22:57<05:04,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.03it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:20:53,196 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2080\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:53,196 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:53,292 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:53,292 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:53,300 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:53,300 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:53,351 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:53,351 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:53,417 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:53,418 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:54,900 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:54,901 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:54,911 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:20:54,911 >> Configuration saved in models/ZeroShot/0/checkpoint-2080/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:20:56,264 >> Module weights saved in models/ZeroShot/0/checkpoint-2080/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:20:56,265 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2080/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:20:56,265 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2080/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:20:56,736 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1976] due to args.save_total_limit\n",
            " 84% 2184/2600 [24:02<04:03,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:21:58,049 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:21:58,051 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:21:58,051 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:21:58,051 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.87it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.04it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.98it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.32it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.03it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.72it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.62it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.52it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.54it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.59it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.65it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.66it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.61it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.46it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.36it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.35it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.40it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.42it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.38it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.35it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.35it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.40it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.46it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.56it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.62it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0020782947540283, 'eval_accuracy': 0.6716738343238831, 'eval_f1': 0.6651591360581597, 'eval_runtime': 4.6776, 'eval_samples_per_second': 99.624, 'eval_steps_per_second': 12.613, 'epoch': 21.0}\n",
            " 84% 2184/2600 [24:07<04:03,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:22:02,730 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2184\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:02,730 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:02,825 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:02,826 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:02,833 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:02,834 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:02,885 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:02,886 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:02,952 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:02,953 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:04,349 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:04,411 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:04,522 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:22:04,523 >> Configuration saved in models/ZeroShot/0/checkpoint-2184/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:22:05,980 >> Module weights saved in models/ZeroShot/0/checkpoint-2184/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:22:05,980 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2184/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:22:05,981 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2184/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:22:06,448 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2080] due to args.save_total_limit\n",
            " 88% 2288/2600 [25:12<03:02,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:23:07,548 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:23:07,550 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:23:07,550 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:23:07,550 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.08it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.12it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.04it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.32it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.98it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.60it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.57it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.58it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.49it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.43it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.41it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.48it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.54it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.48it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.39it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.40it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.42it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.48it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.57it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9477245807647705, 'eval_accuracy': 0.6630901098251343, 'eval_f1': 0.6478312562876961, 'eval_runtime': 4.6624, 'eval_samples_per_second': 99.949, 'eval_steps_per_second': 12.654, 'epoch': 22.0}\n",
            " 88% 2288/2600 [25:16<03:02,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:23:12,214 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2288\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:12,214 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:12,305 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:12,305 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:12,313 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:12,313 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:12,370 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:12,371 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:12,443 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:12,444 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:13,877 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:14,018 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:14,043 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:23:14,043 >> Configuration saved in models/ZeroShot/0/checkpoint-2288/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:23:15,478 >> Module weights saved in models/ZeroShot/0/checkpoint-2288/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:23:15,478 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2288/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:23:15,479 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2288/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:23:15,944 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2184] due to args.save_total_limit\n",
            " 92% 2392/2600 [26:21<02:01,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:24:17,215 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:24:17,216 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:24:17,217 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:24:17,217 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.82it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.02it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.03it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.44it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.11it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.94it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.82it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.36it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.42it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.42it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.59it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.49it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.51it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.51it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.46it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.39it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.41it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.37it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.34it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.32it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.35it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.42it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.55it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9087820053100586, 'eval_accuracy': 0.667382001876831, 'eval_f1': 0.654790068583172, 'eval_runtime': 4.6693, 'eval_samples_per_second': 99.801, 'eval_steps_per_second': 12.636, 'epoch': 23.0}\n",
            " 92% 2392/2600 [26:26<02:01,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 13.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:24:21,887 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2392\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:21,888 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:21,983 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:21,984 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:21,991 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:21,991 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:22,042 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:22,042 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:22,109 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:22,109 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:23,558 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:23,558 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:23,569 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:24:23,569 >> Configuration saved in models/ZeroShot/0/checkpoint-2392/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:24:25,000 >> Module weights saved in models/ZeroShot/0/checkpoint-2392/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:24:25,001 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2392/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:24:25,001 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2392/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:24:25,635 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2288] due to args.save_total_limit\n",
            " 96% 2496/2600 [27:31<01:01,  1.70it/s][INFO|trainer.py:623] 2022-08-25 04:25:26,744 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:25:26,746 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:25:26,746 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:25:26,746 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.16it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.05it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.00it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.36it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.03it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.76it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.48it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.50it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.43it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.33it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.33it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.37it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.33it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.38it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.45it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.52it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.52it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.49it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.52it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.54it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.45it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.44it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.40it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.38it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.41it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.47it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.921909809112549, 'eval_accuracy': 0.667382001876831, 'eval_f1': 0.657539767204798, 'eval_runtime': 4.6817, 'eval_samples_per_second': 99.536, 'eval_steps_per_second': 12.602, 'epoch': 24.0}\n",
            " 96% 2496/2600 [27:35<01:01,  1.70it/s]\n",
            "100% 59/59 [00:04<00:00, 14.01it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:25:31,429 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2496\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:31,430 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:31,521 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:31,521 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:31,528 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:31,529 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:31,587 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:31,588 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:31,655 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:31,656 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:32,993 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:33,077 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:33,104 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:25:33,104 >> Configuration saved in models/ZeroShot/0/checkpoint-2496/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:25:34,484 >> Module weights saved in models/ZeroShot/0/checkpoint-2496/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:25:34,485 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2496/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:25:34,485 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2496/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:25:34,972 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2392] due to args.save_total_limit\n",
            "{'loss': 0.0028, 'learning_rate': 3.846153846153847e-06, 'epoch': 24.04}\n",
            "100% 2600/2600 [28:40<00:00,  1.71it/s][INFO|trainer.py:623] 2022-08-25 04:26:36,108 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:26:36,110 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:26:36,110 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:26:36,110 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.05it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.05it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.01it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.31it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.99it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.77it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.52it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.49it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.50it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.57it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.61it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.61it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.54it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.59it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.63it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.53it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.48it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.41it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.39it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.38it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.44it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.55it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9313743114471436, 'eval_accuracy': 0.6695278882980347, 'eval_f1': 0.6604976818998959, 'eval_runtime': 4.6583, 'eval_samples_per_second': 100.036, 'eval_steps_per_second': 12.665, 'epoch': 25.0}\n",
            "100% 2600/2600 [28:45<00:00,  1.71it/s]\n",
            "100% 59/59 [00:04<00:00, 14.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:26:40,770 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-2600\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:40,770 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:40,862 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:40,863 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:40,870 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:40,871 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:40,926 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:40,926 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:40,999 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:41,000 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:42,477 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:42,581 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:42,595 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:42,596 >> Configuration saved in models/ZeroShot/0/checkpoint-2600/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:43,968 >> Module weights saved in models/ZeroShot/0/checkpoint-2600/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:26:43,968 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-2600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:26:43,969 >> Special tokens file saved in models/ZeroShot/0/checkpoint-2600/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:26:44,432 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-2496] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 04:26:44,490 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 04:26:44,490 >> Loading best model from models/ZeroShot/0/checkpoint-312 (score: 0.6720430879712747).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 04:26:44,490 >> Could not locate the best model at models/ZeroShot/0/checkpoint-312/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 1729.0783, 'train_samples_per_second': 48.104, 'train_steps_per_second': 1.504, 'train_loss': 0.06462089024484158, 'epoch': 25.0}\n",
            "100% 2600/2600 [28:49<00:00,  1.71it/s][INFO|trainer.py:238] 2022-08-25 04:26:44,522 >> Loading best adapter(s) from models/ZeroShot/0/checkpoint-312 (score: 0.6720430879712747).\n",
            "[INFO|loading.py:77] 2022-08-25 04:26:44,522 >> Loading module configuration from models/ZeroShot/0/checkpoint-312/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 04:26:44,527 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:26:45,248 >> Loading module weights from models/ZeroShot/0/checkpoint-312/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:26:45,270 >> Loading module configuration from models/ZeroShot/0/checkpoint-312/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 04:26:45,271 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 04:26:45,283 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 04:26:45,297 >> Loading module weights from models/ZeroShot/0/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:26:45,300 >> Loading module configuration from models/ZeroShot/0/checkpoint-312/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 04:26:45,301 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:26:45,449 >> Loading module weights from models/ZeroShot/0/checkpoint-312/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 04:26:45,466 >> No matching prediction head found in 'models/ZeroShot/0/checkpoint-312/en'\n",
            "[INFO|loading.py:77] 2022-08-25 04:26:45,466 >> Loading module configuration from models/ZeroShot/0/checkpoint-312/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 04:26:45,466 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:26:45,626 >> Loading module weights from models/ZeroShot/0/checkpoint-312/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:26:45,645 >> Loading module configuration from models/ZeroShot/0/checkpoint-312/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 04:26:45,645 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 04:26:46,848 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 04:26:48,486 >> Loading module weights from models/ZeroShot/0/checkpoint-312/pt/pytorch_model_head.bin\n",
            "100% 2600/2600 [28:53<00:00,  1.50it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 04:26:48,574 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:48,575 >> Configuration saved in models/ZeroShot/0/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:48,691 >> Module weights saved in models/ZeroShot/0/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:48,692 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:48,705 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:48,706 >> Configuration saved in models/ZeroShot/0/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:48,764 >> Module weights saved in models/ZeroShot/0/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:48,765 >> Configuration saved in models/ZeroShot/0/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:48,864 >> Module weights saved in models/ZeroShot/0/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:48,864 >> Configuration saved in models/ZeroShot/0/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:50,294 >> Module weights saved in models/ZeroShot/0/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:50,442 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:50,458 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:26:50,459 >> Configuration saved in models/ZeroShot/0/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:26:51,901 >> Module weights saved in models/ZeroShot/0/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:26:51,922 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:26:51,922 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0646\n",
            "  train_runtime            = 0:28:49.07\n",
            "  train_samples            =       3327\n",
            "  train_samples_per_second =     48.104\n",
            "  train_steps_per_second   =      1.504\n",
            "08/25/2022 04:26:52 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:623] 2022-08-25 04:26:52,129 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:26:52,131 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:26:52,131 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:26:52,131 >>   Batch size = 8\n",
            "100% 59/59 [00:04<00:00, 13.23it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.6845\n",
            "  eval_f1                 =      0.672\n",
            "  eval_loss               =     0.9971\n",
            "  eval_runtime            = 0:00:04.56\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    102.183\n",
            "  eval_steps_per_second   =     12.937\n",
            "[INFO|modelcard.py:460] 2022-08-25 04:26:56,957 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6845493316650391}, {'name': 'F1', 'type': 'f1', 'value': 0.6720430879712747}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for zero-shot portuguese idiomaticity detection"
      ],
      "metadata": {
        "id": "DMIhGZVs1SXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Portuguese language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/ZeroShot/0/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --seed 0 \\\n",
        "  --train_file      Data/ZeroShot/PT/train.csv \\\n",
        "  --validation_file Data/ZeroShot/PT/dev.csv \\\n",
        "\t--evaluation_strategy \"epoch\" \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "UprIOfoqiaMm",
        "outputId": "cfe7e20b-9a82-420e-a3c3-6da18f2a62cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 09:24:31 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 09:24:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Aug25_09-24-31_9b17356d7e44,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 09:24:31 - INFO - __main__ - load a local file for train: Data/ZeroShot/PT/train.csv\n",
            "08/25/2022 09:24:31 - INFO - __main__ - load a local file for validation: Data/ZeroShot/PT/dev.csv\n",
            "08/25/2022 09:24:31 - WARNING - datasets.builder - Using custom data configuration default-1ad3dfbb91b842ab\n",
            "08/25/2022 09:24:31 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-1ad3dfbb91b842ab/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1ad3dfbb91b842ab/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 11214.72it/s]\n",
            "08/25/2022 09:24:31 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 09:24:31 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1436.41it/s]\n",
            "08/25/2022 09:24:31 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 09:24:31 - INFO - datasets.builder - Generating train split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 09:24:31 - INFO - datasets.builder - Generating validation split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 09:24:31 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1ad3dfbb91b842ab/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1072.85it/s]\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:31,238 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_t9ervuc\n",
            "Downloading: 100% 625/625 [00:00<00:00, 1.01MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:24:31,263 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:31,263 >> creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 09:24:31,264 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 09:24:31,264 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:31,289 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprjwa6mpa\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 53.3kB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:24:31,314 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:31,314 >> creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 09:24:31,337 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 09:24:31,338 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:31,383 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpukogoukd\n",
            "Downloading: 100% 972k/972k [00:00<00:00, 52.6MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:24:31,465 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:31,465 >> creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:31,488 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpspv4wgf9\n",
            "Downloading: 100% 1.87M/1.87M [00:00<00:00, 63.9MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:24:31,609 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:31,610 >> creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:24:31,679 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:24:31,679 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:24:31,679 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:24:31,679 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:24:31,679 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 09:24:31,701 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 09:24:31,702 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:31,854 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplx_a3ymx\n",
            "Downloading: 100% 681M/681M [00:10<00:00, 68.9MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:24:42,255 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:42,255 >> creating metadata file for /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 09:24:42,255 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 09:24:44,412 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 09:24:44,412 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 09:24:44,423 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 09:24:44,424 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 09:24:44,671 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:44,766 >> https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/v2/index/bert-base-multilingual-cased.json not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmpsaf8rveo\n",
            "Downloading: 24.6kB [00:00, 28.9MB/s]       \n",
            "[INFO|hub.py:595] 2022-08-25 09:24:44,786 >> storing https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/v2/index/bert-base-multilingual-cased.json in cache at ~/.cache/torch/adapters/08d069267cb1d67d820deec1e19b1a91aef2bdf0e64fa8456c440abe23c3ddb4.a7da128024bcd4efcca057684afe69285b8f3279e656c8ec9d8af63e19290817\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:44,786 >> creating metadata file for ~/.cache/torch/adapters/08d069267cb1d67d820deec1e19b1a91aef2bdf0e64fa8456c440abe23c3ddb4.a7da128024bcd4efcca057684afe69285b8f3279e656c8ec9d8af63e19290817\n",
            "[INFO|utils.py:327] 2022-08-25 09:24:44,787 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 09:24:44,882 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:45,385 >> https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmpcr6j7moh\n",
            "Downloading: 100% 28.2M/28.2M [00:03<00:00, 8.70MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:24:49,304 >> storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip in cache at ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf.3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a\n",
            "[INFO|hub.py:603] 2022-08-25 09:24:49,304 >> creating metadata file for ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf.3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a\n",
            "[INFO|loading.py:77] 2022-08-25 09:24:49,410 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 09:24:49,410 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:24:49,526 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 09:24:49,538 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 09:24:49,538 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 09:24:49,562 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 09:24:49,664 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|hub.py:591] 2022-08-25 09:24:50,075 >> https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmp4h5tf967\n",
            "Downloading: 100% 381M/381M [00:15<00:00, 26.2MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 09:25:05,841 >> storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip in cache at ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e.babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c\n",
            "[INFO|hub.py:603] 2022-08-25 09:25:05,842 >> creating metadata file for ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e.babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c\n",
            "[INFO|loading.py:77] 2022-08-25 09:25:08,059 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 09:25:08,060 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:25:08,295 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:25:08,320 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 09:25:08,320 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 09:25:09,994 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 09:25:10,109 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 09:25:10,178 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with PT language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]08/25/2022 09:25:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1ad3dfbb91b842ab/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-a64c1371b77eda86.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  7.48ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 09:25:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1ad3dfbb91b842ab/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-ed189e131d205543.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 17.01ba/s]\n",
            "08/25/2022 09:25:10 - INFO - __main__ - Sample 788 of the training set: {'label': 1, 'sentence1': 'Diálogo, harmonia e transparência foram as palavras destacadas por ele. Ao final, se dirigiu até cada um dos parlamentares para fazer o cumprimento com a mão fechada, tradicional em meio à pandemia. Mesmo em clima aparentemente harmonioso, o tucano não deixou de avisar:   — Não há como nos omitirmos quanto aos ajustes, as pautas e reformas sensíveis que porventura entrarão em discussão — afirmou durante a fala oficial de abertura do ano legislativo.', 'input_ids': [101, 12944, 50390, 117, 105999, 21004, 173, 37241, 22786, 14328, 13395, 10146, 71397, 94388, 10107, 10183, 12637, 119, 17607, 11070, 117, 10126, 20114, 50510, 13168, 11782, 10293, 10398, 93898, 10107, 10220, 26561, 183, 16008, 60259, 15088, 10212, 169, 77868, 24283, 10229, 117, 21752, 10266, 25598, 254, 24960, 93262, 119, 98728, 10266, 21700, 72495, 105999, 14639, 20213, 117, 183, 13055, 25498, 11420, 45916, 10104, 57822, 10354, 131, 100, 31107, 25056, 10225, 11573, 10209, 13903, 73564, 10107, 15696, 13851, 14047, 42043, 10107, 117, 10146, 71548, 11390, 173, 60091, 21542, 55822, 10121, 10183, 100817, 26316, 11449, 10266, 71695, 11449, 100, 72707, 11047, 169, 56773, 13771, 10104, 60497, 10149, 12797, 78198, 61432, 17654, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 09:25:10 - INFO - __main__ - Sample 861 of the training set: {'label': 1, 'sentence1': 'Rudôlf Pikhóia, doutor em História e ex-arquivista russo, argumenta, em seu artigo \"Por que a União Soviética caiu?\", que a principal característica do Estado soviético era a unidade dos órgãos governamentais e do Partido Comunista.  A Constituição Soviética de 1977 definiu o Partido como “o núcleo do sistema político”. Seguindo esta linha, Lênin afirmava que o Soviete - ou seja, os órgãos eleitos de autogestão local - era uma forma de democracia direta e, assim, não havia necessidade de parlamento ou de separação de poderes (legislativo, executivo e judiciário).', 'input_ids': [101, 155, 11679, 16218, 35173, 38329, 48502, 25967, 10113, 117, 10149, 31602, 10266, 56469, 173, 11419, 118, 10456, 39639, 29106, 29052, 117, 36847, 10113, 117, 10266, 10617, 52686, 107, 12399, 10121, 169, 36508, 37078, 39270, 10138, 136, 107, 117, 10121, 169, 11652, 35761, 10149, 14359, 81462, 10411, 169, 54278, 10398, 273, 109942, 10107, 25574, 17275, 12985, 173, 10149, 17084, 41231, 119, 138, 108234, 37078, 10104, 10722, 100745, 106495, 183, 17084, 10225, 100, 183, 38578, 10149, 11708, 17478, 100, 119, 11045, 86041, 10317, 11504, 31174, 117, 33457, 11802, 38439, 10362, 10121, 183, 15277, 10112, 118, 10431, 27271, 117, 10427, 273, 109942, 10107, 44393, 10107, 10104, 18257, 63952, 11449, 11436, 118, 10411, 10437, 11041, 10104, 62072, 19263, 10213, 173, 117, 20459, 117, 11420, 11970, 88966, 10104, 51833, 10431, 10104, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "08/25/2022 09:25:10 - INFO - __main__ - Sample 82 of the training set: {'label': 1, 'sentence1': 'Antes de tudo, preaqueça o forno a 180º C e unte uma assadeira grande. Então, para dar início e colocar de vez a mão na massa, comece misturando a aveia e a farinha integral. Igualmente, adicione à mistura o fermento e o açúcar.', 'input_ids': [101, 31722, 10104, 50172, 117, 12229, 74103, 12989, 183, 10142, 10343, 169, 13912, 15270, 140, 173, 10119, 10216, 10437, 13935, 12930, 13007, 11439, 119, 63412, 11449, 117, 10220, 12923, 23134, 173, 101058, 10104, 11675, 169, 77868, 10132, 20701, 117, 10678, 10419, 12606, 13495, 10605, 169, 47145, 10280, 173, 169, 13301, 61620, 37940, 119, 146, 81629, 10611, 117, 10840, 67645, 10112, 254, 12606, 13495, 183, 50755, 15088, 173, 183, 169, 13406, 63353, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 09:25:17,508 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 09:25:17,524 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 09:25:17,524 >>   Num examples = 1164\n",
            "[INFO|trainer.py:1421] 2022-08-25 09:25:17,524 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 09:25:17,524 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 09:25:17,524 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 09:25:17,524 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 09:25:17,524 >>   Total optimization steps = 925\n",
            "  4% 37/925 [00:22<06:45,  2.19it/s][INFO|trainer.py:623] 2022-08-25 09:25:40,385 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:25:40,387 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:25:40,387 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:25:40,387 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.61it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 15.86it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 14.98it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 14.40it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 14.09it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 13.88it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 13.72it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 13.67it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 13.66it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 13.59it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 13.54it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 13.49it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 13.45it/s]\u001b[A\n",
            " 86% 30/35 [00:02<00:00, 13.41it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 13.49it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6875684261322021, 'eval_accuracy': 0.5641025900840759, 'eval_f1': 0.36065573770491804, 'eval_runtime': 2.5554, 'eval_samples_per_second': 106.832, 'eval_steps_per_second': 13.696, 'epoch': 1.0}\n",
            "  4% 37/925 [00:25<06:45,  2.19it/s]\n",
            "100% 35/35 [00:02<00:00, 13.48it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:25:42,944 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-37\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:42,945 >> Configuration saved in models/ZeroShot/0/checkpoint-37/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:43,068 >> Module weights saved in models/ZeroShot/0/checkpoint-37/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:43,069 >> Configuration saved in models/ZeroShot/0/checkpoint-37/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:43,078 >> Module weights saved in models/ZeroShot/0/checkpoint-37/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:43,078 >> Configuration saved in models/ZeroShot/0/checkpoint-37/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:43,167 >> Module weights saved in models/ZeroShot/0/checkpoint-37/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:43,167 >> Configuration saved in models/ZeroShot/0/checkpoint-37/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:43,280 >> Module weights saved in models/ZeroShot/0/checkpoint-37/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:43,281 >> Configuration saved in models/ZeroShot/0/checkpoint-37/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:44,925 >> Module weights saved in models/ZeroShot/0/checkpoint-37/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:44,926 >> Configuration saved in models/ZeroShot/0/checkpoint-37/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:44,990 >> Module weights saved in models/ZeroShot/0/checkpoint-37/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:25:44,991 >> Configuration saved in models/ZeroShot/0/checkpoint-37/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:25:46,438 >> Module weights saved in models/ZeroShot/0/checkpoint-37/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:25:46,439 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-37/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:25:46,439 >> Special tokens file saved in models/ZeroShot/0/checkpoint-37/special_tokens_map.json\n",
            "  8% 74/925 [00:50<06:38,  2.14it/s][INFO|trainer.py:623] 2022-08-25 09:26:07,659 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:26:07,661 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:26:07,661 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:26:07,661 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.48it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 15.71it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 14.49it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.88it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 13.68it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 13.41it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 13.36it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 13.24it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 13.23it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 13.18it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 13.20it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 13.16it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 13.18it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 13.16it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 13.18it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6991380453109741, 'eval_accuracy': 0.5494505763053894, 'eval_f1': 0.3546099290780142, 'eval_runtime': 2.6139, 'eval_samples_per_second': 104.442, 'eval_steps_per_second': 13.39, 'epoch': 2.0}\n",
            "  8% 74/925 [00:52<06:38,  2.14it/s]\n",
            "100% 35/35 [00:02<00:00, 13.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:26:10,276 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-74\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:10,277 >> Configuration saved in models/ZeroShot/0/checkpoint-74/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:10,368 >> Module weights saved in models/ZeroShot/0/checkpoint-74/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:10,369 >> Configuration saved in models/ZeroShot/0/checkpoint-74/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:10,382 >> Module weights saved in models/ZeroShot/0/checkpoint-74/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:10,383 >> Configuration saved in models/ZeroShot/0/checkpoint-74/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:10,469 >> Module weights saved in models/ZeroShot/0/checkpoint-74/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:10,470 >> Configuration saved in models/ZeroShot/0/checkpoint-74/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:10,522 >> Module weights saved in models/ZeroShot/0/checkpoint-74/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:10,523 >> Configuration saved in models/ZeroShot/0/checkpoint-74/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:11,788 >> Module weights saved in models/ZeroShot/0/checkpoint-74/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:11,823 >> Configuration saved in models/ZeroShot/0/checkpoint-74/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:11,858 >> Module weights saved in models/ZeroShot/0/checkpoint-74/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:11,859 >> Configuration saved in models/ZeroShot/0/checkpoint-74/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:13,689 >> Module weights saved in models/ZeroShot/0/checkpoint-74/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:26:13,689 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-74/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:26:13,690 >> Special tokens file saved in models/ZeroShot/0/checkpoint-74/special_tokens_map.json\n",
            " 12% 111/925 [01:17<06:35,  2.06it/s][INFO|trainer.py:623] 2022-08-25 09:26:35,484 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:26:35,485 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:26:35,485 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:26:35,485 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.78it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 15.17it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 14.03it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.52it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 13.16it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.84it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.77it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.66it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.48it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.45it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.42it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.34it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.38it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.40it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.6969903707504272, 'eval_accuracy': 0.6446886658668518, 'eval_f1': 0.6196843269327437, 'eval_runtime': 2.7602, 'eval_samples_per_second': 98.907, 'eval_steps_per_second': 12.68, 'epoch': 3.0}\n",
            " 12% 111/925 [01:20<06:35,  2.06it/s]\n",
            "100% 35/35 [00:02<00:00, 13.78it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:26:38,247 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-111\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:38,247 >> Configuration saved in models/ZeroShot/0/checkpoint-111/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:38,339 >> Module weights saved in models/ZeroShot/0/checkpoint-111/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:38,340 >> Configuration saved in models/ZeroShot/0/checkpoint-111/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:38,347 >> Module weights saved in models/ZeroShot/0/checkpoint-111/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:38,348 >> Configuration saved in models/ZeroShot/0/checkpoint-111/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:38,404 >> Module weights saved in models/ZeroShot/0/checkpoint-111/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:38,405 >> Configuration saved in models/ZeroShot/0/checkpoint-111/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:38,466 >> Module weights saved in models/ZeroShot/0/checkpoint-111/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:38,466 >> Configuration saved in models/ZeroShot/0/checkpoint-111/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:39,982 >> Module weights saved in models/ZeroShot/0/checkpoint-111/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:39,988 >> Configuration saved in models/ZeroShot/0/checkpoint-111/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:40,002 >> Module weights saved in models/ZeroShot/0/checkpoint-111/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:26:40,009 >> Configuration saved in models/ZeroShot/0/checkpoint-111/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:26:41,478 >> Module weights saved in models/ZeroShot/0/checkpoint-111/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:26:41,479 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-111/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:26:41,479 >> Special tokens file saved in models/ZeroShot/0/checkpoint-111/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:26:41,943 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-37] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:26:42,011 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-74] due to args.save_total_limit\n",
            " 16% 148/925 [01:46<06:32,  1.98it/s][INFO|trainer.py:623] 2022-08-25 09:27:04,221 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:27:04,229 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:27:04,229 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:27:04,229 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.13it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.53it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.49it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.89it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.55it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.35it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.34it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.27it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.20it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.12it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.08it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.06it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.11it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.13it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.07it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.00it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8445414304733276, 'eval_accuracy': 0.6886447072029114, 'eval_f1': 0.6749681341307971, 'eval_runtime': 2.8421, 'eval_samples_per_second': 96.055, 'eval_steps_per_second': 12.315, 'epoch': 4.0}\n",
            " 16% 148/925 [01:49<06:32,  1.98it/s]\n",
            "100% 35/35 [00:02<00:00, 13.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:27:07,072 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-148\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:07,073 >> Configuration saved in models/ZeroShot/0/checkpoint-148/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:07,162 >> Module weights saved in models/ZeroShot/0/checkpoint-148/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:07,163 >> Configuration saved in models/ZeroShot/0/checkpoint-148/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:07,170 >> Module weights saved in models/ZeroShot/0/checkpoint-148/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:07,171 >> Configuration saved in models/ZeroShot/0/checkpoint-148/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:07,226 >> Module weights saved in models/ZeroShot/0/checkpoint-148/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:07,227 >> Configuration saved in models/ZeroShot/0/checkpoint-148/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:07,284 >> Module weights saved in models/ZeroShot/0/checkpoint-148/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:07,285 >> Configuration saved in models/ZeroShot/0/checkpoint-148/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:08,732 >> Module weights saved in models/ZeroShot/0/checkpoint-148/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:08,804 >> Configuration saved in models/ZeroShot/0/checkpoint-148/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:08,818 >> Module weights saved in models/ZeroShot/0/checkpoint-148/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:08,819 >> Configuration saved in models/ZeroShot/0/checkpoint-148/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:10,259 >> Module weights saved in models/ZeroShot/0/checkpoint-148/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:27:10,260 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-148/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:27:10,260 >> Special tokens file saved in models/ZeroShot/0/checkpoint-148/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:27:10,710 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-111] due to args.save_total_limit\n",
            " 20% 185/925 [02:16<06:31,  1.89it/s][INFO|trainer.py:623] 2022-08-25 09:27:33,950 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:27:33,952 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:27:33,952 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:27:33,952 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.88it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.85it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.93it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.20it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.82it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.82it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.72it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.63it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.52it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.45it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.36it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.48it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.44it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.47it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.40it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.35it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1222615242004395, 'eval_accuracy': 0.6776556968688965, 'eval_f1': 0.6448255470136014, 'eval_runtime': 2.9976, 'eval_samples_per_second': 91.074, 'eval_steps_per_second': 11.676, 'epoch': 5.0}\n",
            " 20% 185/925 [02:19<06:31,  1.89it/s]\n",
            "100% 35/35 [00:02<00:00, 12.71it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:27:36,951 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-185\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:36,952 >> Configuration saved in models/ZeroShot/0/checkpoint-185/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:37,037 >> Module weights saved in models/ZeroShot/0/checkpoint-185/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:37,038 >> Configuration saved in models/ZeroShot/0/checkpoint-185/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:37,046 >> Module weights saved in models/ZeroShot/0/checkpoint-185/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:37,046 >> Configuration saved in models/ZeroShot/0/checkpoint-185/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:37,097 >> Module weights saved in models/ZeroShot/0/checkpoint-185/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:37,097 >> Configuration saved in models/ZeroShot/0/checkpoint-185/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:37,152 >> Module weights saved in models/ZeroShot/0/checkpoint-185/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:37,153 >> Configuration saved in models/ZeroShot/0/checkpoint-185/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:38,606 >> Module weights saved in models/ZeroShot/0/checkpoint-185/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:38,607 >> Configuration saved in models/ZeroShot/0/checkpoint-185/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:38,619 >> Module weights saved in models/ZeroShot/0/checkpoint-185/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:27:38,620 >> Configuration saved in models/ZeroShot/0/checkpoint-185/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:27:40,133 >> Module weights saved in models/ZeroShot/0/checkpoint-185/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:27:40,134 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-185/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:27:40,134 >> Special tokens file saved in models/ZeroShot/0/checkpoint-185/special_tokens_map.json\n",
            " 24% 222/925 [02:46<06:05,  1.92it/s][INFO|trainer.py:623] 2022-08-25 09:28:03,647 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:28:03,649 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:28:03,649 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:28:03,649 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.29it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.52it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.46it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.31it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.04it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.89it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.96it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.02it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.07it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.10it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.97it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5873709917068481, 'eval_accuracy': 0.6410256624221802, 'eval_f1': 0.6350267379679144, 'eval_runtime': 2.8659, 'eval_samples_per_second': 95.257, 'eval_steps_per_second': 12.212, 'epoch': 6.0}\n",
            " 24% 222/925 [02:48<06:05,  1.92it/s]\n",
            "100% 35/35 [00:02<00:00, 13.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:28:06,516 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-222\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:06,517 >> Configuration saved in models/ZeroShot/0/checkpoint-222/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:06,605 >> Module weights saved in models/ZeroShot/0/checkpoint-222/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:06,605 >> Configuration saved in models/ZeroShot/0/checkpoint-222/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:06,612 >> Module weights saved in models/ZeroShot/0/checkpoint-222/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:06,612 >> Configuration saved in models/ZeroShot/0/checkpoint-222/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:06,668 >> Module weights saved in models/ZeroShot/0/checkpoint-222/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:06,669 >> Configuration saved in models/ZeroShot/0/checkpoint-222/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:06,727 >> Module weights saved in models/ZeroShot/0/checkpoint-222/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:06,727 >> Configuration saved in models/ZeroShot/0/checkpoint-222/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:08,048 >> Module weights saved in models/ZeroShot/0/checkpoint-222/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:08,174 >> Configuration saved in models/ZeroShot/0/checkpoint-222/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:08,192 >> Module weights saved in models/ZeroShot/0/checkpoint-222/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:08,192 >> Configuration saved in models/ZeroShot/0/checkpoint-222/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:09,682 >> Module weights saved in models/ZeroShot/0/checkpoint-222/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:28:09,682 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:28:09,683 >> Special tokens file saved in models/ZeroShot/0/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:28:10,160 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-185] due to args.save_total_limit\n",
            " 28% 259/925 [03:15<05:42,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:28:32,962 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:28:32,963 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:28:32,963 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:28:32,963 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.84it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.22it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.32it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.84it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.31it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.91it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.82it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.80it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.361032009124756, 'eval_accuracy': 0.6300366520881653, 'eval_f1': 0.5450375381569177, 'eval_runtime': 2.8773, 'eval_samples_per_second': 94.882, 'eval_steps_per_second': 12.164, 'epoch': 7.0}\n",
            " 28% 259/925 [03:18<05:42,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:28:35,842 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-259\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:35,843 >> Configuration saved in models/ZeroShot/0/checkpoint-259/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:35,930 >> Module weights saved in models/ZeroShot/0/checkpoint-259/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:35,930 >> Configuration saved in models/ZeroShot/0/checkpoint-259/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:35,937 >> Module weights saved in models/ZeroShot/0/checkpoint-259/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:35,938 >> Configuration saved in models/ZeroShot/0/checkpoint-259/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:35,998 >> Module weights saved in models/ZeroShot/0/checkpoint-259/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:35,999 >> Configuration saved in models/ZeroShot/0/checkpoint-259/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:36,071 >> Module weights saved in models/ZeroShot/0/checkpoint-259/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:36,071 >> Configuration saved in models/ZeroShot/0/checkpoint-259/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:37,511 >> Module weights saved in models/ZeroShot/0/checkpoint-259/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:37,539 >> Configuration saved in models/ZeroShot/0/checkpoint-259/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:37,561 >> Module weights saved in models/ZeroShot/0/checkpoint-259/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:28:37,562 >> Configuration saved in models/ZeroShot/0/checkpoint-259/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:28:39,044 >> Module weights saved in models/ZeroShot/0/checkpoint-259/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:28:39,044 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-259/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:28:39,045 >> Special tokens file saved in models/ZeroShot/0/checkpoint-259/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:28:39,529 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-222] due to args.save_total_limit\n",
            " 32% 296/925 [03:45<05:27,  1.92it/s][INFO|trainer.py:623] 2022-08-25 09:29:02,657 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:29:02,659 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:29:02,659 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:29:02,659 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.15it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.47it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.18it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.45it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.05it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.98it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.81it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.88it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.75it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.75it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.67it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.72it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.70it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.70it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.71it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.1832399368286133, 'eval_accuracy': 0.6739926934242249, 'eval_f1': 0.6234716174123263, 'eval_runtime': 2.9255, 'eval_samples_per_second': 93.317, 'eval_steps_per_second': 11.964, 'epoch': 8.0}\n",
            " 32% 296/925 [03:48<05:27,  1.92it/s]\n",
            "100% 35/35 [00:02<00:00, 13.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:29:05,586 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-296\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:05,586 >> Configuration saved in models/ZeroShot/0/checkpoint-296/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:05,676 >> Module weights saved in models/ZeroShot/0/checkpoint-296/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:05,676 >> Configuration saved in models/ZeroShot/0/checkpoint-296/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:05,683 >> Module weights saved in models/ZeroShot/0/checkpoint-296/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:05,684 >> Configuration saved in models/ZeroShot/0/checkpoint-296/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:05,742 >> Module weights saved in models/ZeroShot/0/checkpoint-296/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:05,743 >> Configuration saved in models/ZeroShot/0/checkpoint-296/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:05,803 >> Module weights saved in models/ZeroShot/0/checkpoint-296/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:05,804 >> Configuration saved in models/ZeroShot/0/checkpoint-296/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:07,272 >> Module weights saved in models/ZeroShot/0/checkpoint-296/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:07,296 >> Configuration saved in models/ZeroShot/0/checkpoint-296/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:07,309 >> Module weights saved in models/ZeroShot/0/checkpoint-296/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:07,310 >> Configuration saved in models/ZeroShot/0/checkpoint-296/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:08,704 >> Module weights saved in models/ZeroShot/0/checkpoint-296/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:29:08,705 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-296/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:29:08,705 >> Special tokens file saved in models/ZeroShot/0/checkpoint-296/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:29:09,187 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-259] due to args.save_total_limit\n",
            " 36% 333/925 [04:14<05:04,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:29:32,167 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:29:32,170 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:29:32,170 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:29:32,170 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.77it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.13it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.01it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.59it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.33it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.20it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.96it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.91it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 12.02it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.05it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.09it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.01it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.0089662075042725, 'eval_accuracy': 0.6556776762008667, 'eval_f1': 0.6321809425524596, 'eval_runtime': 2.8738, 'eval_samples_per_second': 94.995, 'eval_steps_per_second': 12.179, 'epoch': 9.0}\n",
            " 36% 333/925 [04:17<05:04,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.43it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:29:35,044 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-333\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:35,045 >> Configuration saved in models/ZeroShot/0/checkpoint-333/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:35,133 >> Module weights saved in models/ZeroShot/0/checkpoint-333/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:35,133 >> Configuration saved in models/ZeroShot/0/checkpoint-333/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:35,141 >> Module weights saved in models/ZeroShot/0/checkpoint-333/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:35,142 >> Configuration saved in models/ZeroShot/0/checkpoint-333/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:35,200 >> Module weights saved in models/ZeroShot/0/checkpoint-333/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:35,201 >> Configuration saved in models/ZeroShot/0/checkpoint-333/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:35,265 >> Module weights saved in models/ZeroShot/0/checkpoint-333/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:35,266 >> Configuration saved in models/ZeroShot/0/checkpoint-333/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:36,722 >> Module weights saved in models/ZeroShot/0/checkpoint-333/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:36,731 >> Configuration saved in models/ZeroShot/0/checkpoint-333/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:36,743 >> Module weights saved in models/ZeroShot/0/checkpoint-333/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:29:36,744 >> Configuration saved in models/ZeroShot/0/checkpoint-333/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:29:38,123 >> Module weights saved in models/ZeroShot/0/checkpoint-333/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:29:38,124 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-333/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:29:38,124 >> Special tokens file saved in models/ZeroShot/0/checkpoint-333/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:29:38,588 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-296] due to args.save_total_limit\n",
            " 40% 370/925 [04:44<04:46,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:30:01,555 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:30:01,557 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:30:01,557 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:30:01,557 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.17it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.47it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.17it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.56it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.26it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.15it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.99it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4969325065612793, 'eval_accuracy': 0.6483516693115234, 'eval_f1': 0.5975429975429976, 'eval_runtime': 2.8784, 'eval_samples_per_second': 94.844, 'eval_steps_per_second': 12.159, 'epoch': 10.0}\n",
            " 40% 370/925 [04:46<04:46,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:30:04,437 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-370\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:04,438 >> Configuration saved in models/ZeroShot/0/checkpoint-370/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:04,533 >> Module weights saved in models/ZeroShot/0/checkpoint-370/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:04,534 >> Configuration saved in models/ZeroShot/0/checkpoint-370/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:04,542 >> Module weights saved in models/ZeroShot/0/checkpoint-370/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:04,542 >> Configuration saved in models/ZeroShot/0/checkpoint-370/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:04,596 >> Module weights saved in models/ZeroShot/0/checkpoint-370/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:04,597 >> Configuration saved in models/ZeroShot/0/checkpoint-370/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:04,660 >> Module weights saved in models/ZeroShot/0/checkpoint-370/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:04,661 >> Configuration saved in models/ZeroShot/0/checkpoint-370/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:06,193 >> Module weights saved in models/ZeroShot/0/checkpoint-370/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:06,223 >> Configuration saved in models/ZeroShot/0/checkpoint-370/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:06,248 >> Module weights saved in models/ZeroShot/0/checkpoint-370/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:06,252 >> Configuration saved in models/ZeroShot/0/checkpoint-370/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:07,649 >> Module weights saved in models/ZeroShot/0/checkpoint-370/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:30:07,650 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-370/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:30:07,650 >> Special tokens file saved in models/ZeroShot/0/checkpoint-370/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:30:08,112 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-333] due to args.save_total_limit\n",
            " 44% 407/925 [05:13<04:27,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:30:31,167 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:30:31,169 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:30:31,169 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:30:31,169 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.03it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.37it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.16it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.55it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.25it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.11it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.85it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.83it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.82it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.264364004135132, 'eval_accuracy': 0.6703296899795532, 'eval_f1': 0.6492004568817817, 'eval_runtime': 2.896, 'eval_samples_per_second': 94.267, 'eval_steps_per_second': 12.085, 'epoch': 11.0}\n",
            " 44% 407/925 [05:16<04:27,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.27it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:30:34,066 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-407\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:34,067 >> Configuration saved in models/ZeroShot/0/checkpoint-407/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:34,155 >> Module weights saved in models/ZeroShot/0/checkpoint-407/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:34,156 >> Configuration saved in models/ZeroShot/0/checkpoint-407/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:34,163 >> Module weights saved in models/ZeroShot/0/checkpoint-407/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:34,163 >> Configuration saved in models/ZeroShot/0/checkpoint-407/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:34,215 >> Module weights saved in models/ZeroShot/0/checkpoint-407/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:34,216 >> Configuration saved in models/ZeroShot/0/checkpoint-407/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:34,279 >> Module weights saved in models/ZeroShot/0/checkpoint-407/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:34,279 >> Configuration saved in models/ZeroShot/0/checkpoint-407/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:35,731 >> Module weights saved in models/ZeroShot/0/checkpoint-407/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:35,744 >> Configuration saved in models/ZeroShot/0/checkpoint-407/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:35,756 >> Module weights saved in models/ZeroShot/0/checkpoint-407/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:30:35,757 >> Configuration saved in models/ZeroShot/0/checkpoint-407/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:30:37,145 >> Module weights saved in models/ZeroShot/0/checkpoint-407/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:30:37,145 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-407/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:30:37,146 >> Special tokens file saved in models/ZeroShot/0/checkpoint-407/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:30:37,607 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-370] due to args.save_total_limit\n",
            " 48% 444/925 [05:43<04:07,  1.95it/s][INFO|trainer.py:623] 2022-08-25 09:31:00,585 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:31:00,587 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:31:00,587 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:31:00,587 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.82it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.10it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.04it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.51it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.37it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.33it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.23it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.97it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.97it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.86it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.98it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5165393352508545, 'eval_accuracy': 0.6776556968688965, 'eval_f1': 0.6430524188755498, 'eval_runtime': 2.8843, 'eval_samples_per_second': 94.652, 'eval_steps_per_second': 12.135, 'epoch': 12.0}\n",
            " 48% 444/925 [05:45<04:07,  1.95it/s]\n",
            "100% 35/35 [00:02<00:00, 13.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:31:03,473 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-444\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:03,473 >> Configuration saved in models/ZeroShot/0/checkpoint-444/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:03,566 >> Module weights saved in models/ZeroShot/0/checkpoint-444/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:03,567 >> Configuration saved in models/ZeroShot/0/checkpoint-444/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:03,575 >> Module weights saved in models/ZeroShot/0/checkpoint-444/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:03,575 >> Configuration saved in models/ZeroShot/0/checkpoint-444/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:03,627 >> Module weights saved in models/ZeroShot/0/checkpoint-444/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:03,628 >> Configuration saved in models/ZeroShot/0/checkpoint-444/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:03,698 >> Module weights saved in models/ZeroShot/0/checkpoint-444/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:03,699 >> Configuration saved in models/ZeroShot/0/checkpoint-444/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:05,142 >> Module weights saved in models/ZeroShot/0/checkpoint-444/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:05,143 >> Configuration saved in models/ZeroShot/0/checkpoint-444/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:05,154 >> Module weights saved in models/ZeroShot/0/checkpoint-444/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:05,155 >> Configuration saved in models/ZeroShot/0/checkpoint-444/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:06,599 >> Module weights saved in models/ZeroShot/0/checkpoint-444/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:31:06,599 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-444/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:31:06,600 >> Special tokens file saved in models/ZeroShot/0/checkpoint-444/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:31:07,078 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-407] due to args.save_total_limit\n",
            " 52% 481/925 [06:12<03:49,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:31:30,095 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:31:30,098 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:31:30,098 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:31:30,098 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.90it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.46it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.15it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.35it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.91it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.84it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.73it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.75it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.77it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.82it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4212253093719482, 'eval_accuracy': 0.6776556968688965, 'eval_f1': 0.6465395480225988, 'eval_runtime': 2.9071, 'eval_samples_per_second': 93.907, 'eval_steps_per_second': 12.039, 'epoch': 13.0}\n",
            " 52% 481/925 [06:15<03:49,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 12.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:31:33,006 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-481\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:33,007 >> Configuration saved in models/ZeroShot/0/checkpoint-481/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:33,094 >> Module weights saved in models/ZeroShot/0/checkpoint-481/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:33,094 >> Configuration saved in models/ZeroShot/0/checkpoint-481/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:33,105 >> Module weights saved in models/ZeroShot/0/checkpoint-481/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:33,106 >> Configuration saved in models/ZeroShot/0/checkpoint-481/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:33,159 >> Module weights saved in models/ZeroShot/0/checkpoint-481/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:33,160 >> Configuration saved in models/ZeroShot/0/checkpoint-481/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:33,225 >> Module weights saved in models/ZeroShot/0/checkpoint-481/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:33,225 >> Configuration saved in models/ZeroShot/0/checkpoint-481/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:34,675 >> Module weights saved in models/ZeroShot/0/checkpoint-481/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:34,675 >> Configuration saved in models/ZeroShot/0/checkpoint-481/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:34,685 >> Module weights saved in models/ZeroShot/0/checkpoint-481/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:31:34,685 >> Configuration saved in models/ZeroShot/0/checkpoint-481/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:31:36,261 >> Module weights saved in models/ZeroShot/0/checkpoint-481/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:31:36,262 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-481/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:31:36,262 >> Special tokens file saved in models/ZeroShot/0/checkpoint-481/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:31:36,754 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-444] due to args.save_total_limit\n",
            "{'loss': 0.1881, 'learning_rate': 4.594594594594595e-05, 'epoch': 13.51}\n",
            " 56% 518/925 [06:42<03:29,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:31:59,724 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:31:59,726 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:31:59,726 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:31:59,726 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.38it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.65it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.27it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.60it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.33it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.15it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.12it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.97it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.85it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.441361665725708, 'eval_accuracy': 0.6886447072029114, 'eval_f1': 0.6653688081332467, 'eval_runtime': 2.8808, 'eval_samples_per_second': 94.765, 'eval_steps_per_second': 12.149, 'epoch': 14.0}\n",
            " 56% 518/925 [06:45<03:29,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.34it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:32:02,608 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-518\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:02,609 >> Configuration saved in models/ZeroShot/0/checkpoint-518/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:02,697 >> Module weights saved in models/ZeroShot/0/checkpoint-518/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:02,698 >> Configuration saved in models/ZeroShot/0/checkpoint-518/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:02,705 >> Module weights saved in models/ZeroShot/0/checkpoint-518/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:02,705 >> Configuration saved in models/ZeroShot/0/checkpoint-518/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:02,762 >> Module weights saved in models/ZeroShot/0/checkpoint-518/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:02,763 >> Configuration saved in models/ZeroShot/0/checkpoint-518/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:02,828 >> Module weights saved in models/ZeroShot/0/checkpoint-518/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:02,829 >> Configuration saved in models/ZeroShot/0/checkpoint-518/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:04,319 >> Module weights saved in models/ZeroShot/0/checkpoint-518/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:04,320 >> Configuration saved in models/ZeroShot/0/checkpoint-518/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:04,330 >> Module weights saved in models/ZeroShot/0/checkpoint-518/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:04,330 >> Configuration saved in models/ZeroShot/0/checkpoint-518/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:05,856 >> Module weights saved in models/ZeroShot/0/checkpoint-518/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:32:05,857 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-518/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:32:05,857 >> Special tokens file saved in models/ZeroShot/0/checkpoint-518/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:32:06,338 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-481] due to args.save_total_limit\n",
            " 60% 555/925 [07:11<03:09,  1.95it/s][INFO|trainer.py:623] 2022-08-25 09:32:29,264 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:32:29,266 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:32:29,266 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:32:29,266 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.99it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.21it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.20it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.61it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.23it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.26it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.28it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.11it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.87it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.76it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.654778003692627, 'eval_accuracy': 0.6849817037582397, 'eval_f1': 0.6475198174393466, 'eval_runtime': 2.8863, 'eval_samples_per_second': 94.586, 'eval_steps_per_second': 12.126, 'epoch': 15.0}\n",
            " 60% 555/925 [07:14<03:09,  1.95it/s]\n",
            "100% 35/35 [00:02<00:00, 13.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:32:32,154 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-555\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:32,154 >> Configuration saved in models/ZeroShot/0/checkpoint-555/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:32,250 >> Module weights saved in models/ZeroShot/0/checkpoint-555/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:32,251 >> Configuration saved in models/ZeroShot/0/checkpoint-555/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:32,259 >> Module weights saved in models/ZeroShot/0/checkpoint-555/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:32,259 >> Configuration saved in models/ZeroShot/0/checkpoint-555/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:32,311 >> Module weights saved in models/ZeroShot/0/checkpoint-555/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:32,312 >> Configuration saved in models/ZeroShot/0/checkpoint-555/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:32,382 >> Module weights saved in models/ZeroShot/0/checkpoint-555/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:32,383 >> Configuration saved in models/ZeroShot/0/checkpoint-555/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:33,928 >> Module weights saved in models/ZeroShot/0/checkpoint-555/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:33,945 >> Configuration saved in models/ZeroShot/0/checkpoint-555/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:33,959 >> Module weights saved in models/ZeroShot/0/checkpoint-555/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:32:33,960 >> Configuration saved in models/ZeroShot/0/checkpoint-555/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:32:35,363 >> Module weights saved in models/ZeroShot/0/checkpoint-555/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:32:35,363 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-555/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:32:35,363 >> Special tokens file saved in models/ZeroShot/0/checkpoint-555/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:32:35,829 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-518] due to args.save_total_limit\n",
            " 64% 592/925 [07:41<02:52,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:32:58,849 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:32:58,851 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:32:58,851 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:32:58,851 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.18it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.45it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.38it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.54it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.27it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.04it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.98it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.82it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.75it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.04it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.06it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.97it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6996047496795654, 'eval_accuracy': 0.6703296899795532, 'eval_f1': 0.6311253903435022, 'eval_runtime': 2.8854, 'eval_samples_per_second': 94.613, 'eval_steps_per_second': 12.13, 'epoch': 16.0}\n",
            " 64% 592/925 [07:44<02:52,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 13.32it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:33:01,738 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-592\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:01,739 >> Configuration saved in models/ZeroShot/0/checkpoint-592/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:01,825 >> Module weights saved in models/ZeroShot/0/checkpoint-592/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:01,826 >> Configuration saved in models/ZeroShot/0/checkpoint-592/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:01,834 >> Module weights saved in models/ZeroShot/0/checkpoint-592/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:01,834 >> Configuration saved in models/ZeroShot/0/checkpoint-592/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:01,884 >> Module weights saved in models/ZeroShot/0/checkpoint-592/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:01,885 >> Configuration saved in models/ZeroShot/0/checkpoint-592/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:01,995 >> Module weights saved in models/ZeroShot/0/checkpoint-592/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:01,996 >> Configuration saved in models/ZeroShot/0/checkpoint-592/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:03,482 >> Module weights saved in models/ZeroShot/0/checkpoint-592/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:03,531 >> Configuration saved in models/ZeroShot/0/checkpoint-592/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:03,608 >> Module weights saved in models/ZeroShot/0/checkpoint-592/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:03,608 >> Configuration saved in models/ZeroShot/0/checkpoint-592/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:05,070 >> Module weights saved in models/ZeroShot/0/checkpoint-592/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:33:05,070 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-592/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:33:05,071 >> Special tokens file saved in models/ZeroShot/0/checkpoint-592/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:33:05,527 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-555] due to args.save_total_limit\n",
            " 68% 629/925 [08:10<02:32,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:33:28,516 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:33:28,518 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:33:28,518 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:33:28,518 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.72it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.21it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.02it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.52it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.28it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.87it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.85it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.85it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.80it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.9759981632232666, 'eval_accuracy': 0.6593406796455383, 'eval_f1': 0.5988655933515554, 'eval_runtime': 2.8974, 'eval_samples_per_second': 94.221, 'eval_steps_per_second': 12.08, 'epoch': 17.0}\n",
            " 68% 629/925 [08:13<02:32,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.28it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:33:31,416 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-629\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:31,417 >> Configuration saved in models/ZeroShot/0/checkpoint-629/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:31,508 >> Module weights saved in models/ZeroShot/0/checkpoint-629/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:31,508 >> Configuration saved in models/ZeroShot/0/checkpoint-629/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:31,515 >> Module weights saved in models/ZeroShot/0/checkpoint-629/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:31,516 >> Configuration saved in models/ZeroShot/0/checkpoint-629/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:31,567 >> Module weights saved in models/ZeroShot/0/checkpoint-629/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:31,567 >> Configuration saved in models/ZeroShot/0/checkpoint-629/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:31,632 >> Module weights saved in models/ZeroShot/0/checkpoint-629/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:31,632 >> Configuration saved in models/ZeroShot/0/checkpoint-629/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:33,090 >> Module weights saved in models/ZeroShot/0/checkpoint-629/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:33,091 >> Configuration saved in models/ZeroShot/0/checkpoint-629/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:33,202 >> Module weights saved in models/ZeroShot/0/checkpoint-629/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:33:33,202 >> Configuration saved in models/ZeroShot/0/checkpoint-629/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:33:34,677 >> Module weights saved in models/ZeroShot/0/checkpoint-629/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:33:34,678 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-629/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:33:34,678 >> Special tokens file saved in models/ZeroShot/0/checkpoint-629/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:33:35,166 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-592] due to args.save_total_limit\n",
            " 72% 666/925 [08:40<02:13,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:33:58,155 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:33:58,157 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:33:58,157 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:33:58,157 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.96it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.44it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.18it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.57it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.30it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.11it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.87it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.01it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.8993031978607178, 'eval_accuracy': 0.6483516693115234, 'eval_f1': 0.5975429975429976, 'eval_runtime': 2.8816, 'eval_samples_per_second': 94.739, 'eval_steps_per_second': 12.146, 'epoch': 18.0}\n",
            " 72% 666/925 [08:43<02:13,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.32it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:34:01,040 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-666\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:01,040 >> Configuration saved in models/ZeroShot/0/checkpoint-666/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:01,136 >> Module weights saved in models/ZeroShot/0/checkpoint-666/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:01,136 >> Configuration saved in models/ZeroShot/0/checkpoint-666/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:01,148 >> Module weights saved in models/ZeroShot/0/checkpoint-666/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:01,149 >> Configuration saved in models/ZeroShot/0/checkpoint-666/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:01,202 >> Module weights saved in models/ZeroShot/0/checkpoint-666/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:01,203 >> Configuration saved in models/ZeroShot/0/checkpoint-666/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:01,269 >> Module weights saved in models/ZeroShot/0/checkpoint-666/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:01,269 >> Configuration saved in models/ZeroShot/0/checkpoint-666/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:02,757 >> Module weights saved in models/ZeroShot/0/checkpoint-666/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:02,791 >> Configuration saved in models/ZeroShot/0/checkpoint-666/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:02,805 >> Module weights saved in models/ZeroShot/0/checkpoint-666/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:02,805 >> Configuration saved in models/ZeroShot/0/checkpoint-666/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:04,305 >> Module weights saved in models/ZeroShot/0/checkpoint-666/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:34:04,306 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-666/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:34:04,307 >> Special tokens file saved in models/ZeroShot/0/checkpoint-666/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:34:04,800 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-629] due to args.save_total_limit\n",
            " 76% 703/925 [09:10<01:54,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:34:27,745 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:34:27,748 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:34:27,748 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:34:27,748 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.60it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.15it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.05it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.49it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.19it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.95it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.83it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.83it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.03it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.85it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.8193440437316895, 'eval_accuracy': 0.6703296899795532, 'eval_f1': 0.6291208791208791, 'eval_runtime': 2.8927, 'eval_samples_per_second': 94.375, 'eval_steps_per_second': 12.099, 'epoch': 19.0}\n",
            " 76% 703/925 [09:13<01:54,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 13.33it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:34:30,642 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-703\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:30,642 >> Configuration saved in models/ZeroShot/0/checkpoint-703/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:30,730 >> Module weights saved in models/ZeroShot/0/checkpoint-703/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:30,731 >> Configuration saved in models/ZeroShot/0/checkpoint-703/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:30,738 >> Module weights saved in models/ZeroShot/0/checkpoint-703/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:30,739 >> Configuration saved in models/ZeroShot/0/checkpoint-703/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:30,795 >> Module weights saved in models/ZeroShot/0/checkpoint-703/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:30,795 >> Configuration saved in models/ZeroShot/0/checkpoint-703/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:30,863 >> Module weights saved in models/ZeroShot/0/checkpoint-703/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:30,864 >> Configuration saved in models/ZeroShot/0/checkpoint-703/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:32,277 >> Module weights saved in models/ZeroShot/0/checkpoint-703/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:32,451 >> Configuration saved in models/ZeroShot/0/checkpoint-703/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:32,464 >> Module weights saved in models/ZeroShot/0/checkpoint-703/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:34:32,464 >> Configuration saved in models/ZeroShot/0/checkpoint-703/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:34:33,892 >> Module weights saved in models/ZeroShot/0/checkpoint-703/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:34:33,893 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-703/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:34:33,893 >> Special tokens file saved in models/ZeroShot/0/checkpoint-703/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:34:34,373 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-666] due to args.save_total_limit\n",
            " 80% 740/925 [09:39<01:35,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:34:57,303 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:34:57,305 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:34:57,305 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:34:57,305 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.87it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.42it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.21it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.56it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.30it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.99it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.88it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.84it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.73it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.78it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.78it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.76it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.8262951374053955, 'eval_accuracy': 0.6776556968688965, 'eval_f1': 0.6373626373626374, 'eval_runtime': 2.9103, 'eval_samples_per_second': 93.804, 'eval_steps_per_second': 12.026, 'epoch': 20.0}\n",
            " 80% 740/925 [09:42<01:35,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:35:00,217 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-740\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:00,218 >> Configuration saved in models/ZeroShot/0/checkpoint-740/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:00,305 >> Module weights saved in models/ZeroShot/0/checkpoint-740/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:00,305 >> Configuration saved in models/ZeroShot/0/checkpoint-740/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:00,313 >> Module weights saved in models/ZeroShot/0/checkpoint-740/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:00,314 >> Configuration saved in models/ZeroShot/0/checkpoint-740/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:00,365 >> Module weights saved in models/ZeroShot/0/checkpoint-740/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:00,366 >> Configuration saved in models/ZeroShot/0/checkpoint-740/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:00,434 >> Module weights saved in models/ZeroShot/0/checkpoint-740/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:00,434 >> Configuration saved in models/ZeroShot/0/checkpoint-740/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:01,927 >> Module weights saved in models/ZeroShot/0/checkpoint-740/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:01,928 >> Configuration saved in models/ZeroShot/0/checkpoint-740/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:01,938 >> Module weights saved in models/ZeroShot/0/checkpoint-740/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:01,938 >> Configuration saved in models/ZeroShot/0/checkpoint-740/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:03,401 >> Module weights saved in models/ZeroShot/0/checkpoint-740/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:35:03,402 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-740/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:35:03,402 >> Special tokens file saved in models/ZeroShot/0/checkpoint-740/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:35:03,886 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-703] due to args.save_total_limit\n",
            " 84% 777/925 [10:09<01:16,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:35:26,900 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:35:26,901 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:35:26,901 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:35:26,902 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.57it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.16it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.13it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.46it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.20it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.11it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.85it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.80it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.78it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.78it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.85it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.78it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.736456871032715, 'eval_accuracy': 0.6813187003135681, 'eval_f1': 0.6497160976329179, 'eval_runtime': 2.8969, 'eval_samples_per_second': 94.239, 'eval_steps_per_second': 12.082, 'epoch': 21.0}\n",
            " 84% 777/925 [10:12<01:16,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 13.35it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:35:29,800 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-777\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:29,800 >> Configuration saved in models/ZeroShot/0/checkpoint-777/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:29,891 >> Module weights saved in models/ZeroShot/0/checkpoint-777/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:29,892 >> Configuration saved in models/ZeroShot/0/checkpoint-777/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:29,899 >> Module weights saved in models/ZeroShot/0/checkpoint-777/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:29,900 >> Configuration saved in models/ZeroShot/0/checkpoint-777/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:29,951 >> Module weights saved in models/ZeroShot/0/checkpoint-777/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:29,951 >> Configuration saved in models/ZeroShot/0/checkpoint-777/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:30,020 >> Module weights saved in models/ZeroShot/0/checkpoint-777/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:30,020 >> Configuration saved in models/ZeroShot/0/checkpoint-777/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:31,290 >> Module weights saved in models/ZeroShot/0/checkpoint-777/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:31,319 >> Configuration saved in models/ZeroShot/0/checkpoint-777/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:31,332 >> Module weights saved in models/ZeroShot/0/checkpoint-777/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:31,332 >> Configuration saved in models/ZeroShot/0/checkpoint-777/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:32,800 >> Module weights saved in models/ZeroShot/0/checkpoint-777/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:35:32,800 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-777/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:35:32,801 >> Special tokens file saved in models/ZeroShot/0/checkpoint-777/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:35:33,273 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-740] due to args.save_total_limit\n",
            " 88% 814/925 [10:38<00:57,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:35:56,242 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:35:56,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:35:56,245 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:35:56,245 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.73it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.11it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.00it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.50it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.26it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.83it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.87it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.98it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.76it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.7930192947387695, 'eval_accuracy': 0.6703296899795532, 'eval_f1': 0.634939973849994, 'eval_runtime': 2.8976, 'eval_samples_per_second': 94.216, 'eval_steps_per_second': 12.079, 'epoch': 22.0}\n",
            " 88% 814/925 [10:41<00:57,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 13.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:35:59,144 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-814\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:59,144 >> Configuration saved in models/ZeroShot/0/checkpoint-814/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:59,238 >> Module weights saved in models/ZeroShot/0/checkpoint-814/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:59,238 >> Configuration saved in models/ZeroShot/0/checkpoint-814/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:59,246 >> Module weights saved in models/ZeroShot/0/checkpoint-814/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:59,246 >> Configuration saved in models/ZeroShot/0/checkpoint-814/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:59,296 >> Module weights saved in models/ZeroShot/0/checkpoint-814/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:59,296 >> Configuration saved in models/ZeroShot/0/checkpoint-814/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:35:59,361 >> Module weights saved in models/ZeroShot/0/checkpoint-814/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:35:59,362 >> Configuration saved in models/ZeroShot/0/checkpoint-814/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:00,457 >> Module weights saved in models/ZeroShot/0/checkpoint-814/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:00,610 >> Configuration saved in models/ZeroShot/0/checkpoint-814/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:00,626 >> Module weights saved in models/ZeroShot/0/checkpoint-814/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:00,626 >> Configuration saved in models/ZeroShot/0/checkpoint-814/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:02,098 >> Module weights saved in models/ZeroShot/0/checkpoint-814/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:36:02,099 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-814/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:36:02,099 >> Special tokens file saved in models/ZeroShot/0/checkpoint-814/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:36:02,563 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-777] due to args.save_total_limit\n",
            " 92% 851/925 [11:08<00:38,  1.94it/s][INFO|trainer.py:623] 2022-08-25 09:36:25,549 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:36:25,551 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:36:25,551 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:36:25,551 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.33it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.12it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.15it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.46it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.18it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.12it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.91it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.84it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.79it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.81it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.82it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.68it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.71it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.796830177307129, 'eval_accuracy': 0.6739926934242249, 'eval_f1': 0.6399004046062869, 'eval_runtime': 2.9098, 'eval_samples_per_second': 93.822, 'eval_steps_per_second': 12.028, 'epoch': 23.0}\n",
            " 92% 851/925 [11:10<00:38,  1.94it/s]\n",
            "100% 35/35 [00:02<00:00, 13.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:36:28,462 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-851\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:28,463 >> Configuration saved in models/ZeroShot/0/checkpoint-851/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:28,552 >> Module weights saved in models/ZeroShot/0/checkpoint-851/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:28,553 >> Configuration saved in models/ZeroShot/0/checkpoint-851/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:28,560 >> Module weights saved in models/ZeroShot/0/checkpoint-851/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:28,561 >> Configuration saved in models/ZeroShot/0/checkpoint-851/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:28,612 >> Module weights saved in models/ZeroShot/0/checkpoint-851/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:28,613 >> Configuration saved in models/ZeroShot/0/checkpoint-851/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:28,705 >> Module weights saved in models/ZeroShot/0/checkpoint-851/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:28,706 >> Configuration saved in models/ZeroShot/0/checkpoint-851/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:30,131 >> Module weights saved in models/ZeroShot/0/checkpoint-851/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:30,282 >> Configuration saved in models/ZeroShot/0/checkpoint-851/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:30,296 >> Module weights saved in models/ZeroShot/0/checkpoint-851/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:30,297 >> Configuration saved in models/ZeroShot/0/checkpoint-851/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:31,762 >> Module weights saved in models/ZeroShot/0/checkpoint-851/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:36:31,763 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-851/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:36:31,763 >> Special tokens file saved in models/ZeroShot/0/checkpoint-851/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:36:32,210 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-814] due to args.save_total_limit\n",
            " 96% 888/925 [11:37<00:19,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:36:55,174 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:36:55,176 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:36:55,176 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:36:55,176 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.05it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.47it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.18it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.57it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.08it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.11it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.83it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.71it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.77it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.8303940296173096, 'eval_accuracy': 0.6739926934242249, 'eval_f1': 0.6399004046062869, 'eval_runtime': 2.8852, 'eval_samples_per_second': 94.621, 'eval_steps_per_second': 12.131, 'epoch': 24.0}\n",
            " 96% 888/925 [11:40<00:19,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 13.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:36:58,063 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-888\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:58,063 >> Configuration saved in models/ZeroShot/0/checkpoint-888/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:58,153 >> Module weights saved in models/ZeroShot/0/checkpoint-888/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:58,153 >> Configuration saved in models/ZeroShot/0/checkpoint-888/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:58,160 >> Module weights saved in models/ZeroShot/0/checkpoint-888/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:58,161 >> Configuration saved in models/ZeroShot/0/checkpoint-888/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:58,219 >> Module weights saved in models/ZeroShot/0/checkpoint-888/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:58,219 >> Configuration saved in models/ZeroShot/0/checkpoint-888/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:58,291 >> Module weights saved in models/ZeroShot/0/checkpoint-888/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:58,291 >> Configuration saved in models/ZeroShot/0/checkpoint-888/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:59,442 >> Module weights saved in models/ZeroShot/0/checkpoint-888/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:59,466 >> Configuration saved in models/ZeroShot/0/checkpoint-888/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:36:59,479 >> Module weights saved in models/ZeroShot/0/checkpoint-888/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:36:59,480 >> Configuration saved in models/ZeroShot/0/checkpoint-888/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:00,996 >> Module weights saved in models/ZeroShot/0/checkpoint-888/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:37:00,996 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-888/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:37:00,997 >> Special tokens file saved in models/ZeroShot/0/checkpoint-888/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:37:01,481 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-851] due to args.save_total_limit\n",
            "100% 925/925 [12:06<00:00,  1.93it/s][INFO|trainer.py:623] 2022-08-25 09:37:24,501 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:37:24,504 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:37:24,504 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:37:24,504 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.89it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.28it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.08it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.56it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.19it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.90it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.81it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.82it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.8324036598205566, 'eval_accuracy': 0.6739926934242249, 'eval_f1': 0.6399004046062869, 'eval_runtime': 2.8948, 'eval_samples_per_second': 94.308, 'eval_steps_per_second': 12.091, 'epoch': 25.0}\n",
            "100% 925/925 [12:09<00:00,  1.93it/s]\n",
            "100% 35/35 [00:02<00:00, 13.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:37:27,400 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-925\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:27,401 >> Configuration saved in models/ZeroShot/0/checkpoint-925/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:27,491 >> Module weights saved in models/ZeroShot/0/checkpoint-925/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:27,492 >> Configuration saved in models/ZeroShot/0/checkpoint-925/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:27,499 >> Module weights saved in models/ZeroShot/0/checkpoint-925/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:27,500 >> Configuration saved in models/ZeroShot/0/checkpoint-925/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:27,559 >> Module weights saved in models/ZeroShot/0/checkpoint-925/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:27,559 >> Configuration saved in models/ZeroShot/0/checkpoint-925/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:27,625 >> Module weights saved in models/ZeroShot/0/checkpoint-925/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:27,625 >> Configuration saved in models/ZeroShot/0/checkpoint-925/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:29,104 >> Module weights saved in models/ZeroShot/0/checkpoint-925/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:29,170 >> Configuration saved in models/ZeroShot/0/checkpoint-925/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:29,184 >> Module weights saved in models/ZeroShot/0/checkpoint-925/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:29,185 >> Configuration saved in models/ZeroShot/0/checkpoint-925/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:30,612 >> Module weights saved in models/ZeroShot/0/checkpoint-925/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:37:30,613 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-925/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:37:30,613 >> Special tokens file saved in models/ZeroShot/0/checkpoint-925/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:37:31,088 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-888] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 09:37:31,145 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 09:37:31,146 >> Loading best model from models/ZeroShot/0/checkpoint-148 (score: 0.6749681341307971).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 09:37:31,146 >> Could not locate the best model at models/ZeroShot/0/checkpoint-148/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 733.6216, 'train_samples_per_second': 39.666, 'train_steps_per_second': 1.261, 'train_loss': 0.10283183574676513, 'epoch': 25.0}\n",
            "100% 925/925 [12:13<00:00,  1.93it/s][INFO|trainer.py:238] 2022-08-25 09:37:31,177 >> Loading best adapter(s) from models/ZeroShot/0/checkpoint-148 (score: 0.6749681341307971).\n",
            "[INFO|loading.py:77] 2022-08-25 09:37:31,178 >> Loading module configuration from models/ZeroShot/0/checkpoint-148/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 09:37:31,184 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:37:31,800 >> Loading module weights from models/ZeroShot/0/checkpoint-148/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:37:31,823 >> Loading module configuration from models/ZeroShot/0/checkpoint-148/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 09:37:31,823 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 09:37:31,835 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 09:37:31,850 >> Loading module weights from models/ZeroShot/0/checkpoint-148/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:37:31,853 >> Loading module configuration from models/ZeroShot/0/checkpoint-148/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 09:37:31,854 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:37:32,013 >> Loading module weights from models/ZeroShot/0/checkpoint-148/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 09:37:32,031 >> No matching prediction head found in 'models/ZeroShot/0/checkpoint-148/en'\n",
            "[INFO|loading.py:77] 2022-08-25 09:37:32,031 >> Loading module configuration from models/ZeroShot/0/checkpoint-148/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 09:37:32,032 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:37:32,180 >> Loading module weights from models/ZeroShot/0/checkpoint-148/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:37:32,199 >> Loading module configuration from models/ZeroShot/0/checkpoint-148/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 09:37:32,199 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 09:37:33,398 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 09:37:35,023 >> Loading module weights from models/ZeroShot/0/checkpoint-148/pt/pytorch_model_head.bin\n",
            "100% 925/925 [12:17<00:00,  1.25it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 09:37:35,112 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:35,113 >> Configuration saved in models/ZeroShot/0/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:35,229 >> Module weights saved in models/ZeroShot/0/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:35,230 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:35,237 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:35,238 >> Configuration saved in models/ZeroShot/0/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:35,298 >> Module weights saved in models/ZeroShot/0/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:35,299 >> Configuration saved in models/ZeroShot/0/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:35,372 >> Module weights saved in models/ZeroShot/0/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:35,372 >> Configuration saved in models/ZeroShot/0/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:36,837 >> Module weights saved in models/ZeroShot/0/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:36,930 >> Configuration saved in models/ZeroShot/0/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:36,951 >> Module weights saved in models/ZeroShot/0/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:37:36,952 >> Configuration saved in models/ZeroShot/0/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:37:38,425 >> Module weights saved in models/ZeroShot/0/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:37:38,475 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:37:38,476 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.1028\n",
            "  train_runtime            = 0:12:13.62\n",
            "  train_samples            =       1164\n",
            "  train_samples_per_second =     39.666\n",
            "  train_steps_per_second   =      1.261\n",
            "08/25/2022 09:37:38 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:623] 2022-08-25 09:37:38,677 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:37:38,678 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:37:38,678 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:37:38,678 >>   Batch size = 8\n",
            "100% 35/35 [00:02<00:00, 12.88it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.6886\n",
            "  eval_f1                 =      0.675\n",
            "  eval_loss               =     0.8445\n",
            "  eval_runtime            = 0:00:02.81\n",
            "  eval_samples            =        273\n",
            "  eval_samples_per_second =     96.827\n",
            "  eval_steps_per_second   =     12.414\n",
            "[INFO|modelcard.py:460] 2022-08-25 09:37:41,606 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6886447072029114}, {'name': 'F1', 'type': 'f1', 'value': 0.6749681341307971}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for zero-shot EN-PT idiomatic knowledge transfer"
      ],
      "metadata": {
        "id": "nTJPiYN2LAeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer from English to Portuguese\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/ZeroShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/ZeroShot/EN/train.csv \\\n",
        "  --validation_file Data/ZeroShot/PT/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "kuad1ZspKdgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5070eaf-4178-430d-b685-7078e7ff4792"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 05:08:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 05:08:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_05-08-42_4f312c5f34f7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 05:08:42 - INFO - __main__ - load a local file for train: Data/ZeroShot/EN/train.csv\n",
            "08/25/2022 05:08:42 - INFO - __main__ - load a local file for validation: Data/ZeroShot/PT/dev.csv\n",
            "08/25/2022 05:08:42 - WARNING - datasets.builder - Using custom data configuration default-e5bf3206734fcf83\n",
            "08/25/2022 05:08:42 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-e5bf3206734fcf83/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e5bf3206734fcf83/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11491.24it/s]\n",
            "08/25/2022 05:08:42 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 05:08:42 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1410.09it/s]\n",
            "08/25/2022 05:08:42 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 05:08:42 - INFO - datasets.builder - Generating train split\n",
            "08/25/2022 05:08:42 - INFO - datasets.builder - Generating validation split\n",
            "08/25/2022 05:08:42 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e5bf3206734fcf83/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1064.00it/s]\n",
            "[INFO|hub.py:591] 2022-08-25 05:08:43,120 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbnz0e6s8\n",
            "Downloading: 100% 625/625 [00:00<00:00, 662kB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:08:43,387 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|hub.py:603] 2022-08-25 05:08:43,387 >> creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 05:08:43,387 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 05:08:43,388 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 05:08:43,662 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpat60301g\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 28.5kB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:08:43,929 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|hub.py:603] 2022-08-25 05:08:43,929 >> creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 05:08:44,201 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 05:08:44,202 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 05:08:44,729 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp60g3_e3d\n",
            "Downloading: 100% 972k/972k [00:00<00:00, 2.67MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:08:45,393 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:603] 2022-08-25 05:08:45,393 >> creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|hub.py:591] 2022-08-25 05:08:45,658 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4bvqleme\n",
            "Downloading: 100% 1.87M/1.87M [00:00<00:00, 4.51MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:08:46,437 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|hub.py:603] 2022-08-25 05:08:46,437 >> creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 05:08:47,233 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 05:08:47,233 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 05:08:47,233 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 05:08:47,233 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 05:08:47,233 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 05:08:47,497 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 05:08:47,498 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|hub.py:591] 2022-08-25 05:08:47,880 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6c6hnqai\n",
            "Downloading: 100% 681M/681M [00:09<00:00, 72.5MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:08:57,861 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|hub.py:603] 2022-08-25 05:08:57,861 >> creating metadata file for /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 05:08:57,861 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 05:09:00,010 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 05:09:00,010 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 05:09:00,021 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 05:09:00,022 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 05:09:00,279 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|hub.py:591] 2022-08-25 05:09:00,454 >> https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/v2/index/bert-base-multilingual-cased.json not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmpyon5v808\n",
            "Downloading: 24.6kB [00:00, 26.0MB/s]       \n",
            "[INFO|hub.py:595] 2022-08-25 05:09:00,489 >> storing https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/v2/index/bert-base-multilingual-cased.json in cache at ~/.cache/torch/adapters/08d069267cb1d67d820deec1e19b1a91aef2bdf0e64fa8456c440abe23c3ddb4.a7da128024bcd4efcca057684afe69285b8f3279e656c8ec9d8af63e19290817\n",
            "[INFO|hub.py:603] 2022-08-25 05:09:00,489 >> creating metadata file for ~/.cache/torch/adapters/08d069267cb1d67d820deec1e19b1a91aef2bdf0e64fa8456c440abe23c3ddb4.a7da128024bcd4efcca057684afe69285b8f3279e656c8ec9d8af63e19290817\n",
            "[INFO|utils.py:327] 2022-08-25 05:09:00,490 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 05:09:00,665 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|hub.py:591] 2022-08-25 05:09:01,458 >> https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmphxsu8zcg\n",
            "Downloading: 100% 28.2M/28.2M [00:05<00:00, 5.04MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:09:08,107 >> storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip in cache at ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf.3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a\n",
            "[INFO|hub.py:603] 2022-08-25 05:09:08,107 >> creating metadata file for ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf.3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a\n",
            "[INFO|loading.py:77] 2022-08-25 05:09:08,205 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 05:09:08,206 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 05:09:08,351 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 05:09:08,361 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 05:09:08,362 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 05:09:08,401 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 05:09:08,575 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|hub.py:591] 2022-08-25 05:09:09,194 >> https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip not found in cache or force_download set to True, downloading to /content/~/.cache/torch/adapters/tmpri2aigac\n",
            "Downloading: 100% 381M/381M [00:24<00:00, 16.3MB/s]\n",
            "[INFO|hub.py:595] 2022-08-25 05:09:34,493 >> storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip in cache at ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e.babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c\n",
            "[INFO|hub.py:603] 2022-08-25 05:09:34,493 >> creating metadata file for ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e.babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c\n",
            "[INFO|loading.py:77] 2022-08-25 05:09:36,391 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 05:09:36,391 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 05:09:36,518 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 05:09:36,530 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 05:09:36,530 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 05:09:37,714 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 05:09:37,825 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 05:09:37,892 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with EN language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]08/25/2022 05:09:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e5bf3206734fcf83/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-fc54532ae7759f98.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  5.93ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 05:09:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e5bf3206734fcf83/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-a49f9767de93b254.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 16.72ba/s]\n",
            "08/25/2022 05:09:38 - INFO - __main__ - Sample 550 of the training set: {'label': 1, 'sentence1': 'Georgia, like California, has one of the highest case rates in the country. Unlike California, Georgia recently completed its high school football season. It did not cause the massive outbreaks we were worried about,” Burroughs said.', 'input_ids': [101, 15234, 117, 11850, 11621, 117, 10393, 10464, 10108, 10105, 18134, 13474, 38200, 10106, 10105, 12723, 119, 45227, 11621, 117, 15234, 23746, 15782, 10474, 11846, 11393, 12485, 11226, 119, 10377, 12172, 10472, 15311, 10105, 35394, 63831, 10107, 11951, 10309, 12796, 24874, 10336, 10978, 117, 100, 109110, 12415, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 05:09:38 - INFO - __main__ - Sample 2331 of the training set: {'label': 0, 'sentence1': 'Unfortunately, one head teacher, Liam, whose professional frustration I featured on this page some months ago, was back in touch this week with a sad tale, which floored me when I read his thoughts: “I have left the profession, retiring at a ridiculously young age, despite the many years I could have given. Mental wellbeing is too important and there are head teachers who are sacrificing their own sanity at the moment to graft for our children, despite the truly awful experience of dealing with an Education Minister who simply doesn’t care and is not fit for office. I had at least 10 more years to give to the profession.', 'input_ids': [101, 109320, 117, 10464, 13578, 24996, 117, 48437, 117, 16879, 14054, 12127, 106178, 146, 15873, 10135, 10531, 15975, 11152, 15555, 36390, 117, 10134, 12014, 10106, 54981, 10531, 16118, 10169, 169, 81708, 17307, 117, 10319, 23861, 10336, 10911, 10841, 146, 24944, 10226, 18957, 10107, 131, 100, 146, 10529, 12153, 10105, 56401, 117, 75933, 10160, 169, 29956, 55170, 22540, 61289, 14739, 12089, 117, 22087, 10105, 11299, 10855, 146, 12174, 10529, 13507, 119, 69268, 11206, 40946, 10376, 10124, 16683, 12452, 10111, 11155, 10301, 13578, 38160, 10479, 10301, 109436, 52070, 13439, 10376, 10455, 12542, 14608, 11949, 10160, 10105, 14316, 10114, 26194, 10123, 10142, 17446, 12694, 117, 22087, 10105, 92755, 56237, 14446, 20627, 10108, 73082, 10169, 10151, 15063, 14355, 10479, 26097, 47798, 100, 188, 11131, 10111, 10124, 10472, 21635, 10142, 14301, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "08/25/2022 05:09:38 - INFO - __main__ - Sample 3286 of the training set: {'label': 1, 'sentence1': 'In the episode, the film\\'s director, producer and screenwriter Malcolm D. Lee will be joined by cast members Morris Chestnut, Monica Calhoun, Melissa DeSoussa, Terrence Howard, Harold Perrineau and more to share stories and memories about the film. The 1999 rom-com (which spawned the 2013 sequel, \"The Best Man Holiday\") serves as a highlight in the careers for many involved in the film, but most notably, launched Lee\\'s career. The Best Man\\' is incredibly special to me: it was my first feature and truly a story and characters that came from the heart representing images of Black culture that I felt was missing on screen,\" Lee said.', 'input_ids': [101, 10167, 10105, 14320, 117, 10105, 10458, 112, 187, 12461, 117, 16607, 10111, 99204, 26572, 141, 119, 12006, 11337, 10347, 13914, 10155, 18922, 12464, 17161, 44131, 10562, 18548, 117, 29242, 109523, 117, 42409, 10190, 10731, 13499, 10466, 117, 27203, 12150, 15084, 117, 19675, 81068, 19713, 10111, 10798, 10114, 23867, 21158, 10111, 87012, 10978, 10105, 10458, 119, 10117, 10324, 86945, 118, 10212, 113, 10319, 32650, 80766, 10336, 10105, 10207, 48333, 117, 107, 10117, 11730, 11343, 40205, 107, 114, 24474, 10146, 169, 11846, 24310, 10106, 10105, 110196, 10142, 11299, 16247, 10106, 10105, 10458, 117, 10473, 10992, 36900, 117, 18850, 12006, 112, 187, 13021, 119, 10117, 11730, 11343, 112, 10124, 10106, 27794, 10703, 31748, 14478, 10114, 10911, 131, 10271, 10134, 15127, 10422, 19072, 10111, 92755, 169, 13617, 10111, 19174, 10189, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 05:09:44,888 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 05:09:44,904 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 05:09:44,904 >>   Num examples = 3327\n",
            "[INFO|trainer.py:1421] 2022-08-25 05:09:44,904 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 05:09:44,904 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 05:09:44,904 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 05:09:44,905 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 05:09:44,905 >>   Total optimization steps = 2600\n",
            "  4% 104/2600 [01:01<24:40,  1.69it/s][INFO|trainer.py:623] 2022-08-25 05:10:46,517 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:10:46,519 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:10:46,519 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:10:46,519 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.75it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.76it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.69it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.03it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.86it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.57it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.61it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.51it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.42it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.30it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.29it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.28it/s]\u001b[A\n",
            "100% 35/35 [00:02<00:00, 13.61it/s]\u001b[A\n",
            "{'eval_loss': 0.8024904727935791, 'eval_accuracy': 0.5677655935287476, 'eval_f1': 0.5494797493846498, 'eval_runtime': 2.7974, 'eval_samples_per_second': 97.592, 'eval_steps_per_second': 12.512, 'epoch': 1.0}\n",
            "\n",
            "  4% 104/2600 [01:04<24:40,  1.69it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:10:49,318 >> Saving model checkpoint to models/OneShot/1/checkpoint-104\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:49,319 >> Configuration saved in models/OneShot/1/checkpoint-104/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:49,441 >> Module weights saved in models/OneShot/1/checkpoint-104/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:49,441 >> Configuration saved in models/OneShot/1/checkpoint-104/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:49,450 >> Module weights saved in models/OneShot/1/checkpoint-104/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:49,450 >> Configuration saved in models/OneShot/1/checkpoint-104/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:49,537 >> Module weights saved in models/OneShot/1/checkpoint-104/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:49,538 >> Configuration saved in models/OneShot/1/checkpoint-104/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:49,643 >> Module weights saved in models/OneShot/1/checkpoint-104/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:49,644 >> Configuration saved in models/OneShot/1/checkpoint-104/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:51,362 >> Module weights saved in models/OneShot/1/checkpoint-104/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:51,364 >> Configuration saved in models/OneShot/1/checkpoint-104/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:51,388 >> Module weights saved in models/OneShot/1/checkpoint-104/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:10:51,388 >> Configuration saved in models/OneShot/1/checkpoint-104/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:10:52,841 >> Module weights saved in models/OneShot/1/checkpoint-104/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:10:52,842 >> tokenizer config file saved in models/OneShot/1/checkpoint-104/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:10:52,842 >> Special tokens file saved in models/OneShot/1/checkpoint-104/special_tokens_map.json\n",
            "  8% 208/2600 [02:14<26:12,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:11:59,171 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:11:59,172 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:11:59,172 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:11:59,172 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.50it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.36it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.22it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.75it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.42it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.35it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.22it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.10it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.01it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.03it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.97it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.33it/s]\u001b[A\n",
            "{'eval_loss': 1.1967525482177734, 'eval_accuracy': 0.5604395866394043, 'eval_f1': 0.5601503759398496, 'eval_runtime': 3.1137, 'eval_samples_per_second': 87.677, 'eval_steps_per_second': 11.241, 'epoch': 2.0}\n",
            "\n",
            "  8% 208/2600 [02:17<26:12,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:12:02,287 >> Saving model checkpoint to models/OneShot/1/checkpoint-208\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:02,288 >> Configuration saved in models/OneShot/1/checkpoint-208/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:02,372 >> Module weights saved in models/OneShot/1/checkpoint-208/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:02,373 >> Configuration saved in models/OneShot/1/checkpoint-208/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:02,381 >> Module weights saved in models/OneShot/1/checkpoint-208/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:02,381 >> Configuration saved in models/OneShot/1/checkpoint-208/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:02,438 >> Module weights saved in models/OneShot/1/checkpoint-208/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:02,438 >> Configuration saved in models/OneShot/1/checkpoint-208/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:02,499 >> Module weights saved in models/OneShot/1/checkpoint-208/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:02,499 >> Configuration saved in models/OneShot/1/checkpoint-208/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:03,607 >> Module weights saved in models/OneShot/1/checkpoint-208/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:03,705 >> Configuration saved in models/OneShot/1/checkpoint-208/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:03,724 >> Module weights saved in models/OneShot/1/checkpoint-208/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:12:03,724 >> Configuration saved in models/OneShot/1/checkpoint-208/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:12:05,163 >> Module weights saved in models/OneShot/1/checkpoint-208/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:12:05,163 >> tokenizer config file saved in models/OneShot/1/checkpoint-208/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:12:05,164 >> Special tokens file saved in models/OneShot/1/checkpoint-208/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:12:05,629 >> Deleting older checkpoint [models/OneShot/1/checkpoint-104] due to args.save_total_limit\n",
            " 12% 312/2600 [03:30<24:57,  1.53it/s][INFO|trainer.py:623] 2022-08-25 05:13:15,412 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:13:15,414 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:13:15,414 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:13:15,414 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.22it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.18it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.12it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.72it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.37it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.30it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.20it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.14it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.14it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.14it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.13it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.10it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.08it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.05it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.56it/s]\u001b[A\n",
            "{'eval_loss': 1.419012188911438, 'eval_accuracy': 0.5604395866394043, 'eval_f1': 0.5578231292517007, 'eval_runtime': 3.0929, 'eval_samples_per_second': 88.267, 'eval_steps_per_second': 11.316, 'epoch': 3.0}\n",
            "\n",
            " 12% 312/2600 [03:33<24:57,  1.53it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:13:18,508 >> Saving model checkpoint to models/OneShot/1/checkpoint-312\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:18,509 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:18,596 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:18,596 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:18,604 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:18,604 >> Configuration saved in models/OneShot/1/checkpoint-312/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:18,657 >> Module weights saved in models/OneShot/1/checkpoint-312/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:18,658 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:18,718 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:18,719 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:20,149 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:20,187 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:20,208 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:13:20,209 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:13:21,683 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:13:21,684 >> tokenizer config file saved in models/OneShot/1/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:13:21,684 >> Special tokens file saved in models/OneShot/1/checkpoint-312/special_tokens_map.json\n",
            " 16% 416/2600 [04:47<24:06,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:14:31,996 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:14:31,997 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:14:31,998 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:14:31,998 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.05it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.20it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.11it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.66it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.33it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.26it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.19it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.12it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.12it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.04it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.99it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.94it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.99it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.99it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.92it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.29it/s]\u001b[A\n",
            "{'eval_loss': 1.755499005317688, 'eval_accuracy': 0.5860806107521057, 'eval_f1': 0.5532561945172549, 'eval_runtime': 3.1239, 'eval_samples_per_second': 87.392, 'eval_steps_per_second': 11.204, 'epoch': 4.0}\n",
            "\n",
            " 16% 416/2600 [04:50<24:06,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:14:35,123 >> Saving model checkpoint to models/OneShot/1/checkpoint-416\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:35,123 >> Configuration saved in models/OneShot/1/checkpoint-416/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:35,211 >> Module weights saved in models/OneShot/1/checkpoint-416/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:35,211 >> Configuration saved in models/OneShot/1/checkpoint-416/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:35,219 >> Module weights saved in models/OneShot/1/checkpoint-416/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:35,220 >> Configuration saved in models/OneShot/1/checkpoint-416/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:35,272 >> Module weights saved in models/OneShot/1/checkpoint-416/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:35,272 >> Configuration saved in models/OneShot/1/checkpoint-416/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:35,330 >> Module weights saved in models/OneShot/1/checkpoint-416/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:35,331 >> Configuration saved in models/OneShot/1/checkpoint-416/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:36,796 >> Module weights saved in models/OneShot/1/checkpoint-416/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:36,825 >> Configuration saved in models/OneShot/1/checkpoint-416/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:36,848 >> Module weights saved in models/OneShot/1/checkpoint-416/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:14:36,849 >> Configuration saved in models/OneShot/1/checkpoint-416/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:14:38,299 >> Module weights saved in models/OneShot/1/checkpoint-416/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:14:38,300 >> tokenizer config file saved in models/OneShot/1/checkpoint-416/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:14:38,300 >> Special tokens file saved in models/OneShot/1/checkpoint-416/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:14:38,761 >> Deleting older checkpoint [models/OneShot/1/checkpoint-312] due to args.save_total_limit\n",
            "{'loss': 0.2267, 'learning_rate': 8.076923076923078e-05, 'epoch': 4.81}\n",
            " 20% 520/2600 [06:03<22:54,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:15:48,333 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:15:48,335 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:15:48,335 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:15:48,335 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.13it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.14it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.03it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.31it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.27it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.13it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.11it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.03it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.97it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.01it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.93it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.89it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.31it/s]\u001b[A\n",
            "{'eval_loss': 2.3144021034240723, 'eval_accuracy': 0.5750916004180908, 'eval_f1': 0.5671405139420449, 'eval_runtime': 3.1291, 'eval_samples_per_second': 87.245, 'eval_steps_per_second': 11.185, 'epoch': 5.0}\n",
            "\n",
            " 20% 520/2600 [06:06<22:54,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:15:51,465 >> Saving model checkpoint to models/OneShot/1/checkpoint-520\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:51,466 >> Configuration saved in models/OneShot/1/checkpoint-520/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:51,560 >> Module weights saved in models/OneShot/1/checkpoint-520/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:51,561 >> Configuration saved in models/OneShot/1/checkpoint-520/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:51,569 >> Module weights saved in models/OneShot/1/checkpoint-520/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:51,570 >> Configuration saved in models/OneShot/1/checkpoint-520/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:51,626 >> Module weights saved in models/OneShot/1/checkpoint-520/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:51,627 >> Configuration saved in models/OneShot/1/checkpoint-520/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:51,687 >> Module weights saved in models/OneShot/1/checkpoint-520/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:51,687 >> Configuration saved in models/OneShot/1/checkpoint-520/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:53,152 >> Module weights saved in models/OneShot/1/checkpoint-520/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:53,152 >> Configuration saved in models/OneShot/1/checkpoint-520/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:53,163 >> Module weights saved in models/OneShot/1/checkpoint-520/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:15:53,163 >> Configuration saved in models/OneShot/1/checkpoint-520/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:15:54,652 >> Module weights saved in models/OneShot/1/checkpoint-520/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:15:54,657 >> tokenizer config file saved in models/OneShot/1/checkpoint-520/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:15:54,658 >> Special tokens file saved in models/OneShot/1/checkpoint-520/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:15:55,134 >> Deleting older checkpoint [models/OneShot/1/checkpoint-208] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:15:55,201 >> Deleting older checkpoint [models/OneShot/1/checkpoint-416] due to args.save_total_limit\n",
            " 24% 624/2600 [07:20<21:46,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:17:04,975 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:17:04,976 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:17:04,977 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:17:04,977 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.35it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.30it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.11it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.68it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.24it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.26it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.11it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.06it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.93it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.92it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.87it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.85it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.27it/s]\u001b[A\n",
            "{'eval_loss': 2.1842501163482666, 'eval_accuracy': 0.5641025900840759, 'eval_f1': 0.5524775116058023, 'eval_runtime': 3.1338, 'eval_samples_per_second': 87.114, 'eval_steps_per_second': 11.169, 'epoch': 6.0}\n",
            "\n",
            " 24% 624/2600 [07:23<21:46,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:17:08,112 >> Saving model checkpoint to models/OneShot/1/checkpoint-624\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:08,113 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:08,199 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:08,199 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:08,207 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:08,207 >> Configuration saved in models/OneShot/1/checkpoint-624/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:08,259 >> Module weights saved in models/OneShot/1/checkpoint-624/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:08,259 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:08,314 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:08,315 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:09,679 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:09,680 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:09,690 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:17:09,690 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:17:11,217 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:17:11,218 >> tokenizer config file saved in models/OneShot/1/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:17:11,218 >> Special tokens file saved in models/OneShot/1/checkpoint-624/special_tokens_map.json\n",
            " 28% 728/2600 [08:36<20:44,  1.50it/s][INFO|trainer.py:623] 2022-08-25 05:18:21,698 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:18:21,700 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:18:21,700 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:18:21,700 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.30it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.35it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.16it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.69it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.28it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.21it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.01it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.00it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.88it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.89it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.90it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.87it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.89it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.86it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.08it/s]\u001b[A\n",
            "{'eval_loss': 2.4422366619110107, 'eval_accuracy': 0.5567765831947327, 'eval_f1': 0.510426392779334, 'eval_runtime': 3.147, 'eval_samples_per_second': 86.749, 'eval_steps_per_second': 11.122, 'epoch': 7.0}\n",
            "\n",
            " 28% 728/2600 [08:39<20:44,  1.50it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:18:24,849 >> Saving model checkpoint to models/OneShot/1/checkpoint-728\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:24,849 >> Configuration saved in models/OneShot/1/checkpoint-728/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:24,993 >> Module weights saved in models/OneShot/1/checkpoint-728/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:24,993 >> Configuration saved in models/OneShot/1/checkpoint-728/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:25,006 >> Module weights saved in models/OneShot/1/checkpoint-728/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:25,007 >> Configuration saved in models/OneShot/1/checkpoint-728/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:25,089 >> Module weights saved in models/OneShot/1/checkpoint-728/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:25,090 >> Configuration saved in models/OneShot/1/checkpoint-728/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:25,173 >> Module weights saved in models/OneShot/1/checkpoint-728/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:25,174 >> Configuration saved in models/OneShot/1/checkpoint-728/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:26,774 >> Module weights saved in models/OneShot/1/checkpoint-728/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:26,887 >> Configuration saved in models/OneShot/1/checkpoint-728/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:26,911 >> Module weights saved in models/OneShot/1/checkpoint-728/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:18:26,912 >> Configuration saved in models/OneShot/1/checkpoint-728/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:18:28,405 >> Module weights saved in models/OneShot/1/checkpoint-728/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:18:28,405 >> tokenizer config file saved in models/OneShot/1/checkpoint-728/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:18:28,406 >> Special tokens file saved in models/OneShot/1/checkpoint-728/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:18:28,886 >> Deleting older checkpoint [models/OneShot/1/checkpoint-624] due to args.save_total_limit\n",
            " 32% 832/2600 [09:53<19:33,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:19:38,855 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:19:38,857 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:19:38,857 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:19:38,857 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.10it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.35it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.16it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.72it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.42it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.28it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.08it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.02it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.01it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.03it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.02it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.05it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.09it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.06it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.04it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.4732956886291504, 'eval_accuracy': 0.5641025900840759, 'eval_f1': 0.5638919093069148, 'eval_runtime': 3.1076, 'eval_samples_per_second': 87.848, 'eval_steps_per_second': 11.263, 'epoch': 8.0}\n",
            " 32% 832/2600 [09:57<19:33,  1.51it/s]\n",
            "100% 35/35 [00:03<00:00, 12.44it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:19:41,966 >> Saving model checkpoint to models/OneShot/1/checkpoint-832\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:41,967 >> Configuration saved in models/OneShot/1/checkpoint-832/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:42,053 >> Module weights saved in models/OneShot/1/checkpoint-832/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:42,053 >> Configuration saved in models/OneShot/1/checkpoint-832/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:42,060 >> Module weights saved in models/OneShot/1/checkpoint-832/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:42,061 >> Configuration saved in models/OneShot/1/checkpoint-832/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:42,112 >> Module weights saved in models/OneShot/1/checkpoint-832/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:42,112 >> Configuration saved in models/OneShot/1/checkpoint-832/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:42,172 >> Module weights saved in models/OneShot/1/checkpoint-832/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:42,173 >> Configuration saved in models/OneShot/1/checkpoint-832/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:43,566 >> Module weights saved in models/OneShot/1/checkpoint-832/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:43,720 >> Configuration saved in models/OneShot/1/checkpoint-832/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:43,775 >> Module weights saved in models/OneShot/1/checkpoint-832/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:19:43,776 >> Configuration saved in models/OneShot/1/checkpoint-832/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:19:45,096 >> Module weights saved in models/OneShot/1/checkpoint-832/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:19:45,097 >> tokenizer config file saved in models/OneShot/1/checkpoint-832/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:19:45,097 >> Special tokens file saved in models/OneShot/1/checkpoint-832/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:19:45,578 >> Deleting older checkpoint [models/OneShot/1/checkpoint-728] due to args.save_total_limit\n",
            " 36% 936/2600 [11:10<18:25,  1.50it/s][INFO|trainer.py:623] 2022-08-25 05:20:55,427 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:20:55,429 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:20:55,429 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:20:55,429 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.23it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.32it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.14it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.72it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.35it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.32it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.20it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.07it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.05it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.95it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.97it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.02it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.97it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.32it/s]\u001b[A\n",
            "{'eval_loss': 2.6003825664520264, 'eval_accuracy': 0.5641025900840759, 'eval_f1': 0.5638919093069148, 'eval_runtime': 3.1219, 'eval_samples_per_second': 87.446, 'eval_steps_per_second': 11.211, 'epoch': 9.0}\n",
            "\n",
            " 36% 936/2600 [11:13<18:25,  1.50it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:20:58,552 >> Saving model checkpoint to models/OneShot/1/checkpoint-936\n",
            "[INFO|loading.py:60] 2022-08-25 05:20:58,553 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:20:58,641 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:20:58,642 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:20:58,649 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:20:58,650 >> Configuration saved in models/OneShot/1/checkpoint-936/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:20:58,702 >> Module weights saved in models/OneShot/1/checkpoint-936/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:20:58,703 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:20:58,764 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:20:58,765 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:21:00,225 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:21:00,226 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:21:00,237 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:21:00,237 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:21:01,639 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:21:01,639 >> tokenizer config file saved in models/OneShot/1/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:21:01,639 >> Special tokens file saved in models/OneShot/1/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:21:02,159 >> Deleting older checkpoint [models/OneShot/1/checkpoint-832] due to args.save_total_limit\n",
            "{'loss': 0.0327, 'learning_rate': 6.153846153846155e-05, 'epoch': 9.62}\n",
            " 40% 1040/2600 [12:26<17:12,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:22:11,881 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:22:11,883 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:22:11,883 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:22:11,883 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.06it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.30it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.07it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.22it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.17it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.04it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 10.97it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 10.94it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.91it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.97it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.93it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.88it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.22it/s]\u001b[A\n",
            "{'eval_loss': 2.4091434478759766, 'eval_accuracy': 0.5970696210861206, 'eval_f1': 0.5800234951890804, 'eval_runtime': 3.1434, 'eval_samples_per_second': 86.849, 'eval_steps_per_second': 11.134, 'epoch': 10.0}\n",
            "\n",
            " 40% 1040/2600 [12:30<17:12,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:22:15,028 >> Saving model checkpoint to models/OneShot/1/checkpoint-1040\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:15,028 >> Configuration saved in models/OneShot/1/checkpoint-1040/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:15,116 >> Module weights saved in models/OneShot/1/checkpoint-1040/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:15,116 >> Configuration saved in models/OneShot/1/checkpoint-1040/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:15,124 >> Module weights saved in models/OneShot/1/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:15,124 >> Configuration saved in models/OneShot/1/checkpoint-1040/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:15,174 >> Module weights saved in models/OneShot/1/checkpoint-1040/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:15,174 >> Configuration saved in models/OneShot/1/checkpoint-1040/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:15,247 >> Module weights saved in models/OneShot/1/checkpoint-1040/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:15,247 >> Configuration saved in models/OneShot/1/checkpoint-1040/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:16,644 >> Module weights saved in models/OneShot/1/checkpoint-1040/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:16,739 >> Configuration saved in models/OneShot/1/checkpoint-1040/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:16,755 >> Module weights saved in models/OneShot/1/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:22:16,755 >> Configuration saved in models/OneShot/1/checkpoint-1040/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:22:18,139 >> Module weights saved in models/OneShot/1/checkpoint-1040/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:22:18,140 >> tokenizer config file saved in models/OneShot/1/checkpoint-1040/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:22:18,140 >> Special tokens file saved in models/OneShot/1/checkpoint-1040/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:22:18,609 >> Deleting older checkpoint [models/OneShot/1/checkpoint-520] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:22:18,647 >> Deleting older checkpoint [models/OneShot/1/checkpoint-936] due to args.save_total_limit\n",
            " 44% 1144/2600 [13:43<16:04,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:23:28,416 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:23:28,417 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:23:28,417 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:23:28,417 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.62it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.31it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.08it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.69it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.30it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.22it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.16it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.11it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.04it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.96it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.96it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.01it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.97it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.89it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.37it/s]\u001b[A\n",
            "{'eval_loss': 3.18444561958313, 'eval_accuracy': 0.5128205418586731, 'eval_f1': 0.5127159019231543, 'eval_runtime': 3.1236, 'eval_samples_per_second': 87.4, 'eval_steps_per_second': 11.205, 'epoch': 11.0}\n",
            "\n",
            " 44% 1144/2600 [13:46<16:04,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:23:31,542 >> Saving model checkpoint to models/OneShot/1/checkpoint-1144\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:31,543 >> Configuration saved in models/OneShot/1/checkpoint-1144/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:31,629 >> Module weights saved in models/OneShot/1/checkpoint-1144/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:31,630 >> Configuration saved in models/OneShot/1/checkpoint-1144/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:31,637 >> Module weights saved in models/OneShot/1/checkpoint-1144/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:31,637 >> Configuration saved in models/OneShot/1/checkpoint-1144/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:31,688 >> Module weights saved in models/OneShot/1/checkpoint-1144/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:31,688 >> Configuration saved in models/OneShot/1/checkpoint-1144/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:31,758 >> Module weights saved in models/OneShot/1/checkpoint-1144/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:31,759 >> Configuration saved in models/OneShot/1/checkpoint-1144/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:33,178 >> Module weights saved in models/OneShot/1/checkpoint-1144/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:33,325 >> Configuration saved in models/OneShot/1/checkpoint-1144/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:33,350 >> Module weights saved in models/OneShot/1/checkpoint-1144/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:23:33,350 >> Configuration saved in models/OneShot/1/checkpoint-1144/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:23:34,722 >> Module weights saved in models/OneShot/1/checkpoint-1144/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:23:34,722 >> tokenizer config file saved in models/OneShot/1/checkpoint-1144/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:23:34,723 >> Special tokens file saved in models/OneShot/1/checkpoint-1144/special_tokens_map.json\n",
            " 48% 1248/2600 [14:59<14:54,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:24:44,797 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:24:44,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:24:44,799 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:24:44,799 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:02, 16.00it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.24it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.06it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.66it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.35it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.30it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.20it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.12it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.02it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.95it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.98it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.93it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.126188039779663, 'eval_accuracy': 0.5604395866394043, 'eval_f1': 0.5587284482758621, 'eval_runtime': 3.1245, 'eval_samples_per_second': 87.374, 'eval_steps_per_second': 11.202, 'epoch': 12.0}\n",
            " 48% 1248/2600 [15:03<14:54,  1.51it/s]\n",
            "100% 35/35 [00:03<00:00, 12.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:24:47,925 >> Saving model checkpoint to models/OneShot/1/checkpoint-1248\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:47,926 >> Configuration saved in models/OneShot/1/checkpoint-1248/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:48,037 >> Module weights saved in models/OneShot/1/checkpoint-1248/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:48,038 >> Configuration saved in models/OneShot/1/checkpoint-1248/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:48,045 >> Module weights saved in models/OneShot/1/checkpoint-1248/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:48,046 >> Configuration saved in models/OneShot/1/checkpoint-1248/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:48,097 >> Module weights saved in models/OneShot/1/checkpoint-1248/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:48,099 >> Configuration saved in models/OneShot/1/checkpoint-1248/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:48,211 >> Module weights saved in models/OneShot/1/checkpoint-1248/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:48,211 >> Configuration saved in models/OneShot/1/checkpoint-1248/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:49,620 >> Module weights saved in models/OneShot/1/checkpoint-1248/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:49,716 >> Configuration saved in models/OneShot/1/checkpoint-1248/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:49,734 >> Module weights saved in models/OneShot/1/checkpoint-1248/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:24:49,735 >> Configuration saved in models/OneShot/1/checkpoint-1248/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:24:51,213 >> Module weights saved in models/OneShot/1/checkpoint-1248/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:24:51,214 >> tokenizer config file saved in models/OneShot/1/checkpoint-1248/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:24:51,214 >> Special tokens file saved in models/OneShot/1/checkpoint-1248/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:24:51,677 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1144] due to args.save_total_limit\n",
            " 52% 1352/2600 [16:16<13:39,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:26:01,495 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:26:01,497 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:26:01,497 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:26:01,497 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.50it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.43it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.24it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.81it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.43it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.30it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.18it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.11it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.04it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.00it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.00it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.99it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.97it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.92it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.36it/s]\u001b[A\n",
            "{'eval_loss': 2.8586912155151367, 'eval_accuracy': 0.5714285969734192, 'eval_f1': 0.5599988979653685, 'eval_runtime': 3.1145, 'eval_samples_per_second': 87.655, 'eval_steps_per_second': 11.238, 'epoch': 13.0}\n",
            "\n",
            " 52% 1352/2600 [16:19<13:39,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:26:04,613 >> Saving model checkpoint to models/OneShot/1/checkpoint-1352\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:04,613 >> Configuration saved in models/OneShot/1/checkpoint-1352/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:04,700 >> Module weights saved in models/OneShot/1/checkpoint-1352/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:04,700 >> Configuration saved in models/OneShot/1/checkpoint-1352/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:04,707 >> Module weights saved in models/OneShot/1/checkpoint-1352/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:04,708 >> Configuration saved in models/OneShot/1/checkpoint-1352/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:04,758 >> Module weights saved in models/OneShot/1/checkpoint-1352/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:04,759 >> Configuration saved in models/OneShot/1/checkpoint-1352/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:04,827 >> Module weights saved in models/OneShot/1/checkpoint-1352/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:04,827 >> Configuration saved in models/OneShot/1/checkpoint-1352/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:06,321 >> Module weights saved in models/OneShot/1/checkpoint-1352/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:06,321 >> Configuration saved in models/OneShot/1/checkpoint-1352/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:06,331 >> Module weights saved in models/OneShot/1/checkpoint-1352/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:26:06,332 >> Configuration saved in models/OneShot/1/checkpoint-1352/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:26:07,778 >> Module weights saved in models/OneShot/1/checkpoint-1352/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:26:07,778 >> tokenizer config file saved in models/OneShot/1/checkpoint-1352/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:26:07,778 >> Special tokens file saved in models/OneShot/1/checkpoint-1352/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:26:08,239 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1248] due to args.save_total_limit\n",
            " 56% 1456/2600 [17:33<12:33,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:27:18,261 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:27:18,263 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:27:18,263 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:27:18,263 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.50it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.34it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.21it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.74it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.46it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.36it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.24it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.16it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.12it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.07it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.04it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.03it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.08it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.01it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.25it/s]\u001b[A\n",
            "{'eval_loss': 3.024780035018921, 'eval_accuracy': 0.5787546038627625, 'eval_f1': 0.5764815389388769, 'eval_runtime': 3.1111, 'eval_samples_per_second': 87.75, 'eval_steps_per_second': 11.25, 'epoch': 14.0}\n",
            "\n",
            " 56% 1456/2600 [17:36<12:33,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:27:21,375 >> Saving model checkpoint to models/OneShot/1/checkpoint-1456\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:21,376 >> Configuration saved in models/OneShot/1/checkpoint-1456/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:21,465 >> Module weights saved in models/OneShot/1/checkpoint-1456/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:21,466 >> Configuration saved in models/OneShot/1/checkpoint-1456/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:21,473 >> Module weights saved in models/OneShot/1/checkpoint-1456/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:21,474 >> Configuration saved in models/OneShot/1/checkpoint-1456/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:21,526 >> Module weights saved in models/OneShot/1/checkpoint-1456/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:21,527 >> Configuration saved in models/OneShot/1/checkpoint-1456/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:21,596 >> Module weights saved in models/OneShot/1/checkpoint-1456/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:21,596 >> Configuration saved in models/OneShot/1/checkpoint-1456/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:23,107 >> Module weights saved in models/OneShot/1/checkpoint-1456/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:23,123 >> Configuration saved in models/OneShot/1/checkpoint-1456/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:23,136 >> Module weights saved in models/OneShot/1/checkpoint-1456/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:27:23,137 >> Configuration saved in models/OneShot/1/checkpoint-1456/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:27:24,470 >> Module weights saved in models/OneShot/1/checkpoint-1456/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:27:24,470 >> tokenizer config file saved in models/OneShot/1/checkpoint-1456/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:27:24,471 >> Special tokens file saved in models/OneShot/1/checkpoint-1456/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:27:24,931 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1352] due to args.save_total_limit\n",
            "{'loss': 0.0128, 'learning_rate': 4.230769230769231e-05, 'epoch': 14.42}\n",
            " 60% 1560/2600 [18:49<11:29,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:28:34,441 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:28:34,443 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:28:34,443 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:28:34,443 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:02, 15.76it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.07it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.02it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.24it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.09it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.06it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.02it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.02it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.93it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.98it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.94it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.24it/s]\u001b[A\n",
            "{'eval_loss': 3.442256212234497, 'eval_accuracy': 0.5311355590820312, 'eval_f1': 0.5303730781636382, 'eval_runtime': 3.1416, 'eval_samples_per_second': 86.899, 'eval_steps_per_second': 11.141, 'epoch': 15.0}\n",
            "\n",
            " 60% 1560/2600 [18:52<11:29,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:28:37,586 >> Saving model checkpoint to models/OneShot/1/checkpoint-1560\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:37,587 >> Configuration saved in models/OneShot/1/checkpoint-1560/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:37,674 >> Module weights saved in models/OneShot/1/checkpoint-1560/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:37,674 >> Configuration saved in models/OneShot/1/checkpoint-1560/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:37,683 >> Module weights saved in models/OneShot/1/checkpoint-1560/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:37,684 >> Configuration saved in models/OneShot/1/checkpoint-1560/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:37,733 >> Module weights saved in models/OneShot/1/checkpoint-1560/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:37,734 >> Configuration saved in models/OneShot/1/checkpoint-1560/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:37,798 >> Module weights saved in models/OneShot/1/checkpoint-1560/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:37,799 >> Configuration saved in models/OneShot/1/checkpoint-1560/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:39,249 >> Module weights saved in models/OneShot/1/checkpoint-1560/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:39,308 >> Configuration saved in models/OneShot/1/checkpoint-1560/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:39,327 >> Module weights saved in models/OneShot/1/checkpoint-1560/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:28:39,327 >> Configuration saved in models/OneShot/1/checkpoint-1560/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:28:40,845 >> Module weights saved in models/OneShot/1/checkpoint-1560/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:28:40,846 >> tokenizer config file saved in models/OneShot/1/checkpoint-1560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:28:40,846 >> Special tokens file saved in models/OneShot/1/checkpoint-1560/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:28:41,299 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1456] due to args.save_total_limit\n",
            " 64% 1664/2600 [20:06<10:15,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:29:51,075 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:29:51,077 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:29:51,077 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:29:51,077 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.17it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.16it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.72it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.34it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.24it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.12it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.08it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.07it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.97it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.02it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.01it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.02it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.46it/s]\u001b[A\n",
            "{'eval_loss': 3.4248058795928955, 'eval_accuracy': 0.5421245694160461, 'eval_f1': 0.5299133525271031, 'eval_runtime': 3.1138, 'eval_samples_per_second': 87.675, 'eval_steps_per_second': 11.24, 'epoch': 16.0}\n",
            "\n",
            " 64% 1664/2600 [20:09<10:15,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:29:54,192 >> Saving model checkpoint to models/OneShot/1/checkpoint-1664\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:54,192 >> Configuration saved in models/OneShot/1/checkpoint-1664/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:54,279 >> Module weights saved in models/OneShot/1/checkpoint-1664/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:54,279 >> Configuration saved in models/OneShot/1/checkpoint-1664/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:54,286 >> Module weights saved in models/OneShot/1/checkpoint-1664/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:54,287 >> Configuration saved in models/OneShot/1/checkpoint-1664/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:54,339 >> Module weights saved in models/OneShot/1/checkpoint-1664/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:54,340 >> Configuration saved in models/OneShot/1/checkpoint-1664/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:54,407 >> Module weights saved in models/OneShot/1/checkpoint-1664/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:54,407 >> Configuration saved in models/OneShot/1/checkpoint-1664/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:55,860 >> Module weights saved in models/OneShot/1/checkpoint-1664/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:55,985 >> Configuration saved in models/OneShot/1/checkpoint-1664/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:56,000 >> Module weights saved in models/OneShot/1/checkpoint-1664/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:29:56,000 >> Configuration saved in models/OneShot/1/checkpoint-1664/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:29:57,432 >> Module weights saved in models/OneShot/1/checkpoint-1664/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:29:57,432 >> tokenizer config file saved in models/OneShot/1/checkpoint-1664/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:29:57,433 >> Special tokens file saved in models/OneShot/1/checkpoint-1664/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:29:57,914 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1560] due to args.save_total_limit\n",
            " 68% 1768/2600 [21:22<09:06,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:31:07,797 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:31:07,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:31:07,799 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:31:07,799 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.40it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.40it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.13it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.75it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.42it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.40it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.29it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.22it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.16it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.13it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.10it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.07it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.07it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.01it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.03it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.03it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.47it/s]\u001b[A\n",
            "{'eval_loss': 3.6353278160095215, 'eval_accuracy': 0.5457875728607178, 'eval_f1': 0.5444121447028424, 'eval_runtime': 3.0955, 'eval_samples_per_second': 88.194, 'eval_steps_per_second': 11.307, 'epoch': 17.0}\n",
            "\n",
            " 68% 1768/2600 [21:25<09:06,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:31:10,895 >> Saving model checkpoint to models/OneShot/1/checkpoint-1768\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:10,896 >> Configuration saved in models/OneShot/1/checkpoint-1768/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:10,983 >> Module weights saved in models/OneShot/1/checkpoint-1768/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:10,983 >> Configuration saved in models/OneShot/1/checkpoint-1768/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:10,991 >> Module weights saved in models/OneShot/1/checkpoint-1768/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:10,991 >> Configuration saved in models/OneShot/1/checkpoint-1768/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:11,044 >> Module weights saved in models/OneShot/1/checkpoint-1768/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:11,045 >> Configuration saved in models/OneShot/1/checkpoint-1768/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:11,112 >> Module weights saved in models/OneShot/1/checkpoint-1768/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:11,112 >> Configuration saved in models/OneShot/1/checkpoint-1768/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:12,668 >> Module weights saved in models/OneShot/1/checkpoint-1768/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:12,669 >> Configuration saved in models/OneShot/1/checkpoint-1768/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:12,684 >> Module weights saved in models/OneShot/1/checkpoint-1768/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:31:12,685 >> Configuration saved in models/OneShot/1/checkpoint-1768/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:31:14,085 >> Module weights saved in models/OneShot/1/checkpoint-1768/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:31:14,086 >> tokenizer config file saved in models/OneShot/1/checkpoint-1768/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:31:14,086 >> Special tokens file saved in models/OneShot/1/checkpoint-1768/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:31:14,567 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1664] due to args.save_total_limit\n",
            " 72% 1872/2600 [22:39<07:58,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:32:24,250 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:32:24,251 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:32:24,252 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:32:24,252 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.50it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.30it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.27it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.85it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.46it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.33it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.19it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.13it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.05it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.03it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.99it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.93it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.32it/s]\u001b[A\n",
            "{'eval_loss': 3.683316707611084, 'eval_accuracy': 0.5750916004180908, 'eval_f1': 0.5478583666476299, 'eval_runtime': 3.1126, 'eval_samples_per_second': 87.709, 'eval_steps_per_second': 11.245, 'epoch': 18.0}\n",
            "\n",
            " 72% 1872/2600 [22:42<07:58,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:32:27,365 >> Saving model checkpoint to models/OneShot/1/checkpoint-1872\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:27,366 >> Configuration saved in models/OneShot/1/checkpoint-1872/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:27,456 >> Module weights saved in models/OneShot/1/checkpoint-1872/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:27,456 >> Configuration saved in models/OneShot/1/checkpoint-1872/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:27,463 >> Module weights saved in models/OneShot/1/checkpoint-1872/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:27,464 >> Configuration saved in models/OneShot/1/checkpoint-1872/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:27,523 >> Module weights saved in models/OneShot/1/checkpoint-1872/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:27,523 >> Configuration saved in models/OneShot/1/checkpoint-1872/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:27,599 >> Module weights saved in models/OneShot/1/checkpoint-1872/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:27,599 >> Configuration saved in models/OneShot/1/checkpoint-1872/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:29,086 >> Module weights saved in models/OneShot/1/checkpoint-1872/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:29,119 >> Configuration saved in models/OneShot/1/checkpoint-1872/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:29,134 >> Module weights saved in models/OneShot/1/checkpoint-1872/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:32:29,135 >> Configuration saved in models/OneShot/1/checkpoint-1872/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:32:30,597 >> Module weights saved in models/OneShot/1/checkpoint-1872/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:32:30,597 >> tokenizer config file saved in models/OneShot/1/checkpoint-1872/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:32:30,598 >> Special tokens file saved in models/OneShot/1/checkpoint-1872/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:32:31,031 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1768] due to args.save_total_limit\n",
            " 76% 1976/2600 [23:56<06:45,  1.54it/s][INFO|trainer.py:623] 2022-08-25 05:33:40,987 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:33:40,989 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:33:40,989 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:33:40,989 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.73it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.49it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.24it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.73it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.46it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.42it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.33it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.25it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.13it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.11it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.09it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.12it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.14it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.03it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.03it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.07it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.50it/s]\u001b[A\n",
            "{'eval_loss': 3.7134690284729004, 'eval_accuracy': 0.5567765831947327, 'eval_f1': 0.552064546748932, 'eval_runtime': 3.0874, 'eval_samples_per_second': 88.423, 'eval_steps_per_second': 11.336, 'epoch': 19.0}\n",
            "\n",
            " 76% 1976/2600 [23:59<06:45,  1.54it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:33:44,078 >> Saving model checkpoint to models/OneShot/1/checkpoint-1976\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:44,079 >> Configuration saved in models/OneShot/1/checkpoint-1976/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:44,169 >> Module weights saved in models/OneShot/1/checkpoint-1976/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:44,169 >> Configuration saved in models/OneShot/1/checkpoint-1976/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:44,176 >> Module weights saved in models/OneShot/1/checkpoint-1976/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:44,177 >> Configuration saved in models/OneShot/1/checkpoint-1976/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:44,230 >> Module weights saved in models/OneShot/1/checkpoint-1976/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:44,230 >> Configuration saved in models/OneShot/1/checkpoint-1976/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:44,297 >> Module weights saved in models/OneShot/1/checkpoint-1976/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:44,298 >> Configuration saved in models/OneShot/1/checkpoint-1976/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:45,743 >> Module weights saved in models/OneShot/1/checkpoint-1976/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:45,878 >> Configuration saved in models/OneShot/1/checkpoint-1976/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:45,893 >> Module weights saved in models/OneShot/1/checkpoint-1976/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:33:45,893 >> Configuration saved in models/OneShot/1/checkpoint-1976/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:33:47,307 >> Module weights saved in models/OneShot/1/checkpoint-1976/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:33:47,307 >> tokenizer config file saved in models/OneShot/1/checkpoint-1976/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:33:47,308 >> Special tokens file saved in models/OneShot/1/checkpoint-1976/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:33:47,769 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1872] due to args.save_total_limit\n",
            "{'loss': 0.0028, 'learning_rate': 2.307692307692308e-05, 'epoch': 19.23}\n",
            " 80% 2080/2600 [25:12<05:42,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:34:57,650 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:34:57,652 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:34:57,652 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:34:57,652 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.19it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.39it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.09it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.73it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.44it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.34it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.18it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.06it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 10.99it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.97it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.01it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.93it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.90it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.94it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.8320724964141846, 'eval_accuracy': 0.5567765831947327, 'eval_f1': 0.5387290017175652, 'eval_runtime': 3.1189, 'eval_samples_per_second': 87.531, 'eval_steps_per_second': 11.222, 'epoch': 20.0}\n",
            " 80% 2080/2600 [25:15<05:42,  1.52it/s]\n",
            "100% 35/35 [00:03<00:00, 12.43it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:35:00,772 >> Saving model checkpoint to models/OneShot/1/checkpoint-2080\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:00,773 >> Configuration saved in models/OneShot/1/checkpoint-2080/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:00,863 >> Module weights saved in models/OneShot/1/checkpoint-2080/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:00,863 >> Configuration saved in models/OneShot/1/checkpoint-2080/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:00,870 >> Module weights saved in models/OneShot/1/checkpoint-2080/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:00,871 >> Configuration saved in models/OneShot/1/checkpoint-2080/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:00,922 >> Module weights saved in models/OneShot/1/checkpoint-2080/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:00,923 >> Configuration saved in models/OneShot/1/checkpoint-2080/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:00,989 >> Module weights saved in models/OneShot/1/checkpoint-2080/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:00,989 >> Configuration saved in models/OneShot/1/checkpoint-2080/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:02,463 >> Module weights saved in models/OneShot/1/checkpoint-2080/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:02,463 >> Configuration saved in models/OneShot/1/checkpoint-2080/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:02,474 >> Module weights saved in models/OneShot/1/checkpoint-2080/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:35:02,474 >> Configuration saved in models/OneShot/1/checkpoint-2080/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:35:04,031 >> Module weights saved in models/OneShot/1/checkpoint-2080/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:35:04,031 >> tokenizer config file saved in models/OneShot/1/checkpoint-2080/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:35:04,032 >> Special tokens file saved in models/OneShot/1/checkpoint-2080/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:35:04,508 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1976] due to args.save_total_limit\n",
            " 84% 2184/2600 [26:29<04:35,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:36:14,112 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:36:14,114 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:36:14,114 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:36:14,114 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.16it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.32it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.17it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.74it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.43it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.32it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.21it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.10it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.05it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.00it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.99it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.39it/s]\u001b[A\n",
            "{'eval_loss': 3.9027395248413086, 'eval_accuracy': 0.5384615659713745, 'eval_f1': 0.5316176470588235, 'eval_runtime': 3.1157, 'eval_samples_per_second': 87.62, 'eval_steps_per_second': 11.233, 'epoch': 21.0}\n",
            "\n",
            " 84% 2184/2600 [26:32<04:35,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:36:17,231 >> Saving model checkpoint to models/OneShot/1/checkpoint-2184\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:17,231 >> Configuration saved in models/OneShot/1/checkpoint-2184/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:17,319 >> Module weights saved in models/OneShot/1/checkpoint-2184/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:17,319 >> Configuration saved in models/OneShot/1/checkpoint-2184/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:17,327 >> Module weights saved in models/OneShot/1/checkpoint-2184/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:17,327 >> Configuration saved in models/OneShot/1/checkpoint-2184/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:17,384 >> Module weights saved in models/OneShot/1/checkpoint-2184/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:17,384 >> Configuration saved in models/OneShot/1/checkpoint-2184/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:17,451 >> Module weights saved in models/OneShot/1/checkpoint-2184/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:17,452 >> Configuration saved in models/OneShot/1/checkpoint-2184/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:18,902 >> Module weights saved in models/OneShot/1/checkpoint-2184/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:19,059 >> Configuration saved in models/OneShot/1/checkpoint-2184/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:19,086 >> Module weights saved in models/OneShot/1/checkpoint-2184/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:36:19,087 >> Configuration saved in models/OneShot/1/checkpoint-2184/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:36:20,472 >> Module weights saved in models/OneShot/1/checkpoint-2184/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:36:20,472 >> tokenizer config file saved in models/OneShot/1/checkpoint-2184/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:36:20,472 >> Special tokens file saved in models/OneShot/1/checkpoint-2184/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:36:20,927 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2080] due to args.save_total_limit\n",
            " 88% 2288/2600 [27:45<03:26,  1.51it/s][INFO|trainer.py:623] 2022-08-25 05:37:30,682 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:37:30,684 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:37:30,684 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:37:30,684 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.19it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.34it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.08it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.65it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.25it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.18it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.20it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.13it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.00it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.95it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.90it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.92it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.96it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.89it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.38it/s]\u001b[A\n",
            "{'eval_loss': 3.8752007484436035, 'eval_accuracy': 0.5494505763053894, 'eval_f1': 0.5446606549596582, 'eval_runtime': 3.1298, 'eval_samples_per_second': 87.225, 'eval_steps_per_second': 11.183, 'epoch': 22.0}\n",
            "\n",
            " 88% 2288/2600 [27:48<03:26,  1.51it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:37:33,815 >> Saving model checkpoint to models/OneShot/1/checkpoint-2288\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:33,816 >> Configuration saved in models/OneShot/1/checkpoint-2288/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:33,910 >> Module weights saved in models/OneShot/1/checkpoint-2288/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:33,911 >> Configuration saved in models/OneShot/1/checkpoint-2288/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:33,919 >> Module weights saved in models/OneShot/1/checkpoint-2288/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:33,920 >> Configuration saved in models/OneShot/1/checkpoint-2288/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:33,972 >> Module weights saved in models/OneShot/1/checkpoint-2288/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:33,973 >> Configuration saved in models/OneShot/1/checkpoint-2288/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:34,039 >> Module weights saved in models/OneShot/1/checkpoint-2288/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:34,040 >> Configuration saved in models/OneShot/1/checkpoint-2288/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:35,421 >> Module weights saved in models/OneShot/1/checkpoint-2288/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:35,447 >> Configuration saved in models/OneShot/1/checkpoint-2288/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:35,463 >> Module weights saved in models/OneShot/1/checkpoint-2288/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:37:35,526 >> Configuration saved in models/OneShot/1/checkpoint-2288/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:37:36,928 >> Module weights saved in models/OneShot/1/checkpoint-2288/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:37:36,929 >> tokenizer config file saved in models/OneShot/1/checkpoint-2288/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:37:36,929 >> Special tokens file saved in models/OneShot/1/checkpoint-2288/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:37:37,397 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2184] due to args.save_total_limit\n",
            " 92% 2392/2600 [29:02<02:17,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:38:47,177 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:38:47,179 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:38:47,179 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:38:47,179 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.56it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.21it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.06it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.52it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.24it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.20it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.15it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.05it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 10.95it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 10.90it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.90it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.91it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 10.87it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.87it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.86it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.8762824535369873, 'eval_accuracy': 0.5457875728607178, 'eval_f1': 0.5390522875816994, 'eval_runtime': 3.144, 'eval_samples_per_second': 86.833, 'eval_steps_per_second': 11.132, 'epoch': 23.0}\n",
            " 92% 2392/2600 [29:05<02:17,  1.52it/s]\n",
            "100% 35/35 [00:03<00:00, 12.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:38:50,325 >> Saving model checkpoint to models/OneShot/1/checkpoint-2392\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:50,325 >> Configuration saved in models/OneShot/1/checkpoint-2392/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:50,417 >> Module weights saved in models/OneShot/1/checkpoint-2392/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:50,417 >> Configuration saved in models/OneShot/1/checkpoint-2392/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:50,425 >> Module weights saved in models/OneShot/1/checkpoint-2392/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:50,425 >> Configuration saved in models/OneShot/1/checkpoint-2392/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:50,476 >> Module weights saved in models/OneShot/1/checkpoint-2392/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:50,476 >> Configuration saved in models/OneShot/1/checkpoint-2392/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:50,543 >> Module weights saved in models/OneShot/1/checkpoint-2392/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:50,544 >> Configuration saved in models/OneShot/1/checkpoint-2392/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:51,984 >> Module weights saved in models/OneShot/1/checkpoint-2392/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:52,095 >> Configuration saved in models/OneShot/1/checkpoint-2392/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:52,115 >> Module weights saved in models/OneShot/1/checkpoint-2392/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:38:52,115 >> Configuration saved in models/OneShot/1/checkpoint-2392/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:38:53,564 >> Module weights saved in models/OneShot/1/checkpoint-2392/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:38:53,565 >> tokenizer config file saved in models/OneShot/1/checkpoint-2392/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:38:53,566 >> Special tokens file saved in models/OneShot/1/checkpoint-2392/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:38:54,029 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2288] due to args.save_total_limit\n",
            " 96% 2496/2600 [30:18<01:08,  1.53it/s][INFO|trainer.py:623] 2022-08-25 05:40:03,789 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:40:03,791 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:40:03,791 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:40:03,791 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:02, 15.81it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.20it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.10it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.76it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.34it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.26it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.21it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.18it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.09it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.06it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 11.06it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.08it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.11it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.07it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.00it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.03it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.8915953636169434, 'eval_accuracy': 0.5677655935287476, 'eval_f1': 0.5545630530973451, 'eval_runtime': 3.1076, 'eval_samples_per_second': 87.848, 'eval_steps_per_second': 11.263, 'epoch': 24.0}\n",
            " 96% 2496/2600 [30:21<01:08,  1.53it/s]\n",
            "100% 35/35 [00:03<00:00, 12.40it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:40:06,900 >> Saving model checkpoint to models/OneShot/1/checkpoint-2496\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:06,901 >> Configuration saved in models/OneShot/1/checkpoint-2496/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:06,988 >> Module weights saved in models/OneShot/1/checkpoint-2496/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:06,988 >> Configuration saved in models/OneShot/1/checkpoint-2496/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:06,996 >> Module weights saved in models/OneShot/1/checkpoint-2496/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:06,996 >> Configuration saved in models/OneShot/1/checkpoint-2496/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:07,047 >> Module weights saved in models/OneShot/1/checkpoint-2496/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:07,048 >> Configuration saved in models/OneShot/1/checkpoint-2496/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:07,114 >> Module weights saved in models/OneShot/1/checkpoint-2496/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:07,114 >> Configuration saved in models/OneShot/1/checkpoint-2496/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:08,552 >> Module weights saved in models/OneShot/1/checkpoint-2496/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:08,682 >> Configuration saved in models/OneShot/1/checkpoint-2496/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:08,704 >> Module weights saved in models/OneShot/1/checkpoint-2496/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:40:08,704 >> Configuration saved in models/OneShot/1/checkpoint-2496/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:40:10,126 >> Module weights saved in models/OneShot/1/checkpoint-2496/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:40:10,127 >> tokenizer config file saved in models/OneShot/1/checkpoint-2496/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:40:10,127 >> Special tokens file saved in models/OneShot/1/checkpoint-2496/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:40:10,599 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2392] due to args.save_total_limit\n",
            "{'loss': 0.0008, 'learning_rate': 3.846153846153847e-06, 'epoch': 24.04}\n",
            "100% 2600/2600 [31:35<00:00,  1.52it/s][INFO|trainer.py:623] 2022-08-25 05:41:20,423 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:41:20,425 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:41:20,425 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:41:20,425 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.04it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.21it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.09it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 11.73it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.43it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.30it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.19it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.14it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.08it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.00it/s]\u001b[A\n",
            " 66% 23/35 [00:02<00:01, 10.98it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 10.94it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.02it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.01it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 10.98it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 10.95it/s]\u001b[A\n",
            "100% 35/35 [00:03<00:00, 12.36it/s]\u001b[A\n",
            "{'eval_loss': 3.892061710357666, 'eval_accuracy': 0.5677655935287476, 'eval_f1': 0.55569347898047, 'eval_runtime': 3.1179, 'eval_samples_per_second': 87.559, 'eval_steps_per_second': 11.225, 'epoch': 25.0}\n",
            "\n",
            "100% 2600/2600 [31:38<00:00,  1.52it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 05:41:23,544 >> Saving model checkpoint to models/OneShot/1/checkpoint-2600\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:23,544 >> Configuration saved in models/OneShot/1/checkpoint-2600/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:23,637 >> Module weights saved in models/OneShot/1/checkpoint-2600/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:23,638 >> Configuration saved in models/OneShot/1/checkpoint-2600/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:23,645 >> Module weights saved in models/OneShot/1/checkpoint-2600/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:23,645 >> Configuration saved in models/OneShot/1/checkpoint-2600/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:23,696 >> Module weights saved in models/OneShot/1/checkpoint-2600/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:23,696 >> Configuration saved in models/OneShot/1/checkpoint-2600/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:23,763 >> Module weights saved in models/OneShot/1/checkpoint-2600/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:23,764 >> Configuration saved in models/OneShot/1/checkpoint-2600/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:25,192 >> Module weights saved in models/OneShot/1/checkpoint-2600/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:25,328 >> Configuration saved in models/OneShot/1/checkpoint-2600/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:25,362 >> Module weights saved in models/OneShot/1/checkpoint-2600/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:25,362 >> Configuration saved in models/OneShot/1/checkpoint-2600/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:26,714 >> Module weights saved in models/OneShot/1/checkpoint-2600/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:41:26,714 >> tokenizer config file saved in models/OneShot/1/checkpoint-2600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:41:26,714 >> Special tokens file saved in models/OneShot/1/checkpoint-2600/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 05:41:27,198 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2496] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 05:41:27,258 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 05:41:27,258 >> Loading best model from models/OneShot/1/checkpoint-1040 (score: 0.5800234951890804).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 05:41:27,258 >> Could not locate the best model at models/OneShot/1/checkpoint-1040/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 1902.3537, 'train_samples_per_second': 43.722, 'train_steps_per_second': 1.367, 'train_loss': 0.053083912718754546, 'epoch': 25.0}\n",
            "100% 2600/2600 [31:42<00:00,  1.52it/s][INFO|trainer.py:238] 2022-08-25 05:41:27,314 >> Loading best adapter(s) from models/OneShot/1/checkpoint-1040 (score: 0.5800234951890804).\n",
            "[INFO|loading.py:77] 2022-08-25 05:41:27,315 >> Loading module configuration from models/OneShot/1/checkpoint-1040/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 05:41:27,320 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 05:41:27,832 >> Loading module weights from models/OneShot/1/checkpoint-1040/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 05:41:27,854 >> Loading module configuration from models/OneShot/1/checkpoint-1040/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 05:41:27,855 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 05:41:27,866 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 05:41:27,889 >> Loading module weights from models/OneShot/1/checkpoint-1040/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 05:41:27,892 >> Loading module configuration from models/OneShot/1/checkpoint-1040/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 05:41:27,893 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 05:41:28,037 >> Loading module weights from models/OneShot/1/checkpoint-1040/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 05:41:28,054 >> No matching prediction head found in 'models/OneShot/1/checkpoint-1040/en'\n",
            "[INFO|loading.py:77] 2022-08-25 05:41:28,055 >> Loading module configuration from models/OneShot/1/checkpoint-1040/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 05:41:28,055 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 05:41:28,207 >> Loading module weights from models/OneShot/1/checkpoint-1040/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 05:41:28,224 >> Loading module configuration from models/OneShot/1/checkpoint-1040/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 05:41:28,225 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 05:41:29,402 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 05:41:31,114 >> Loading module weights from models/OneShot/1/checkpoint-1040/pt/pytorch_model_head.bin\n",
            "100% 2600/2600 [31:46<00:00,  1.36it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 05:41:31,201 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:31,202 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:31,328 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:31,328 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:31,336 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:31,337 >> Configuration saved in models/OneShot/1/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:31,398 >> Module weights saved in models/OneShot/1/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:31,398 >> Configuration saved in models/OneShot/1/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:31,472 >> Module weights saved in models/OneShot/1/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:31,472 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:32,900 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:33,056 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:33,071 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 05:41:33,072 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 05:41:34,485 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 05:41:34,495 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 05:41:34,495 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0531\n",
            "  train_runtime            = 0:31:42.35\n",
            "  train_samples            =       3327\n",
            "  train_samples_per_second =     43.722\n",
            "  train_steps_per_second   =      1.367\n",
            "08/25/2022 05:41:34 - INFO - __main__ - *** Evaluate ***\n",
            "\n",
            "\n",
            "Changing the language adapter to PT during evaluation..\n",
            "\n",
            "\n",
            "[INFO|trainer.py:623] 2022-08-25 05:41:34,718 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 05:41:34,719 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 05:41:34,719 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 05:41:34,720 >>   Batch size = 8\n",
            "100% 35/35 [00:02<00:00, 11.97it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.5458\n",
            "  eval_f1                 =     0.5406\n",
            "  eval_loss               =     2.5661\n",
            "  eval_runtime            = 0:00:03.53\n",
            "  eval_samples            =        273\n",
            "  eval_samples_per_second =     77.278\n",
            "  eval_steps_per_second   =      9.907\n",
            "[INFO|modelcard.py:460] 2022-08-25 05:41:38,615 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5457875728607178}, {'name': 'F1', 'type': 'f1', 'value': 0.5406036260992291}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for zero-shot PT-EN idiomatic knowledge transfer"
      ],
      "metadata": {
        "id": "Vypmi3DNLFhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer from Portuguese to English\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/ZeroShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/ZeroShot/PT/train.csv \\\n",
        "  --validation_file Data/ZeroShot/EN/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "ODMVuvtaKhqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c584424-27e4-4824-975e-94a7d070271a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 10:25:16 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 10:25:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/1/runs/Aug25_10-25-16_9b17356d7e44,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/ZeroShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 10:25:16 - INFO - __main__ - load a local file for train: Data/ZeroShot/PT/train.csv\n",
            "08/25/2022 10:25:16 - INFO - __main__ - load a local file for validation: Data/ZeroShot/EN/dev.csv\n",
            "08/25/2022 10:25:16 - WARNING - datasets.builder - Using custom data configuration default-79c3e9ef8b17b30a\n",
            "08/25/2022 10:25:16 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-79c3e9ef8b17b30a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-79c3e9ef8b17b30a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 11008.67it/s]\n",
            "08/25/2022 10:25:16 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 10:25:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1339.82it/s]\n",
            "08/25/2022 10:25:16 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 10:25:16 - INFO - datasets.builder - Generating train split\n",
            "08/25/2022 10:25:16 - INFO - datasets.builder - Generating validation split\n",
            "08/25/2022 10:25:16 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-79c3e9ef8b17b30a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 464.69it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:25:16,555 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:25:16,556 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:25:16,606 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:25:16,606 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:25:16,748 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:25:16,748 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:25:16,748 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:25:16,748 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:25:16,748 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:25:16,772 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:25:16,773 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 10:25:16,976 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 10:25:21,834 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 10:25:21,835 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 10:25:21,845 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 10:25:21,884 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 10:25:22,088 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:327] 2022-08-25 10:25:22,173 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 10:25:22,252 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 10:25:23,055 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 10:25:23,056 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:25:23,295 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 10:25:23,307 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 10:25:23,308 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 10:25:23,332 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 10:25:23,407 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 10:25:26,729 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 10:25:26,737 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:25:27,008 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:25:27,022 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 10:25:27,022 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 10:25:28,200 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:25:29,830 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 10:25:29,898 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with PT language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]08/25/2022 10:25:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-79c3e9ef8b17b30a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-7ed39b6e17dd7758.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  7.22ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 10:25:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-79c3e9ef8b17b30a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-5a5a764cd8b677ad.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 10.76ba/s]\n",
            "08/25/2022 10:25:30 - INFO - __main__ - Sample 275 of the training set: {'label': 0, 'sentence1': 'O enigmático fenômeno, que foi descoberto há cerca de 20 anos, foi explicado pelos cientistas através de sua reprodução em laboratório. Para tanto, eles colocaram gelo seco em contato com materiais quentes semelhantes aos encontrados no solo marciano. Embora evidentemente em escala menor, foi possível então observar a formação das mesmas “aranhas”.', 'input_ids': [101, 152, 51428, 10240, 71080, 34778, 10115, 40386, 10343, 117, 10121, 10448, 43264, 25056, 13698, 10104, 10197, 12024, 117, 10448, 44364, 10317, 18008, 99485, 41555, 10107, 21056, 10104, 10603, 76456, 101386, 10266, 27605, 90642, 12013, 119, 13497, 12921, 117, 19695, 101058, 11008, 74458, 10133, 96004, 10266, 101912, 10212, 75985, 47618, 11197, 82482, 10107, 13851, 77685, 10192, 11395, 89770, 19972, 119, 53109, 56956, 10611, 10266, 28461, 17618, 117, 10448, 39822, 17650, 55451, 169, 42561, 10242, 18404, 10107, 100, 13785, 36925, 10107, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 10:25:30 - INFO - __main__ - Sample 129 of the training set: {'label': 0, 'sentence1': 'Em 2020 os problemas dos indígenas se agravaram, devido tanto à covid-19 quanto ao garimpo ilegal, incêndios florestais e posses de terra, enquanto as autoridades se empenharam para desmontar as instituições que monitoram e protegem essas áreas. A Anistia Internacional destacou que houve um fortalecimento da retórica contra os direitos humanos no Brasil no ano passado, condição que elevou os riscos para ativistas, jornalistas e minorias, além de promover mais violência no país. Prosseguiu o encolhimento do espaço cívico fomentado por uma narrativa oficial que estigmatizou as ONG, jornalistas, ativistas, defensores dos direitos humanos e movimentos sociais.', 'input_ids': [101, 11289, 23607, 10427, 20088, 10398, 49856, 10126, 16942, 47537, 13845, 117, 24715, 12921, 254, 11170, 32194, 118, 10270, 15696, 10610, 47243, 11759, 13520, 101114, 117, 10106, 96411, 109771, 10107, 29927, 58715, 173, 83509, 10107, 10104, 15336, 117, 22516, 10146, 37350, 10126, 10266, 12708, 32169, 10147, 10220, 10139, 21984, 10354, 10146, 99595, 10121, 60774, 11008, 173, 11284, 102318, 10451, 83959, 23571, 119, 138, 90796, 88617, 15111, 25358, 30656, 10121, 68752, 10293, 103962, 42507, 20470, 10143, 62893, 65356, 11473, 10427, 58475, 24806, 10192, 12264, 10192, 12797, 73733, 117, 107653, 10121, 105181, 11010, 10427, 84697, 10107, 10220, 48458, 29106, 10107, 117, 73539, 10107, 173, 65411, 10403, 117, 21847, 10104, 57833, 10614, 43005, 102698, 10192, 12115, 119, 14021, 12818, 55818, 10138, 183, 10110, 27840, 31362, 17057, 10149, 40463, 171, 29244, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "08/25/2022 10:25:30 - INFO - __main__ - Sample 522 of the training set: {'label': 1, 'sentence1': 'A Moutai tem uma vantagem inconfundível: a bebida é o destilado nacional da China. A moutai baijiu — o tipo de bebida que a empresa fabrica — é uma bebida alcoólica clara e potente que foi apelidada de aguardente, graças ao fato de ter 53% de álcool. As garrafas vermelhas e brancas de seu principal produto, chamado “Feitian” (ou “Fada Voadora”), são disputadas em banquetes estatais chineses e eventos de negócios.', 'input_ids': [101, 138, 34987, 14118, 10116, 12900, 10437, 23266, 29111, 10106, 23486, 55227, 42134, 131, 169, 10347, 33341, 263, 183, 10139, 49999, 10317, 14352, 10143, 11593, 119, 138, 46912, 14118, 10116, 48775, 10775, 10138, 100, 183, 13113, 10104, 10347, 33341, 10121, 169, 14277, 12211, 38244, 100, 263, 10437, 10347, 33341, 10164, 10812, 20315, 11043, 49055, 173, 50689, 10121, 10448, 26219, 17641, 41798, 10104, 16271, 26505, 10216, 117, 63706, 31759, 10610, 38840, 10104, 12718, 11756, 110, 10104, 255, 82115, 11481, 119, 10882, 47243, 29552, 10403, 16719, 105219, 10107, 173, 50729, 10107, 10104, 10617, 11652, 67727, 117, 23046, 100, 20187, 46087, 10115, 100, 113, 10431, 100, 44271, 10229, 59482, 56969, 100, 114, 117, 12372, 78638, 10107, 10266, 87073, 11197, 14543, 12985, 14325, 77687, 173, 28719, 10104, 10554, 102496, 119, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 10:25:35,697 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 10:25:35,715 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 10:25:35,715 >>   Num examples = 1164\n",
            "[INFO|trainer.py:1421] 2022-08-25 10:25:35,715 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 10:25:35,715 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 10:25:35,715 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 10:25:35,715 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 10:25:35,715 >>   Total optimization steps = 925\n",
            "  4% 37/925 [00:20<06:44,  2.19it/s][INFO|trainer.py:623] 2022-08-25 10:25:56,395 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:25:56,397 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:25:56,397 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:25:56,397 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.76it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 16.20it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 15.04it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 14.39it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 14.07it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.78it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.64it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.64it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:02, 13.60it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.55it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.50it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.50it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.38it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.39it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.39it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.40it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.45it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.46it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.43it/s]\u001b[A\n",
            " 69% 41/59 [00:02<00:01, 13.37it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.37it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.37it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.38it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.40it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.42it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.44it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 13.40it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8053786754608154, 'eval_accuracy': 0.4012875556945801, 'eval_f1': 0.3055809258280056, 'eval_runtime': 4.3467, 'eval_samples_per_second': 107.207, 'eval_steps_per_second': 13.573, 'epoch': 1.0}\n",
            "  4% 37/925 [00:25<06:44,  2.19it/s]\n",
            "100% 59/59 [00:04<00:00, 13.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:26:00,745 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-37\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:00,746 >> Configuration saved in models/ZeroShot/1/checkpoint-37/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:00,876 >> Module weights saved in models/ZeroShot/1/checkpoint-37/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:00,876 >> Configuration saved in models/ZeroShot/1/checkpoint-37/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:00,885 >> Module weights saved in models/ZeroShot/1/checkpoint-37/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:00,886 >> Configuration saved in models/ZeroShot/1/checkpoint-37/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:00,982 >> Module weights saved in models/ZeroShot/1/checkpoint-37/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:00,982 >> Configuration saved in models/ZeroShot/1/checkpoint-37/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:01,100 >> Module weights saved in models/ZeroShot/1/checkpoint-37/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:01,101 >> Configuration saved in models/ZeroShot/1/checkpoint-37/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:02,978 >> Module weights saved in models/ZeroShot/1/checkpoint-37/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:02,997 >> Configuration saved in models/ZeroShot/1/checkpoint-37/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:03,014 >> Module weights saved in models/ZeroShot/1/checkpoint-37/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:03,014 >> Configuration saved in models/ZeroShot/1/checkpoint-37/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:04,396 >> Module weights saved in models/ZeroShot/1/checkpoint-37/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:26:04,397 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-37/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:26:04,397 >> Special tokens file saved in models/ZeroShot/1/checkpoint-37/special_tokens_map.json\n",
            "  8% 74/925 [00:49<06:39,  2.13it/s][INFO|trainer.py:623] 2022-08-25 10:26:25,534 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:26:25,536 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:26:25,536 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:26:25,536 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.68it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.86it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.63it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.99it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.72it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.36it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.26it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.24it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 13.19it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.17it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.19it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.15it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.06it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.09it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.06it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.10it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.04it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.10it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.05it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.98it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.03it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.02it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.06it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.02it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.09it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.03it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 13.05it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 13.05it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8052512407302856, 'eval_accuracy': 0.5364806652069092, 'eval_f1': 0.5342846038532008, 'eval_runtime': 4.4635, 'eval_samples_per_second': 104.402, 'eval_steps_per_second': 13.218, 'epoch': 2.0}\n",
            "  8% 74/925 [00:54<06:39,  2.13it/s]\n",
            "100% 59/59 [00:04<00:00, 14.49it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:26:30,001 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-74\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:30,001 >> Configuration saved in models/ZeroShot/1/checkpoint-74/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:30,097 >> Module weights saved in models/ZeroShot/1/checkpoint-74/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:30,097 >> Configuration saved in models/ZeroShot/1/checkpoint-74/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:30,106 >> Module weights saved in models/ZeroShot/1/checkpoint-74/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:30,106 >> Configuration saved in models/ZeroShot/1/checkpoint-74/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:30,175 >> Module weights saved in models/ZeroShot/1/checkpoint-74/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:30,176 >> Configuration saved in models/ZeroShot/1/checkpoint-74/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:30,237 >> Module weights saved in models/ZeroShot/1/checkpoint-74/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:30,238 >> Configuration saved in models/ZeroShot/1/checkpoint-74/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:31,696 >> Module weights saved in models/ZeroShot/1/checkpoint-74/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:31,697 >> Configuration saved in models/ZeroShot/1/checkpoint-74/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:31,706 >> Module weights saved in models/ZeroShot/1/checkpoint-74/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:31,707 >> Configuration saved in models/ZeroShot/1/checkpoint-74/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:33,200 >> Module weights saved in models/ZeroShot/1/checkpoint-74/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:26:33,201 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-74/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:26:33,201 >> Special tokens file saved in models/ZeroShot/1/checkpoint-74/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:26:33,659 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-37] due to args.save_total_limit\n",
            " 12% 111/925 [01:19<06:38,  2.04it/s][INFO|trainer.py:623] 2022-08-25 10:26:55,138 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:26:55,140 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:26:55,140 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:26:55,140 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.94it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.31it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.96it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.37it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.07it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.81it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.53it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.37it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.39it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.40it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.37it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.41it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.34it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.23it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.25it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.33it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.35it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.38it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.34it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.21it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.25it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.31it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.31it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.38it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.36it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.26it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.22it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.849730372428894, 'eval_accuracy': 0.4935622215270996, 'eval_f1': 0.4933289105115731, 'eval_runtime': 4.7261, 'eval_samples_per_second': 98.602, 'eval_steps_per_second': 12.484, 'epoch': 3.0}\n",
            " 12% 111/925 [01:24<06:38,  2.04it/s]\n",
            "100% 59/59 [00:04<00:00, 13.60it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:26:59,867 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-111\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:59,868 >> Configuration saved in models/ZeroShot/1/checkpoint-111/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:59,961 >> Module weights saved in models/ZeroShot/1/checkpoint-111/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:59,962 >> Configuration saved in models/ZeroShot/1/checkpoint-111/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:26:59,970 >> Module weights saved in models/ZeroShot/1/checkpoint-111/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:26:59,970 >> Configuration saved in models/ZeroShot/1/checkpoint-111/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:00,024 >> Module weights saved in models/ZeroShot/1/checkpoint-111/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:00,025 >> Configuration saved in models/ZeroShot/1/checkpoint-111/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:00,084 >> Module weights saved in models/ZeroShot/1/checkpoint-111/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:00,085 >> Configuration saved in models/ZeroShot/1/checkpoint-111/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:01,511 >> Module weights saved in models/ZeroShot/1/checkpoint-111/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:01,551 >> Configuration saved in models/ZeroShot/1/checkpoint-111/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:01,565 >> Module weights saved in models/ZeroShot/1/checkpoint-111/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:01,566 >> Configuration saved in models/ZeroShot/1/checkpoint-111/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:02,987 >> Module weights saved in models/ZeroShot/1/checkpoint-111/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:27:02,987 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-111/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:27:02,988 >> Special tokens file saved in models/ZeroShot/1/checkpoint-111/special_tokens_map.json\n",
            " 16% 148/925 [01:50<06:35,  1.96it/s][INFO|trainer.py:623] 2022-08-25 10:27:25,749 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:27:25,751 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:27:25,751 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:27:25,751 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.45it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.67it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.40it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.92it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.66it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.38it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.28it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.15it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.17it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.20it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.24it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.29it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.25it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.27it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.21it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.07it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.01it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.10it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.13it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.12it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.09it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.97it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 11.87it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.84it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.05it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.122881531715393, 'eval_accuracy': 0.5686695575714111, 'eval_f1': 0.5677918002907044, 'eval_runtime': 4.8257, 'eval_samples_per_second': 96.566, 'eval_steps_per_second': 12.226, 'epoch': 4.0}\n",
            " 16% 148/925 [01:54<06:35,  1.96it/s]\n",
            "100% 59/59 [00:04<00:00, 13.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:27:30,578 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-148\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:30,579 >> Configuration saved in models/ZeroShot/1/checkpoint-148/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:30,670 >> Module weights saved in models/ZeroShot/1/checkpoint-148/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:30,671 >> Configuration saved in models/ZeroShot/1/checkpoint-148/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:30,678 >> Module weights saved in models/ZeroShot/1/checkpoint-148/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:30,679 >> Configuration saved in models/ZeroShot/1/checkpoint-148/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:30,734 >> Module weights saved in models/ZeroShot/1/checkpoint-148/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:30,735 >> Configuration saved in models/ZeroShot/1/checkpoint-148/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:30,814 >> Module weights saved in models/ZeroShot/1/checkpoint-148/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:30,814 >> Configuration saved in models/ZeroShot/1/checkpoint-148/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:32,145 >> Module weights saved in models/ZeroShot/1/checkpoint-148/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:32,292 >> Configuration saved in models/ZeroShot/1/checkpoint-148/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:32,314 >> Module weights saved in models/ZeroShot/1/checkpoint-148/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:27:32,314 >> Configuration saved in models/ZeroShot/1/checkpoint-148/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:27:33,798 >> Module weights saved in models/ZeroShot/1/checkpoint-148/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:27:33,799 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-148/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:27:33,799 >> Special tokens file saved in models/ZeroShot/1/checkpoint-148/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:27:34,274 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-74] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:27:34,345 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-111] due to args.save_total_limit\n",
            " 20% 185/925 [02:22<06:32,  1.89it/s][INFO|trainer.py:623] 2022-08-25 10:27:57,791 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:27:57,793 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:27:57,793 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:27:57,793 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.92it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.74it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.75it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.20it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.82it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.61it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.71it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 11.55it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.52it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.55it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.49it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.47it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.50it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.58it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.48it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.50it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.51it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.48it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.53it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.51it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.49it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.48it/s]\u001b[A\n",
            " 80% 47/59 [00:04<00:01, 11.55it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.49it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.54it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.55it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.59it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5792990922927856, 'eval_accuracy': 0.5021459460258484, 'eval_f1': 0.48623783454987834, 'eval_runtime': 5.0685, 'eval_samples_per_second': 91.941, 'eval_steps_per_second': 11.641, 'epoch': 5.0}\n",
            " 20% 185/925 [02:27<06:32,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 12.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:28:02,863 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-185\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:02,864 >> Configuration saved in models/ZeroShot/1/checkpoint-185/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:02,962 >> Module weights saved in models/ZeroShot/1/checkpoint-185/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:02,962 >> Configuration saved in models/ZeroShot/1/checkpoint-185/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:02,971 >> Module weights saved in models/ZeroShot/1/checkpoint-185/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:02,971 >> Configuration saved in models/ZeroShot/1/checkpoint-185/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:03,026 >> Module weights saved in models/ZeroShot/1/checkpoint-185/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:03,027 >> Configuration saved in models/ZeroShot/1/checkpoint-185/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:03,094 >> Module weights saved in models/ZeroShot/1/checkpoint-185/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:03,094 >> Configuration saved in models/ZeroShot/1/checkpoint-185/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:04,451 >> Module weights saved in models/ZeroShot/1/checkpoint-185/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:04,599 >> Configuration saved in models/ZeroShot/1/checkpoint-185/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:04,617 >> Module weights saved in models/ZeroShot/1/checkpoint-185/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:04,617 >> Configuration saved in models/ZeroShot/1/checkpoint-185/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:05,935 >> Module weights saved in models/ZeroShot/1/checkpoint-185/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:28:05,936 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-185/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:28:05,936 >> Special tokens file saved in models/ZeroShot/1/checkpoint-185/special_tokens_map.json\n",
            " 24% 222/925 [02:53<06:00,  1.95it/s][INFO|trainer.py:623] 2022-08-25 10:28:29,276 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:28:29,277 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:28:29,278 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:28:29,278 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.13it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.26it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.28it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.73it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.31it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.25it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.23it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.22it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.25it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.25it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.23it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.29it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.29it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.30it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.31it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.28it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.30it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.32it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.28it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.20it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.09it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.01it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.03it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.13it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.17it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.16it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.25it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.28it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.4086188077926636, 'eval_accuracy': 0.5193132758140564, 'eval_f1': 0.5157453283601477, 'eval_runtime': 4.7977, 'eval_samples_per_second': 97.131, 'eval_steps_per_second': 12.298, 'epoch': 6.0}\n",
            " 24% 222/925 [02:58<06:00,  1.95it/s]\n",
            "100% 59/59 [00:04<00:00, 13.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:28:34,077 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-222\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:34,078 >> Configuration saved in models/ZeroShot/1/checkpoint-222/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:34,168 >> Module weights saved in models/ZeroShot/1/checkpoint-222/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:34,168 >> Configuration saved in models/ZeroShot/1/checkpoint-222/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:34,176 >> Module weights saved in models/ZeroShot/1/checkpoint-222/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:34,176 >> Configuration saved in models/ZeroShot/1/checkpoint-222/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:34,230 >> Module weights saved in models/ZeroShot/1/checkpoint-222/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:34,231 >> Configuration saved in models/ZeroShot/1/checkpoint-222/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:34,292 >> Module weights saved in models/ZeroShot/1/checkpoint-222/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:34,293 >> Configuration saved in models/ZeroShot/1/checkpoint-222/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:35,684 >> Module weights saved in models/ZeroShot/1/checkpoint-222/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:35,831 >> Configuration saved in models/ZeroShot/1/checkpoint-222/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:35,852 >> Module weights saved in models/ZeroShot/1/checkpoint-222/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:28:35,852 >> Configuration saved in models/ZeroShot/1/checkpoint-222/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:28:37,304 >> Module weights saved in models/ZeroShot/1/checkpoint-222/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:28:37,304 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:28:37,305 >> Special tokens file saved in models/ZeroShot/1/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:28:37,812 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-185] due to args.save_total_limit\n",
            " 28% 259/925 [03:24<05:46,  1.92it/s][INFO|trainer.py:623] 2022-08-25 10:29:00,721 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:29:00,724 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:29:00,724 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:29:00,724 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.04it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.33it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.34it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.19it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.84it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.82it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.74it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.83it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.81it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.76it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.73it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.70it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.69it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.76it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.73it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.73it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.70it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.73it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.8364802598953247, 'eval_accuracy': 0.5021459460258484, 'eval_f1': 0.4819542675330152, 'eval_runtime': 4.9501, 'eval_samples_per_second': 94.139, 'eval_steps_per_second': 11.919, 'epoch': 7.0}\n",
            " 28% 259/925 [03:29<05:46,  1.92it/s]\n",
            "100% 59/59 [00:04<00:00, 13.08it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:29:05,676 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-259\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:05,676 >> Configuration saved in models/ZeroShot/1/checkpoint-259/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:05,764 >> Module weights saved in models/ZeroShot/1/checkpoint-259/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:05,765 >> Configuration saved in models/ZeroShot/1/checkpoint-259/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:05,772 >> Module weights saved in models/ZeroShot/1/checkpoint-259/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:05,773 >> Configuration saved in models/ZeroShot/1/checkpoint-259/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:05,826 >> Module weights saved in models/ZeroShot/1/checkpoint-259/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:05,826 >> Configuration saved in models/ZeroShot/1/checkpoint-259/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:05,891 >> Module weights saved in models/ZeroShot/1/checkpoint-259/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:05,892 >> Configuration saved in models/ZeroShot/1/checkpoint-259/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:07,310 >> Module weights saved in models/ZeroShot/1/checkpoint-259/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:07,447 >> Configuration saved in models/ZeroShot/1/checkpoint-259/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:07,475 >> Module weights saved in models/ZeroShot/1/checkpoint-259/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:07,476 >> Configuration saved in models/ZeroShot/1/checkpoint-259/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:08,773 >> Module weights saved in models/ZeroShot/1/checkpoint-259/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:29:08,774 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-259/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:29:08,774 >> Special tokens file saved in models/ZeroShot/1/checkpoint-259/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:29:09,274 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-222] due to args.save_total_limit\n",
            " 32% 296/925 [03:56<05:25,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:29:32,320 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:29:32,322 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:29:32,322 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:29:32,322 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.79it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.32it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.13it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.62it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.40it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.19it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.85it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.93it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.82it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.84it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.84it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.88it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.9050931930541992, 'eval_accuracy': 0.553648054599762, 'eval_f1': 0.5532448377581121, 'eval_runtime': 4.9009, 'eval_samples_per_second': 95.084, 'eval_steps_per_second': 12.039, 'epoch': 8.0}\n",
            " 32% 296/925 [04:01<05:25,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:29:37,224 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-296\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:37,225 >> Configuration saved in models/ZeroShot/1/checkpoint-296/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:37,315 >> Module weights saved in models/ZeroShot/1/checkpoint-296/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:37,315 >> Configuration saved in models/ZeroShot/1/checkpoint-296/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:37,323 >> Module weights saved in models/ZeroShot/1/checkpoint-296/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:37,323 >> Configuration saved in models/ZeroShot/1/checkpoint-296/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:37,374 >> Module weights saved in models/ZeroShot/1/checkpoint-296/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:37,374 >> Configuration saved in models/ZeroShot/1/checkpoint-296/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:37,444 >> Module weights saved in models/ZeroShot/1/checkpoint-296/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:37,444 >> Configuration saved in models/ZeroShot/1/checkpoint-296/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:38,871 >> Module weights saved in models/ZeroShot/1/checkpoint-296/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:39,011 >> Configuration saved in models/ZeroShot/1/checkpoint-296/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:39,027 >> Module weights saved in models/ZeroShot/1/checkpoint-296/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:29:39,028 >> Configuration saved in models/ZeroShot/1/checkpoint-296/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:29:40,520 >> Module weights saved in models/ZeroShot/1/checkpoint-296/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:29:40,521 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-296/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:29:40,521 >> Special tokens file saved in models/ZeroShot/1/checkpoint-296/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:29:41,077 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-259] due to args.save_total_limit\n",
            " 36% 333/925 [04:28<05:05,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:30:04,030 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:30:04,032 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:30:04,032 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:30:04,032 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.70it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.12it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.12it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.42it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.14it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.10it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.96it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.85it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.88it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.88it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.78it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.88it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.03it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.5043892860412598, 'eval_accuracy': 0.4828326106071472, 'eval_f1': 0.44273585701455354, 'eval_runtime': 4.9184, 'eval_samples_per_second': 94.747, 'eval_steps_per_second': 11.996, 'epoch': 9.0}\n",
            " 36% 333/925 [04:33<05:05,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.24it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:30:08,952 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-333\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:08,953 >> Configuration saved in models/ZeroShot/1/checkpoint-333/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:09,050 >> Module weights saved in models/ZeroShot/1/checkpoint-333/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:09,051 >> Configuration saved in models/ZeroShot/1/checkpoint-333/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:09,059 >> Module weights saved in models/ZeroShot/1/checkpoint-333/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:09,060 >> Configuration saved in models/ZeroShot/1/checkpoint-333/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:09,114 >> Module weights saved in models/ZeroShot/1/checkpoint-333/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:09,115 >> Configuration saved in models/ZeroShot/1/checkpoint-333/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:09,183 >> Module weights saved in models/ZeroShot/1/checkpoint-333/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:09,184 >> Configuration saved in models/ZeroShot/1/checkpoint-333/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:10,654 >> Module weights saved in models/ZeroShot/1/checkpoint-333/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:10,671 >> Configuration saved in models/ZeroShot/1/checkpoint-333/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:10,817 >> Module weights saved in models/ZeroShot/1/checkpoint-333/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:10,817 >> Configuration saved in models/ZeroShot/1/checkpoint-333/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:12,163 >> Module weights saved in models/ZeroShot/1/checkpoint-333/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:30:12,163 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-333/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:30:12,164 >> Special tokens file saved in models/ZeroShot/1/checkpoint-333/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:30:12,677 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-296] due to args.save_total_limit\n",
            " 40% 370/925 [04:59<04:45,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:30:35,581 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:30:35,583 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:30:35,583 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:30:35,584 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.16it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.20it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.29it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.75it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.27it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.20it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.22it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.20it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 12.00it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.04it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.92it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.86it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.96it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6744022369384766, 'eval_accuracy': 0.4849785268306732, 'eval_f1': 0.4557452358242656, 'eval_runtime': 4.8892, 'eval_samples_per_second': 95.312, 'eval_steps_per_second': 12.067, 'epoch': 10.0}\n",
            " 40% 370/925 [05:04<04:45,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:30:40,474 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-370\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:40,475 >> Configuration saved in models/ZeroShot/1/checkpoint-370/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:40,565 >> Module weights saved in models/ZeroShot/1/checkpoint-370/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:40,566 >> Configuration saved in models/ZeroShot/1/checkpoint-370/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:40,573 >> Module weights saved in models/ZeroShot/1/checkpoint-370/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:40,574 >> Configuration saved in models/ZeroShot/1/checkpoint-370/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:40,629 >> Module weights saved in models/ZeroShot/1/checkpoint-370/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:40,630 >> Configuration saved in models/ZeroShot/1/checkpoint-370/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:40,703 >> Module weights saved in models/ZeroShot/1/checkpoint-370/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:40,703 >> Configuration saved in models/ZeroShot/1/checkpoint-370/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:42,147 >> Module weights saved in models/ZeroShot/1/checkpoint-370/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:42,242 >> Configuration saved in models/ZeroShot/1/checkpoint-370/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:42,261 >> Module weights saved in models/ZeroShot/1/checkpoint-370/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:30:42,262 >> Configuration saved in models/ZeroShot/1/checkpoint-370/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:30:43,728 >> Module weights saved in models/ZeroShot/1/checkpoint-370/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:30:43,729 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-370/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:30:43,729 >> Special tokens file saved in models/ZeroShot/1/checkpoint-370/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:30:44,219 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-333] due to args.save_total_limit\n",
            " 44% 407/925 [05:31<04:27,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:31:07,284 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:31:07,286 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:31:07,286 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:31:07,286 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.03it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.43it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.18it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.52it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.21it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.17it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.97it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.95it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 12.00it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.80it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.82it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.74it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.74it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.73it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.77it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.91it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6856443881988525, 'eval_accuracy': 0.5128755569458008, 'eval_f1': 0.4935631358467983, 'eval_runtime': 4.9251, 'eval_samples_per_second': 94.617, 'eval_steps_per_second': 11.979, 'epoch': 11.0}\n",
            " 44% 407/925 [05:36<04:27,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:31:12,212 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-407\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:12,213 >> Configuration saved in models/ZeroShot/1/checkpoint-407/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:12,305 >> Module weights saved in models/ZeroShot/1/checkpoint-407/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:12,305 >> Configuration saved in models/ZeroShot/1/checkpoint-407/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:12,314 >> Module weights saved in models/ZeroShot/1/checkpoint-407/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:12,315 >> Configuration saved in models/ZeroShot/1/checkpoint-407/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:12,367 >> Module weights saved in models/ZeroShot/1/checkpoint-407/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:12,367 >> Configuration saved in models/ZeroShot/1/checkpoint-407/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:12,443 >> Module weights saved in models/ZeroShot/1/checkpoint-407/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:12,444 >> Configuration saved in models/ZeroShot/1/checkpoint-407/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:13,889 >> Module weights saved in models/ZeroShot/1/checkpoint-407/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:13,895 >> Configuration saved in models/ZeroShot/1/checkpoint-407/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:13,908 >> Module weights saved in models/ZeroShot/1/checkpoint-407/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:13,908 >> Configuration saved in models/ZeroShot/1/checkpoint-407/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:15,369 >> Module weights saved in models/ZeroShot/1/checkpoint-407/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:31:15,370 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-407/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:31:15,370 >> Special tokens file saved in models/ZeroShot/1/checkpoint-407/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:31:15,879 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-370] due to args.save_total_limit\n",
            " 48% 444/925 [06:03<04:07,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:31:38,838 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:31:38,839 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:31:38,839 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:31:38,839 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.02it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.42it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.21it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.53it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.33it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.85it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.89it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.88it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.83it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.374976634979248, 'eval_accuracy': 0.545064389705658, 'eval_f1': 0.5379060020954947, 'eval_runtime': 4.9046, 'eval_samples_per_second': 95.013, 'eval_steps_per_second': 12.03, 'epoch': 12.0}\n",
            " 48% 444/925 [06:08<04:07,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:31:43,745 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-444\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:43,746 >> Configuration saved in models/ZeroShot/1/checkpoint-444/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:43,839 >> Module weights saved in models/ZeroShot/1/checkpoint-444/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:43,840 >> Configuration saved in models/ZeroShot/1/checkpoint-444/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:43,848 >> Module weights saved in models/ZeroShot/1/checkpoint-444/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:43,849 >> Configuration saved in models/ZeroShot/1/checkpoint-444/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:43,910 >> Module weights saved in models/ZeroShot/1/checkpoint-444/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:43,911 >> Configuration saved in models/ZeroShot/1/checkpoint-444/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:43,980 >> Module weights saved in models/ZeroShot/1/checkpoint-444/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:43,981 >> Configuration saved in models/ZeroShot/1/checkpoint-444/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:45,428 >> Module weights saved in models/ZeroShot/1/checkpoint-444/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:45,534 >> Configuration saved in models/ZeroShot/1/checkpoint-444/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:45,555 >> Module weights saved in models/ZeroShot/1/checkpoint-444/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:31:45,556 >> Configuration saved in models/ZeroShot/1/checkpoint-444/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:31:46,970 >> Module weights saved in models/ZeroShot/1/checkpoint-444/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:31:46,970 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-444/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:31:46,970 >> Special tokens file saved in models/ZeroShot/1/checkpoint-444/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:31:47,476 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-407] due to args.save_total_limit\n",
            " 52% 481/925 [06:34<03:48,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:32:10,383 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:32:10,385 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:32:10,385 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:32:10,385 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.19it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.44it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.35it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.64it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.26it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.96it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.86it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.84it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.76it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.81it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.00it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.75it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.78it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.73it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.203444480895996, 'eval_accuracy': 0.4828326106071472, 'eval_f1': 0.4492941239929191, 'eval_runtime': 4.9248, 'eval_samples_per_second': 94.623, 'eval_steps_per_second': 11.98, 'epoch': 13.0}\n",
            " 52% 481/925 [06:39<03:48,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:32:15,311 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-481\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:15,312 >> Configuration saved in models/ZeroShot/1/checkpoint-481/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:15,404 >> Module weights saved in models/ZeroShot/1/checkpoint-481/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:15,405 >> Configuration saved in models/ZeroShot/1/checkpoint-481/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:15,412 >> Module weights saved in models/ZeroShot/1/checkpoint-481/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:15,413 >> Configuration saved in models/ZeroShot/1/checkpoint-481/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:15,473 >> Module weights saved in models/ZeroShot/1/checkpoint-481/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:15,473 >> Configuration saved in models/ZeroShot/1/checkpoint-481/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:15,543 >> Module weights saved in models/ZeroShot/1/checkpoint-481/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:15,544 >> Configuration saved in models/ZeroShot/1/checkpoint-481/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:17,036 >> Module weights saved in models/ZeroShot/1/checkpoint-481/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:17,127 >> Configuration saved in models/ZeroShot/1/checkpoint-481/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:17,171 >> Module weights saved in models/ZeroShot/1/checkpoint-481/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:17,172 >> Configuration saved in models/ZeroShot/1/checkpoint-481/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:18,607 >> Module weights saved in models/ZeroShot/1/checkpoint-481/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:32:18,608 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-481/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:32:18,608 >> Special tokens file saved in models/ZeroShot/1/checkpoint-481/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:32:19,091 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-444] due to args.save_total_limit\n",
            "{'loss': 0.16, 'learning_rate': 4.594594594594595e-05, 'epoch': 13.51}\n",
            " 56% 518/925 [07:06<03:30,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:32:42,088 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:32:42,090 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:32:42,090 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:32:42,090 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.21it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.46it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.18it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.56it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.17it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.90it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.90it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.83it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.78it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.75it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.73it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.76it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.74it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.71it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.70it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.70it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.74it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.72it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1431827545166016, 'eval_accuracy': 0.5236051678657532, 'eval_f1': 0.5076059019514517, 'eval_runtime': 4.9429, 'eval_samples_per_second': 94.277, 'eval_steps_per_second': 11.936, 'epoch': 14.0}\n",
            " 56% 518/925 [07:11<03:30,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:32:47,034 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-518\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:47,035 >> Configuration saved in models/ZeroShot/1/checkpoint-518/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:47,127 >> Module weights saved in models/ZeroShot/1/checkpoint-518/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:47,128 >> Configuration saved in models/ZeroShot/1/checkpoint-518/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:47,135 >> Module weights saved in models/ZeroShot/1/checkpoint-518/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:47,136 >> Configuration saved in models/ZeroShot/1/checkpoint-518/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:47,191 >> Module weights saved in models/ZeroShot/1/checkpoint-518/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:47,192 >> Configuration saved in models/ZeroShot/1/checkpoint-518/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:47,264 >> Module weights saved in models/ZeroShot/1/checkpoint-518/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:47,265 >> Configuration saved in models/ZeroShot/1/checkpoint-518/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:48,779 >> Module weights saved in models/ZeroShot/1/checkpoint-518/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:48,861 >> Configuration saved in models/ZeroShot/1/checkpoint-518/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:48,878 >> Module weights saved in models/ZeroShot/1/checkpoint-518/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:32:48,878 >> Configuration saved in models/ZeroShot/1/checkpoint-518/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:32:50,325 >> Module weights saved in models/ZeroShot/1/checkpoint-518/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:32:50,326 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-518/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:32:50,326 >> Special tokens file saved in models/ZeroShot/1/checkpoint-518/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:32:50,805 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-481] due to args.save_total_limit\n",
            " 60% 555/925 [07:38<03:11,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:33:13,774 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:33:13,776 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:33:13,776 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:33:13,776 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.98it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.55it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.38it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.60it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.30it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.93it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.89it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.91it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.83it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.97it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.80it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.88it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.78it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.86it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.91it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.88it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.0463523864746094, 'eval_accuracy': 0.54935622215271, 'eval_f1': 0.5436741088915001, 'eval_runtime': 4.9096, 'eval_samples_per_second': 94.916, 'eval_steps_per_second': 12.017, 'epoch': 15.0}\n",
            " 60% 555/925 [07:42<03:11,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.16it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:33:18,687 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-555\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:18,688 >> Configuration saved in models/ZeroShot/1/checkpoint-555/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:18,777 >> Module weights saved in models/ZeroShot/1/checkpoint-555/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:18,777 >> Configuration saved in models/ZeroShot/1/checkpoint-555/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:18,785 >> Module weights saved in models/ZeroShot/1/checkpoint-555/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:18,786 >> Configuration saved in models/ZeroShot/1/checkpoint-555/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:18,837 >> Module weights saved in models/ZeroShot/1/checkpoint-555/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:18,838 >> Configuration saved in models/ZeroShot/1/checkpoint-555/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:18,905 >> Module weights saved in models/ZeroShot/1/checkpoint-555/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:18,906 >> Configuration saved in models/ZeroShot/1/checkpoint-555/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:20,462 >> Module weights saved in models/ZeroShot/1/checkpoint-555/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:20,503 >> Configuration saved in models/ZeroShot/1/checkpoint-555/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:20,521 >> Module weights saved in models/ZeroShot/1/checkpoint-555/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:20,521 >> Configuration saved in models/ZeroShot/1/checkpoint-555/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:21,991 >> Module weights saved in models/ZeroShot/1/checkpoint-555/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:33:21,992 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-555/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:33:21,992 >> Special tokens file saved in models/ZeroShot/1/checkpoint-555/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:33:22,469 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-518] due to args.save_total_limit\n",
            " 64% 592/925 [08:09<02:52,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:33:45,460 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:33:45,462 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:33:45,462 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:33:45,462 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.80it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.51it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.45it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.68it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.34it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.19it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 11.96it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.89it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.92it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.83it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.02it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.91it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.2662861347198486, 'eval_accuracy': 0.5364806652069092, 'eval_f1': 0.5257801899592944, 'eval_runtime': 4.897, 'eval_samples_per_second': 95.161, 'eval_steps_per_second': 12.048, 'epoch': 16.0}\n",
            " 64% 592/925 [08:14<02:52,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:33:50,361 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-592\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:50,361 >> Configuration saved in models/ZeroShot/1/checkpoint-592/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:50,452 >> Module weights saved in models/ZeroShot/1/checkpoint-592/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:50,452 >> Configuration saved in models/ZeroShot/1/checkpoint-592/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:50,460 >> Module weights saved in models/ZeroShot/1/checkpoint-592/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:50,460 >> Configuration saved in models/ZeroShot/1/checkpoint-592/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:50,518 >> Module weights saved in models/ZeroShot/1/checkpoint-592/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:50,518 >> Configuration saved in models/ZeroShot/1/checkpoint-592/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:50,590 >> Module weights saved in models/ZeroShot/1/checkpoint-592/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:50,591 >> Configuration saved in models/ZeroShot/1/checkpoint-592/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:52,127 >> Module weights saved in models/ZeroShot/1/checkpoint-592/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:52,198 >> Configuration saved in models/ZeroShot/1/checkpoint-592/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:52,218 >> Module weights saved in models/ZeroShot/1/checkpoint-592/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:33:52,219 >> Configuration saved in models/ZeroShot/1/checkpoint-592/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:33:53,666 >> Module weights saved in models/ZeroShot/1/checkpoint-592/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:33:53,667 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-592/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:33:53,667 >> Special tokens file saved in models/ZeroShot/1/checkpoint-592/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:33:54,152 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-555] due to args.save_total_limit\n",
            " 68% 629/925 [08:41<02:32,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:34:17,095 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:34:17,096 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:34:17,096 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:34:17,097 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.03it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.45it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.25it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.58it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.33it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.84it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.88it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.4422078132629395, 'eval_accuracy': 0.5257510542869568, 'eval_f1': 0.5117179906597444, 'eval_runtime': 4.9057, 'eval_samples_per_second': 94.992, 'eval_steps_per_second': 12.027, 'epoch': 17.0}\n",
            " 68% 629/925 [08:46<02:32,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:34:22,004 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-629\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:22,004 >> Configuration saved in models/ZeroShot/1/checkpoint-629/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:22,096 >> Module weights saved in models/ZeroShot/1/checkpoint-629/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:22,096 >> Configuration saved in models/ZeroShot/1/checkpoint-629/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:22,105 >> Module weights saved in models/ZeroShot/1/checkpoint-629/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:22,105 >> Configuration saved in models/ZeroShot/1/checkpoint-629/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:22,168 >> Module weights saved in models/ZeroShot/1/checkpoint-629/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:22,169 >> Configuration saved in models/ZeroShot/1/checkpoint-629/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:22,239 >> Module weights saved in models/ZeroShot/1/checkpoint-629/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:22,240 >> Configuration saved in models/ZeroShot/1/checkpoint-629/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:23,745 >> Module weights saved in models/ZeroShot/1/checkpoint-629/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:23,849 >> Configuration saved in models/ZeroShot/1/checkpoint-629/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:23,888 >> Module weights saved in models/ZeroShot/1/checkpoint-629/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:23,888 >> Configuration saved in models/ZeroShot/1/checkpoint-629/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:25,316 >> Module weights saved in models/ZeroShot/1/checkpoint-629/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:34:25,316 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-629/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:34:25,316 >> Special tokens file saved in models/ZeroShot/1/checkpoint-629/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:34:25,792 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-592] due to args.save_total_limit\n",
            " 72% 666/925 [09:13<02:13,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:34:48,762 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:34:48,764 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:34:48,764 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:34:48,764 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.05it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.48it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.29it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.53it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.21it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.91it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.82it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.97it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.84it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.84it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.05it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 12.06it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.85it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.77it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.7006285190582275, 'eval_accuracy': 0.5171673893928528, 'eval_f1': 0.49973042221130126, 'eval_runtime': 4.9088, 'eval_samples_per_second': 94.931, 'eval_steps_per_second': 12.019, 'epoch': 18.0}\n",
            " 72% 666/925 [09:17<02:13,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:34:53,674 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-666\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:53,675 >> Configuration saved in models/ZeroShot/1/checkpoint-666/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:53,766 >> Module weights saved in models/ZeroShot/1/checkpoint-666/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:53,766 >> Configuration saved in models/ZeroShot/1/checkpoint-666/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:53,774 >> Module weights saved in models/ZeroShot/1/checkpoint-666/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:53,774 >> Configuration saved in models/ZeroShot/1/checkpoint-666/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:53,827 >> Module weights saved in models/ZeroShot/1/checkpoint-666/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:53,828 >> Configuration saved in models/ZeroShot/1/checkpoint-666/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:53,897 >> Module weights saved in models/ZeroShot/1/checkpoint-666/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:53,897 >> Configuration saved in models/ZeroShot/1/checkpoint-666/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:55,392 >> Module weights saved in models/ZeroShot/1/checkpoint-666/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:55,469 >> Configuration saved in models/ZeroShot/1/checkpoint-666/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:55,491 >> Module weights saved in models/ZeroShot/1/checkpoint-666/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:34:55,492 >> Configuration saved in models/ZeroShot/1/checkpoint-666/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:34:56,925 >> Module weights saved in models/ZeroShot/1/checkpoint-666/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:34:56,926 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-666/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:34:56,926 >> Special tokens file saved in models/ZeroShot/1/checkpoint-666/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:34:57,526 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-629] due to args.save_total_limit\n",
            " 76% 703/925 [09:44<01:54,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:35:20,554 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:35:20,556 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:35:20,556 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:35:20,556 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.72it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.07it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.97it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.43it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.13it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.88it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.88it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.77it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.78it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.82it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.88it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.77it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.5319130420684814, 'eval_accuracy': 0.5278970003128052, 'eval_f1': 0.5135607052705498, 'eval_runtime': 4.9327, 'eval_samples_per_second': 94.472, 'eval_steps_per_second': 11.961, 'epoch': 19.0}\n",
            " 76% 703/925 [09:49<01:54,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.07it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:35:25,490 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-703\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:25,490 >> Configuration saved in models/ZeroShot/1/checkpoint-703/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:25,587 >> Module weights saved in models/ZeroShot/1/checkpoint-703/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:25,588 >> Configuration saved in models/ZeroShot/1/checkpoint-703/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:25,596 >> Module weights saved in models/ZeroShot/1/checkpoint-703/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:25,596 >> Configuration saved in models/ZeroShot/1/checkpoint-703/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:25,648 >> Module weights saved in models/ZeroShot/1/checkpoint-703/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:25,648 >> Configuration saved in models/ZeroShot/1/checkpoint-703/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:25,715 >> Module weights saved in models/ZeroShot/1/checkpoint-703/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:25,716 >> Configuration saved in models/ZeroShot/1/checkpoint-703/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:27,187 >> Module weights saved in models/ZeroShot/1/checkpoint-703/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:27,499 >> Configuration saved in models/ZeroShot/1/checkpoint-703/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:27,514 >> Module weights saved in models/ZeroShot/1/checkpoint-703/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:27,514 >> Configuration saved in models/ZeroShot/1/checkpoint-703/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:28,873 >> Module weights saved in models/ZeroShot/1/checkpoint-703/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:35:28,874 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-703/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:35:28,874 >> Special tokens file saved in models/ZeroShot/1/checkpoint-703/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:35:29,391 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-666] due to args.save_total_limit\n",
            " 80% 740/925 [10:16<01:35,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:35:52,344 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:35:52,346 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:35:52,346 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:35:52,346 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.96it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.41it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.30it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.60it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.28it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.16it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.96it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.84it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.96it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.0158426761627197, 'eval_accuracy': 0.5643776655197144, 'eval_f1': 0.5626840487252386, 'eval_runtime': 4.905, 'eval_samples_per_second': 95.004, 'eval_steps_per_second': 12.028, 'epoch': 20.0}\n",
            " 80% 740/925 [10:21<01:35,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:35:57,252 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-740\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:57,253 >> Configuration saved in models/ZeroShot/1/checkpoint-740/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:57,342 >> Module weights saved in models/ZeroShot/1/checkpoint-740/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:57,343 >> Configuration saved in models/ZeroShot/1/checkpoint-740/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:57,350 >> Module weights saved in models/ZeroShot/1/checkpoint-740/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:57,351 >> Configuration saved in models/ZeroShot/1/checkpoint-740/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:57,403 >> Module weights saved in models/ZeroShot/1/checkpoint-740/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:57,404 >> Configuration saved in models/ZeroShot/1/checkpoint-740/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:57,491 >> Module weights saved in models/ZeroShot/1/checkpoint-740/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:57,491 >> Configuration saved in models/ZeroShot/1/checkpoint-740/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:58,980 >> Module weights saved in models/ZeroShot/1/checkpoint-740/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:59,079 >> Configuration saved in models/ZeroShot/1/checkpoint-740/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:35:59,099 >> Module weights saved in models/ZeroShot/1/checkpoint-740/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:35:59,100 >> Configuration saved in models/ZeroShot/1/checkpoint-740/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:00,481 >> Module weights saved in models/ZeroShot/1/checkpoint-740/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:36:00,482 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-740/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:36:00,482 >> Special tokens file saved in models/ZeroShot/1/checkpoint-740/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:36:00,971 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-703] due to args.save_total_limit\n",
            " 84% 777/925 [10:48<01:16,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:36:23,917 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:36:23,919 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:36:23,919 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:36:23,919 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.90it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.24it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.21it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.55it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.26it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.92it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.88it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.97it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.76it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.01it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.09it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.11it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.92it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.84it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.85it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.83it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.91it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.2932891845703125, 'eval_accuracy': 0.5557940006256104, 'eval_f1': 0.5475240047470061, 'eval_runtime': 4.8975, 'eval_samples_per_second': 95.151, 'eval_steps_per_second': 12.047, 'epoch': 21.0}\n",
            " 84% 777/925 [10:53<01:16,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:36:28,818 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-777\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:28,819 >> Configuration saved in models/ZeroShot/1/checkpoint-777/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:28,912 >> Module weights saved in models/ZeroShot/1/checkpoint-777/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:28,912 >> Configuration saved in models/ZeroShot/1/checkpoint-777/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:28,920 >> Module weights saved in models/ZeroShot/1/checkpoint-777/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:28,920 >> Configuration saved in models/ZeroShot/1/checkpoint-777/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:28,980 >> Module weights saved in models/ZeroShot/1/checkpoint-777/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:28,981 >> Configuration saved in models/ZeroShot/1/checkpoint-777/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:29,058 >> Module weights saved in models/ZeroShot/1/checkpoint-777/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:29,059 >> Configuration saved in models/ZeroShot/1/checkpoint-777/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:30,526 >> Module weights saved in models/ZeroShot/1/checkpoint-777/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:30,670 >> Configuration saved in models/ZeroShot/1/checkpoint-777/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:30,686 >> Module weights saved in models/ZeroShot/1/checkpoint-777/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:36:30,687 >> Configuration saved in models/ZeroShot/1/checkpoint-777/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:36:32,011 >> Module weights saved in models/ZeroShot/1/checkpoint-777/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:36:32,012 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-777/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:36:32,012 >> Special tokens file saved in models/ZeroShot/1/checkpoint-777/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:36:32,514 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-740] due to args.save_total_limit\n",
            " 88% 814/925 [11:19<00:57,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:36:55,481 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:36:55,483 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:36:55,483 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:36:55,483 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.83it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.51it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.44it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.73it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.23it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.96it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 11.96it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.87it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.85it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.87it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.96it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.83it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.84it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.77it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.3239195346832275, 'eval_accuracy': 0.5557940006256104, 'eval_f1': 0.5469800076080984, 'eval_runtime': 4.9103, 'eval_samples_per_second': 94.903, 'eval_steps_per_second': 12.016, 'epoch': 22.0}\n",
            " 88% 814/925 [11:24<00:57,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:37:00,395 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-814\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:00,396 >> Configuration saved in models/ZeroShot/1/checkpoint-814/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:00,494 >> Module weights saved in models/ZeroShot/1/checkpoint-814/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:00,494 >> Configuration saved in models/ZeroShot/1/checkpoint-814/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:00,502 >> Module weights saved in models/ZeroShot/1/checkpoint-814/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:00,502 >> Configuration saved in models/ZeroShot/1/checkpoint-814/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:00,555 >> Module weights saved in models/ZeroShot/1/checkpoint-814/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:00,556 >> Configuration saved in models/ZeroShot/1/checkpoint-814/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:00,623 >> Module weights saved in models/ZeroShot/1/checkpoint-814/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:00,624 >> Configuration saved in models/ZeroShot/1/checkpoint-814/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:02,070 >> Module weights saved in models/ZeroShot/1/checkpoint-814/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:02,152 >> Configuration saved in models/ZeroShot/1/checkpoint-814/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:02,229 >> Module weights saved in models/ZeroShot/1/checkpoint-814/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:02,230 >> Configuration saved in models/ZeroShot/1/checkpoint-814/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:03,629 >> Module weights saved in models/ZeroShot/1/checkpoint-814/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:37:03,630 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-814/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:37:03,630 >> Special tokens file saved in models/ZeroShot/1/checkpoint-814/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:37:04,110 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-777] due to args.save_total_limit\n",
            " 92% 851/925 [11:51<00:38,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:37:27,071 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:37:27,073 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:37:27,073 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:37:27,073 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.27it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.61it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.30it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.69it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.36it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.17it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.16it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.09it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.03it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.06it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.86it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.04it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.4179787635803223, 'eval_accuracy': 0.54935622215271, 'eval_f1': 0.5395516910394669, 'eval_runtime': 4.8772, 'eval_samples_per_second': 95.546, 'eval_steps_per_second': 12.097, 'epoch': 23.0}\n",
            " 92% 851/925 [11:56<00:38,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.47it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:37:31,952 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-851\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:31,953 >> Configuration saved in models/ZeroShot/1/checkpoint-851/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:32,046 >> Module weights saved in models/ZeroShot/1/checkpoint-851/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:32,046 >> Configuration saved in models/ZeroShot/1/checkpoint-851/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:32,055 >> Module weights saved in models/ZeroShot/1/checkpoint-851/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:32,056 >> Configuration saved in models/ZeroShot/1/checkpoint-851/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:32,122 >> Module weights saved in models/ZeroShot/1/checkpoint-851/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:32,123 >> Configuration saved in models/ZeroShot/1/checkpoint-851/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:32,206 >> Module weights saved in models/ZeroShot/1/checkpoint-851/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:32,207 >> Configuration saved in models/ZeroShot/1/checkpoint-851/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:33,671 >> Module weights saved in models/ZeroShot/1/checkpoint-851/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:33,778 >> Configuration saved in models/ZeroShot/1/checkpoint-851/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:33,828 >> Module weights saved in models/ZeroShot/1/checkpoint-851/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:37:33,828 >> Configuration saved in models/ZeroShot/1/checkpoint-851/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:37:35,126 >> Module weights saved in models/ZeroShot/1/checkpoint-851/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:37:35,126 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-851/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:37:35,126 >> Special tokens file saved in models/ZeroShot/1/checkpoint-851/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:37:35,653 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-814] due to args.save_total_limit\n",
            " 96% 888/925 [12:22<00:19,  1.93it/s][INFO|trainer.py:623] 2022-08-25 10:37:58,611 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:37:58,614 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:37:58,614 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:37:58,614 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.20it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.10it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.16it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.70it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.19it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.18it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.21it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.20it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.88it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.77it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.82it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.09it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.92it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.85it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.5366342067718506, 'eval_accuracy': 0.5321888327598572, 'eval_f1': 0.5194058094427099, 'eval_runtime': 4.8954, 'eval_samples_per_second': 95.191, 'eval_steps_per_second': 12.052, 'epoch': 24.0}\n",
            " 96% 888/925 [12:27<00:19,  1.93it/s]\n",
            "100% 59/59 [00:04<00:00, 13.31it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:38:03,511 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-888\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:03,512 >> Configuration saved in models/ZeroShot/1/checkpoint-888/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:03,606 >> Module weights saved in models/ZeroShot/1/checkpoint-888/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:03,607 >> Configuration saved in models/ZeroShot/1/checkpoint-888/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:03,615 >> Module weights saved in models/ZeroShot/1/checkpoint-888/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:03,615 >> Configuration saved in models/ZeroShot/1/checkpoint-888/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:03,679 >> Module weights saved in models/ZeroShot/1/checkpoint-888/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:03,679 >> Configuration saved in models/ZeroShot/1/checkpoint-888/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:03,752 >> Module weights saved in models/ZeroShot/1/checkpoint-888/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:03,753 >> Configuration saved in models/ZeroShot/1/checkpoint-888/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:05,246 >> Module weights saved in models/ZeroShot/1/checkpoint-888/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:05,333 >> Configuration saved in models/ZeroShot/1/checkpoint-888/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:05,351 >> Module weights saved in models/ZeroShot/1/checkpoint-888/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:05,352 >> Configuration saved in models/ZeroShot/1/checkpoint-888/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:06,831 >> Module weights saved in models/ZeroShot/1/checkpoint-888/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:38:06,832 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-888/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:38:06,833 >> Special tokens file saved in models/ZeroShot/1/checkpoint-888/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:38:07,308 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-851] due to args.save_total_limit\n",
            "100% 925/925 [12:54<00:00,  1.94it/s][INFO|trainer.py:623] 2022-08-25 10:38:30,212 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:38:30,214 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:38:30,214 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:38:30,214 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.10it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.49it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.27it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.56it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.34it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.22it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.10it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.80it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.90it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.97it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.88it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.77it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.88it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.99it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.04it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.471353054046631, 'eval_accuracy': 0.540772557258606, 'eval_f1': 0.5295416375747741, 'eval_runtime': 4.9056, 'eval_samples_per_second': 94.994, 'eval_steps_per_second': 12.027, 'epoch': 25.0}\n",
            "100% 925/925 [12:59<00:00,  1.94it/s]\n",
            "100% 59/59 [00:04<00:00, 13.19it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:38:35,121 >> Saving model checkpoint to models/ZeroShot/1/checkpoint-925\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:35,122 >> Configuration saved in models/ZeroShot/1/checkpoint-925/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:35,216 >> Module weights saved in models/ZeroShot/1/checkpoint-925/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:35,216 >> Configuration saved in models/ZeroShot/1/checkpoint-925/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:35,224 >> Module weights saved in models/ZeroShot/1/checkpoint-925/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:35,225 >> Configuration saved in models/ZeroShot/1/checkpoint-925/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:35,277 >> Module weights saved in models/ZeroShot/1/checkpoint-925/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:35,278 >> Configuration saved in models/ZeroShot/1/checkpoint-925/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:35,348 >> Module weights saved in models/ZeroShot/1/checkpoint-925/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:35,349 >> Configuration saved in models/ZeroShot/1/checkpoint-925/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:36,834 >> Module weights saved in models/ZeroShot/1/checkpoint-925/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:36,962 >> Configuration saved in models/ZeroShot/1/checkpoint-925/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:36,975 >> Module weights saved in models/ZeroShot/1/checkpoint-925/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:36,976 >> Configuration saved in models/ZeroShot/1/checkpoint-925/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:38,301 >> Module weights saved in models/ZeroShot/1/checkpoint-925/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:38:38,301 >> tokenizer config file saved in models/ZeroShot/1/checkpoint-925/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:38:38,301 >> Special tokens file saved in models/ZeroShot/1/checkpoint-925/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:38:38,805 >> Deleting older checkpoint [models/ZeroShot/1/checkpoint-888] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 10:38:38,870 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 10:38:38,870 >> Loading best model from models/ZeroShot/1/checkpoint-148 (score: 0.5677918002907044).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 10:38:38,870 >> Could not locate the best model at models/ZeroShot/1/checkpoint-148/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 783.1549, 'train_samples_per_second': 37.157, 'train_steps_per_second': 1.181, 'train_loss': 0.08751504872296308, 'epoch': 25.0}\n",
            "100% 925/925 [13:03<00:00,  1.94it/s][INFO|trainer.py:238] 2022-08-25 10:38:38,872 >> Loading best adapter(s) from models/ZeroShot/1/checkpoint-148 (score: 0.5677918002907044).\n",
            "[INFO|loading.py:77] 2022-08-25 10:38:38,872 >> Loading module configuration from models/ZeroShot/1/checkpoint-148/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 10:38:38,878 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:38:39,554 >> Loading module weights from models/ZeroShot/1/checkpoint-148/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:38:39,576 >> Loading module configuration from models/ZeroShot/1/checkpoint-148/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 10:38:39,587 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 10:38:39,595 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:38:39,614 >> Loading module weights from models/ZeroShot/1/checkpoint-148/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:38:39,618 >> Loading module configuration from models/ZeroShot/1/checkpoint-148/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 10:38:39,619 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:38:39,780 >> Loading module weights from models/ZeroShot/1/checkpoint-148/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 10:38:39,797 >> No matching prediction head found in 'models/ZeroShot/1/checkpoint-148/en'\n",
            "[INFO|loading.py:77] 2022-08-25 10:38:39,798 >> Loading module configuration from models/ZeroShot/1/checkpoint-148/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 10:38:39,799 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:38:39,968 >> Loading module weights from models/ZeroShot/1/checkpoint-148/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:38:39,985 >> Loading module configuration from models/ZeroShot/1/checkpoint-148/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 10:38:39,986 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 10:38:41,186 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:38:43,011 >> Loading module weights from models/ZeroShot/1/checkpoint-148/pt/pytorch_model_head.bin\n",
            "100% 925/925 [13:07<00:00,  1.17it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 10:38:43,099 >> Saving model checkpoint to models/ZeroShot/1/\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:43,100 >> Configuration saved in models/ZeroShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:43,224 >> Module weights saved in models/ZeroShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:43,225 >> Configuration saved in models/ZeroShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:43,233 >> Module weights saved in models/ZeroShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:43,234 >> Configuration saved in models/ZeroShot/1/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:43,297 >> Module weights saved in models/ZeroShot/1/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:43,298 >> Configuration saved in models/ZeroShot/1/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:43,372 >> Module weights saved in models/ZeroShot/1/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:43,373 >> Configuration saved in models/ZeroShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:44,438 >> Module weights saved in models/ZeroShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:44,500 >> Configuration saved in models/ZeroShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:44,583 >> Module weights saved in models/ZeroShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:38:44,583 >> Configuration saved in models/ZeroShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:38:46,060 >> Module weights saved in models/ZeroShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:38:46,061 >> tokenizer config file saved in models/ZeroShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:38:46,061 >> Special tokens file saved in models/ZeroShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0875\n",
            "  train_runtime            = 0:13:03.15\n",
            "  train_samples            =       1164\n",
            "  train_samples_per_second =     37.157\n",
            "  train_steps_per_second   =      1.181\n",
            "08/25/2022 10:38:46 - INFO - __main__ - *** Evaluate ***\n",
            "\n",
            "\n",
            "Changing the language adapter to EN during evaluation..\n",
            "\n",
            "\n",
            "[INFO|trainer.py:623] 2022-08-25 10:38:46,278 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1. If sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:38:46,280 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:38:46,280 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:38:46,280 >>   Batch size = 8\n",
            "100% 59/59 [00:04<00:00, 12.56it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.5365\n",
            "  eval_f1                 =     0.5365\n",
            "  eval_loss               =     1.0958\n",
            "  eval_runtime            = 0:00:04.80\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =     96.891\n",
            "  eval_steps_per_second   =     12.267\n",
            "[INFO|modelcard.py:460] 2022-08-25 10:38:51,129 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5364806652069092}, {'name': 'F1', 'type': 'f1', 'value': 0.5364806866952789}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7Irn_YvIni"
      },
      "source": [
        "# One Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline EN : Monolingual BERT for one-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "Np_fganSL6oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Gurjel0rJHg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQO751yzvVJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27db37e-b087-4d55-f918-96ea3e644fae"
      },
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 25 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 00:13:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 00:13:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_00-13-19_93ca22c7e237,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 00:13:19 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "08/25/2022 00:13:19 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "08/25/2022 00:13:19 - WARNING - datasets.builder -   Using custom data configuration default-4f0417998fe57652\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-4f0417998fe57652/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 8439.24it/s]\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1367.56it/s]\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \rDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-4f0417998fe57652/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 366.88it/s]\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 00:13:20,139 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 00:13:20,146 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 00:13:20,400 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 00:13:20,401 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:13:21,186 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:13:21,186 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:13:21,186 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:13:21,186 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 00:13:21,187 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 00:13:21,315 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 00:13:21,315 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2022-08-25 00:13:21,554 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:2426] 2022-08-25 00:13:24,313 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2438] 2022-08-25 00:13:24,314 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  9.38ba/s]\n",
            "100% 1/1 [00:00<00:00, 18.28ba/s]\n",
            "08/25/2022 00:13:24 - INFO - __main__ -   Sample 550 of the training set: {'label': 0, 'sentence1': 'When in a current bull market it can be difficult to ask the question: when will the bull market end?', 'sentence2': 'bull market', 'input_ids': [101, 1332, 1107, 170, 1954, 12200, 2319, 1122, 1169, 1129, 2846, 1106, 2367, 1103, 2304, 131, 1165, 1209, 1103, 12200, 2319, 1322, 136, 102, 12200, 2319, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 00:13:24 - INFO - __main__ -   Sample 2331 of the training set: {'label': 1, 'sentence1': 'The Pain Killer consists of light and dark rums, pineapple and orange juices, cream of coconut and nutmeg — it’s blended, making it the ideal drink to sip on the beach at the lakefront resort.', 'sentence2': 'pain killer', 'input_ids': [101, 1109, 13304, 14613, 2923, 1104, 1609, 1105, 1843, 187, 17167, 117, 10194, 11478, 7136, 1105, 5925, 12362, 1116, 117, 7081, 1104, 20331, 1105, 22664, 3263, 1403, 783, 1122, 787, 188, 22357, 117, 1543, 1122, 1103, 7891, 3668, 1106, 11456, 1113, 1103, 4640, 1120, 1103, 3521, 11949, 8037, 119, 102, 2489, 7010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 00:13:24 - INFO - __main__ -   Sample 3286 of the training set: {'label': 1, 'sentence1': 'Renovations are scheduled to begin Monday for the P4 parking lot in Colonial Williamsburg, located behind the Merchants Square bus stop on South Henry Street, which will reopen as paid parking.', 'sentence2': 'parking lot', 'input_ids': [101, 19139, 22553, 1132, 4533, 1106, 3295, 6356, 1111, 1103, 153, 1527, 5030, 1974, 1107, 10319, 2902, 3410, 117, 1388, 1481, 1103, 17225, 1116, 4656, 3592, 1831, 1113, 1375, 1985, 1715, 117, 1134, 1209, 1231, 15622, 1179, 1112, 3004, 5030, 119, 102, 5030, 1974, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:723] 2022-08-25 00:13:28,829 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-08-25 00:13:28,837 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-08-25 00:13:28,837 >>   Num examples = 3414\n",
            "[INFO|trainer.py:1607] 2022-08-25 00:13:28,837 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1608] 2022-08-25 00:13:28,837 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1609] 2022-08-25 00:13:28,837 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1610] 2022-08-25 00:13:28,837 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-08-25 00:13:28,838 >>   Total optimization steps = 2675\n",
            "  4% 107/2675 [01:12<26:17,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:14:41,730 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:14:41,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:14:41,732 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:14:41,732 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.14it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:02, 17.71it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.74it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:02, 16.43it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.82it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.55it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.56it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.60it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.69it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.58it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.56it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.55it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.35it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.35it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.47it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.46it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.54it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.34it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.53it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.67it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.55it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.47it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.41it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.34it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.43it/s]\u001b[A\n",
            "{'eval_loss': 0.6145650744438171, 'eval_accuracy': 0.7918455004692078, 'eval_f1': 0.7762576289307865, 'eval_runtime': 3.7798, 'eval_samples_per_second': 123.288, 'eval_steps_per_second': 15.609, 'epoch': 1.0}\n",
            "\n",
            "  4% 107/2675 [01:16<26:17,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:14:45,513 >> Saving model checkpoint to models/OneShot/1/checkpoint-107\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:14:45,514 >> Configuration saved in models/OneShot/1/checkpoint-107/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:14:46,689 >> Model weights saved in models/OneShot/1/checkpoint-107/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:14:46,690 >> tokenizer config file saved in models/OneShot/1/checkpoint-107/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:14:46,690 >> Special tokens file saved in models/OneShot/1/checkpoint-107/special_tokens_map.json\n",
            "  8% 214/2675 [02:33<25:17,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:16:01,898 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:16:01,900 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:16:01,900 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:16:01,900 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.97it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.43it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.51it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.14it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.59it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.42it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.39it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.07it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.97it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.15it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.35it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.30it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.24it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "{'eval_loss': 0.5256999731063843, 'eval_accuracy': 0.8476395010948181, 'eval_f1': 0.8442242049012454, 'eval_runtime': 3.8347, 'eval_samples_per_second': 121.522, 'eval_steps_per_second': 15.386, 'epoch': 2.0}\n",
            "\n",
            "  8% 214/2675 [02:36<25:17,  1.62it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:16:05,736 >> Saving model checkpoint to models/OneShot/1/checkpoint-214\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:16:05,737 >> Configuration saved in models/OneShot/1/checkpoint-214/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:16:06,819 >> Model weights saved in models/OneShot/1/checkpoint-214/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:16:06,820 >> tokenizer config file saved in models/OneShot/1/checkpoint-214/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:16:06,820 >> Special tokens file saved in models/OneShot/1/checkpoint-214/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:16:10,245 >> Deleting older checkpoint [models/OneShot/1/checkpoint-107] due to args.save_total_limit\n",
            " 12% 321/2675 [03:53<24:06,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:17:22,305 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:17:22,306 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:17:22,307 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:17:22,307 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.89it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.19it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.52it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.01it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.56it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.39it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.06it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.19it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.98it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.91it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.34it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            "{'eval_loss': 0.7312865853309631, 'eval_accuracy': 0.8283261656761169, 'eval_f1': 0.8123426960636263, 'eval_runtime': 3.8417, 'eval_samples_per_second': 121.301, 'eval_steps_per_second': 15.358, 'epoch': 3.0}\n",
            "\n",
            " 12% 321/2675 [03:57<24:06,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:17:26,149 >> Saving model checkpoint to models/OneShot/1/checkpoint-321\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:17:26,150 >> Configuration saved in models/OneShot/1/checkpoint-321/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:17:27,129 >> Model weights saved in models/OneShot/1/checkpoint-321/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:17:27,129 >> tokenizer config file saved in models/OneShot/1/checkpoint-321/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:17:27,129 >> Special tokens file saved in models/OneShot/1/checkpoint-321/special_tokens_map.json\n",
            " 16% 428/2675 [05:13<23:03,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:18:42,543 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:18:42,545 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:18:42,545 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:18:42,545 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.50it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.15it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.38it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.14it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.67it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.46it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.47it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.56it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.55it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.07it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.09it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.98it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.00it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.05it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.15it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.13it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.32it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.01it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7295284867286682, 'eval_accuracy': 0.8369098901748657, 'eval_f1': 0.8261364752086402, 'eval_runtime': 3.8499, 'eval_samples_per_second': 121.043, 'eval_steps_per_second': 15.325, 'epoch': 4.0}\n",
            " 16% 428/2675 [05:17<23:03,  1.62it/s]\n",
            "100% 59/59 [00:03<00:00, 15.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:18:46,396 >> Saving model checkpoint to models/OneShot/1/checkpoint-428\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:18:46,397 >> Configuration saved in models/OneShot/1/checkpoint-428/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:18:47,472 >> Model weights saved in models/OneShot/1/checkpoint-428/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:18:47,472 >> tokenizer config file saved in models/OneShot/1/checkpoint-428/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:18:47,472 >> Special tokens file saved in models/OneShot/1/checkpoint-428/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:18:50,895 >> Deleting older checkpoint [models/OneShot/1/checkpoint-321] due to args.save_total_limit\n",
            "{'loss': 0.1216, 'learning_rate': 1.6261682242990654e-05, 'epoch': 4.67}\n",
            " 20% 535/2675 [06:34<21:56,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:20:03,167 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:20:03,169 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:20:03,169 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:20:03,169 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.07it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.27it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.40it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.05it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.60it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.24it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.34it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.36it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.03it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.90it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.07it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.08it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.03it/s]\u001b[A\n",
            "{'eval_loss': 0.7280852198600769, 'eval_accuracy': 0.8562231659889221, 'eval_f1': 0.8430511081731855, 'eval_runtime': 3.8433, 'eval_samples_per_second': 121.251, 'eval_steps_per_second': 15.352, 'epoch': 5.0}\n",
            "\n",
            " 20% 535/2675 [06:38<21:56,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:20:07,014 >> Saving model checkpoint to models/OneShot/1/checkpoint-535\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:20:07,014 >> Configuration saved in models/OneShot/1/checkpoint-535/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:20:08,001 >> Model weights saved in models/OneShot/1/checkpoint-535/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:20:08,001 >> tokenizer config file saved in models/OneShot/1/checkpoint-535/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:20:08,002 >> Special tokens file saved in models/OneShot/1/checkpoint-535/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:20:11,472 >> Deleting older checkpoint [models/OneShot/1/checkpoint-428] due to args.save_total_limit\n",
            " 24% 642/2675 [07:54<20:57,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:21:23,494 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:21:23,496 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:21:23,496 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:21:23,496 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.38it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.85it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.22it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.84it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.46it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.31it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.46it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.58it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.15it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.41it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.39it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.40it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.44it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "{'eval_loss': 0.7333617806434631, 'eval_accuracy': 0.8519313335418701, 'eval_f1': 0.8430248736788146, 'eval_runtime': 3.8339, 'eval_samples_per_second': 121.547, 'eval_steps_per_second': 15.389, 'epoch': 6.0}\n",
            "\n",
            " 24% 642/2675 [07:58<20:57,  1.62it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:21:27,332 >> Saving model checkpoint to models/OneShot/1/checkpoint-642\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:21:27,332 >> Configuration saved in models/OneShot/1/checkpoint-642/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:21:28,350 >> Model weights saved in models/OneShot/1/checkpoint-642/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:21:28,351 >> tokenizer config file saved in models/OneShot/1/checkpoint-642/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:21:28,351 >> Special tokens file saved in models/OneShot/1/checkpoint-642/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:21:31,863 >> Deleting older checkpoint [models/OneShot/1/checkpoint-535] due to args.save_total_limit\n",
            " 28% 749/2675 [09:15<19:43,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:22:43,903 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:22:43,905 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:22:43,905 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:22:43,905 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.43it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.05it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.19it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.93it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.37it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.19it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.10it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.29it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.35it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.23it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.27it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.38it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.33it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.33it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7954139709472656, 'eval_accuracy': 0.8712446093559265, 'eval_f1': 0.8627393225331369, 'eval_runtime': 3.8344, 'eval_samples_per_second': 121.531, 'eval_steps_per_second': 15.387, 'epoch': 7.0}\n",
            " 28% 749/2675 [09:18<19:43,  1.63it/s]\n",
            "100% 59/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:22:47,741 >> Saving model checkpoint to models/OneShot/1/checkpoint-749\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:22:47,741 >> Configuration saved in models/OneShot/1/checkpoint-749/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:22:48,755 >> Model weights saved in models/OneShot/1/checkpoint-749/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:22:48,756 >> tokenizer config file saved in models/OneShot/1/checkpoint-749/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:22:48,756 >> Special tokens file saved in models/OneShot/1/checkpoint-749/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:22:52,218 >> Deleting older checkpoint [models/OneShot/1/checkpoint-214] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:22:52,237 >> Deleting older checkpoint [models/OneShot/1/checkpoint-642] due to args.save_total_limit\n",
            " 32% 856/2675 [10:35<18:37,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:24:04,312 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:24:04,313 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:24:04,314 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:24:04,314 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.58it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.21it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.33it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.94it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.47it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.36it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.52it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.30it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.12it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.98it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.24it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.40it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.40it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.36it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.09it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.15it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            "{'eval_loss': 0.822927713394165, 'eval_accuracy': 0.8690987229347229, 'eval_f1': 0.8620974040779892, 'eval_runtime': 3.8377, 'eval_samples_per_second': 121.427, 'eval_steps_per_second': 15.374, 'epoch': 8.0}\n",
            "\n",
            " 32% 856/2675 [10:39<18:37,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:24:08,153 >> Saving model checkpoint to models/OneShot/1/checkpoint-856\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:24:08,154 >> Configuration saved in models/OneShot/1/checkpoint-856/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:24:09,156 >> Model weights saved in models/OneShot/1/checkpoint-856/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:24:09,157 >> tokenizer config file saved in models/OneShot/1/checkpoint-856/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:24:09,157 >> Special tokens file saved in models/OneShot/1/checkpoint-856/special_tokens_map.json\n",
            " 36% 963/2675 [11:55<17:29,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:25:24,594 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:25:24,596 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:25:24,596 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:25:24,596 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.58it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.09it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.39it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.96it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.58it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.37it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.46it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.10it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.17it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.01it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.96it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.13it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.33it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            "{'eval_loss': 0.8541198372840881, 'eval_accuracy': 0.8690987229347229, 'eval_f1': 0.8609207043500808, 'eval_runtime': 3.8412, 'eval_samples_per_second': 121.318, 'eval_steps_per_second': 15.36, 'epoch': 9.0}\n",
            "\n",
            " 36% 963/2675 [11:59<17:29,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:25:28,439 >> Saving model checkpoint to models/OneShot/1/checkpoint-963\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:25:28,440 >> Configuration saved in models/OneShot/1/checkpoint-963/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:25:29,602 >> Model weights saved in models/OneShot/1/checkpoint-963/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:25:29,603 >> tokenizer config file saved in models/OneShot/1/checkpoint-963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:25:29,603 >> Special tokens file saved in models/OneShot/1/checkpoint-963/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:25:33,095 >> Deleting older checkpoint [models/OneShot/1/checkpoint-856] due to args.save_total_limit\n",
            "{'loss': 0.0035, 'learning_rate': 1.2523364485981309e-05, 'epoch': 9.35}\n",
            " 40% 1070/2675 [13:16<16:26,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:26:45,280 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:26:45,282 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:26:45,282 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:26:45,282 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.80it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.16it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.49it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.04it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.54it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.27it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.01it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.06it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.03it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.97it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9258242249488831, 'eval_accuracy': 0.8540772795677185, 'eval_f1': 0.8489321128909231, 'eval_runtime': 3.848, 'eval_samples_per_second': 121.101, 'eval_steps_per_second': 15.333, 'epoch': 10.0}\n",
            " 40% 1070/2675 [13:20<16:26,  1.63it/s]\n",
            "100% 59/59 [00:03<00:00, 15.08it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:26:49,132 >> Saving model checkpoint to models/OneShot/1/checkpoint-1070\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:26:49,132 >> Configuration saved in models/OneShot/1/checkpoint-1070/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:26:50,169 >> Model weights saved in models/OneShot/1/checkpoint-1070/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:26:50,169 >> tokenizer config file saved in models/OneShot/1/checkpoint-1070/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:26:50,170 >> Special tokens file saved in models/OneShot/1/checkpoint-1070/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:26:53,708 >> Deleting older checkpoint [models/OneShot/1/checkpoint-963] due to args.save_total_limit\n",
            " 44% 1177/2675 [14:37<15:22,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:28:05,988 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:28:05,990 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:28:05,990 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:28:05,990 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.98it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.20it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.22it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.99it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.56it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.35it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.58it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.54it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.03it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.01it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.36it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.08it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.09it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.39it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            "{'eval_loss': 0.9534684419631958, 'eval_accuracy': 0.8476395010948181, 'eval_f1': 0.8418715798026143, 'eval_runtime': 3.8371, 'eval_samples_per_second': 121.446, 'eval_steps_per_second': 15.376, 'epoch': 11.0}\n",
            "\n",
            " 44% 1177/2675 [14:40<15:22,  1.62it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:28:09,829 >> Saving model checkpoint to models/OneShot/1/checkpoint-1177\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:28:09,829 >> Configuration saved in models/OneShot/1/checkpoint-1177/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:28:10,845 >> Model weights saved in models/OneShot/1/checkpoint-1177/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:28:10,846 >> tokenizer config file saved in models/OneShot/1/checkpoint-1177/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:28:10,846 >> Special tokens file saved in models/OneShot/1/checkpoint-1177/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:28:14,335 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1070] due to args.save_total_limit\n",
            " 48% 1284/2675 [15:57<14:16,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:29:26,439 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:29:26,441 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:29:26,441 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:29:26,441 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.23it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.32it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.54it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.98it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.45it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.36it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.41it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.29it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.28it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.92it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 14.99it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.37it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.20it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.39it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "{'eval_loss': 0.9638877511024475, 'eval_accuracy': 0.8648068904876709, 'eval_f1': 0.8560395427865307, 'eval_runtime': 3.8396, 'eval_samples_per_second': 121.367, 'eval_steps_per_second': 15.366, 'epoch': 12.0}\n",
            "\n",
            " 48% 1284/2675 [16:01<14:16,  1.62it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:29:30,282 >> Saving model checkpoint to models/OneShot/1/checkpoint-1284\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:29:30,283 >> Configuration saved in models/OneShot/1/checkpoint-1284/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:29:31,414 >> Model weights saved in models/OneShot/1/checkpoint-1284/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:29:31,415 >> tokenizer config file saved in models/OneShot/1/checkpoint-1284/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:29:31,415 >> Special tokens file saved in models/OneShot/1/checkpoint-1284/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:29:34,889 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1177] due to args.save_total_limit\n",
            " 52% 1391/2675 [17:17<13:08,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:30:46,849 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:30:46,851 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:30:46,851 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:30:46,851 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.19it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.18it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.47it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.95it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.50it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.30it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.42it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.06it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.05it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.06it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.06it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            "{'eval_loss': 1.0153790712356567, 'eval_accuracy': 0.8519313335418701, 'eval_f1': 0.8423290230519147, 'eval_runtime': 3.841, 'eval_samples_per_second': 121.321, 'eval_steps_per_second': 15.36, 'epoch': 13.0}\n",
            "\n",
            " 52% 1391/2675 [17:21<13:08,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:30:50,693 >> Saving model checkpoint to models/OneShot/1/checkpoint-1391\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:30:50,694 >> Configuration saved in models/OneShot/1/checkpoint-1391/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:30:51,685 >> Model weights saved in models/OneShot/1/checkpoint-1391/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:30:51,686 >> tokenizer config file saved in models/OneShot/1/checkpoint-1391/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:30:51,686 >> Special tokens file saved in models/OneShot/1/checkpoint-1391/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:30:55,277 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1284] due to args.save_total_limit\n",
            " 56% 1498/2675 [18:38<12:03,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:32:07,251 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:32:07,253 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:32:07,253 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:32:07,253 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.52it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.29it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.27it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.00it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.53it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.48it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.62it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.55it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.13it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.13it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.40it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.29it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.09it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            "{'eval_loss': 1.0286105871200562, 'eval_accuracy': 0.8540772795677185, 'eval_f1': 0.8467342516638292, 'eval_runtime': 3.8316, 'eval_samples_per_second': 121.62, 'eval_steps_per_second': 15.398, 'epoch': 14.0}\n",
            "\n",
            " 56% 1498/2675 [18:42<12:03,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:32:11,086 >> Saving model checkpoint to models/OneShot/1/checkpoint-1498\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:32:11,087 >> Configuration saved in models/OneShot/1/checkpoint-1498/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:32:12,164 >> Model weights saved in models/OneShot/1/checkpoint-1498/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:32:12,165 >> tokenizer config file saved in models/OneShot/1/checkpoint-1498/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:32:12,165 >> Special tokens file saved in models/OneShot/1/checkpoint-1498/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:32:15,697 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1391] due to args.save_total_limit\n",
            "{'loss': 0.0027, 'learning_rate': 8.785046728971963e-06, 'epoch': 14.02}\n",
            " 60% 1605/2675 [19:58<10:57,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:33:27,730 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:33:27,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:33:27,732 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:33:27,732 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.20it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.20it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.37it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.06it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.58it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.41it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.42it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.46it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.26it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.05it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.15it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.09it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.16it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            "{'eval_loss': 1.032309651374817, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8524418463037389, 'eval_runtime': 3.8422, 'eval_samples_per_second': 121.285, 'eval_steps_per_second': 15.356, 'epoch': 15.0}\n",
            "\n",
            " 60% 1605/2675 [20:02<10:57,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:33:31,576 >> Saving model checkpoint to models/OneShot/1/checkpoint-1605\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:33:31,577 >> Configuration saved in models/OneShot/1/checkpoint-1605/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:33:32,609 >> Model weights saved in models/OneShot/1/checkpoint-1605/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:33:32,609 >> tokenizer config file saved in models/OneShot/1/checkpoint-1605/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:33:32,610 >> Special tokens file saved in models/OneShot/1/checkpoint-1605/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:33:36,146 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1498] due to args.save_total_limit\n",
            " 64% 1712/2675 [21:19<09:51,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:34:48,251 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:34:48,253 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:34:48,253 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:34:48,253 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.44it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.03it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.35it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.93it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.53it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.38it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.33it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.23it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.07it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.14it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.27it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.26it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.20it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.19it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.22it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0156781673431396, 'eval_accuracy': 0.8669527769088745, 'eval_f1': 0.8584835423197492, 'eval_runtime': 3.8365, 'eval_samples_per_second': 121.466, 'eval_steps_per_second': 15.379, 'epoch': 16.0}\n",
            " 64% 1712/2675 [21:23<09:51,  1.63it/s]\n",
            "100% 59/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:34:52,090 >> Saving model checkpoint to models/OneShot/1/checkpoint-1712\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:34:52,091 >> Configuration saved in models/OneShot/1/checkpoint-1712/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:34:53,159 >> Model weights saved in models/OneShot/1/checkpoint-1712/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:34:53,160 >> tokenizer config file saved in models/OneShot/1/checkpoint-1712/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:34:53,160 >> Special tokens file saved in models/OneShot/1/checkpoint-1712/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:34:56,669 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1605] due to args.save_total_limit\n",
            " 68% 1819/2675 [22:39<08:46,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:36:08,828 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:36:08,830 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:36:08,830 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:36:08,830 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.35it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.13it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.34it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.88it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.36it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.09it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.53it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.52it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.41it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.59it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.15it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.35it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.48it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.39it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.22it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.01it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.13it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.15it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            "{'eval_loss': 1.031002163887024, 'eval_accuracy': 0.8626609444618225, 'eval_f1': 0.8554618590675583, 'eval_runtime': 3.8286, 'eval_samples_per_second': 121.715, 'eval_steps_per_second': 15.41, 'epoch': 17.0}\n",
            "\n",
            " 68% 1819/2675 [22:43<08:46,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:36:12,660 >> Saving model checkpoint to models/OneShot/1/checkpoint-1819\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:36:12,661 >> Configuration saved in models/OneShot/1/checkpoint-1819/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:36:13,676 >> Model weights saved in models/OneShot/1/checkpoint-1819/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:36:13,677 >> tokenizer config file saved in models/OneShot/1/checkpoint-1819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:36:13,677 >> Special tokens file saved in models/OneShot/1/checkpoint-1819/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:36:17,228 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1712] due to args.save_total_limit\n",
            " 72% 1926/2675 [24:00<07:41,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:37:29,465 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:37:29,467 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:37:29,467 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:37:29,467 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.00it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.30it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.43it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.88it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.50it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.18it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.53it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.59it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.45it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.24it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.06it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.06it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.37it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.34it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.12it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.09it/s]\u001b[A\n",
            "{'eval_loss': 1.035143256187439, 'eval_accuracy': 0.8690987229347229, 'eval_f1': 0.8609207043500808, 'eval_runtime': 3.8361, 'eval_samples_per_second': 121.478, 'eval_steps_per_second': 15.38, 'epoch': 18.0}\n",
            "\n",
            " 72% 1926/2675 [24:04<07:41,  1.62it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:37:33,305 >> Saving model checkpoint to models/OneShot/1/checkpoint-1926\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:37:33,306 >> Configuration saved in models/OneShot/1/checkpoint-1926/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:37:34,440 >> Model weights saved in models/OneShot/1/checkpoint-1926/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:37:34,441 >> tokenizer config file saved in models/OneShot/1/checkpoint-1926/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:37:34,441 >> Special tokens file saved in models/OneShot/1/checkpoint-1926/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:37:37,922 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1819] due to args.save_total_limit\n",
            "{'loss': 0.0008, 'learning_rate': 5.046728971962617e-06, 'epoch': 18.69}\n",
            " 76% 2033/2675 [25:21<06:34,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:38:50,008 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:38:50,010 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:38:50,010 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:38:50,010 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.60it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.13it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.44it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.96it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.50it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.29it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.43it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.51it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.42it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.95it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.03it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.07it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.11it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.25it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.26it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.25it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0635933876037598, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8489389821210384, 'eval_runtime': 3.8503, 'eval_samples_per_second': 121.03, 'eval_steps_per_second': 15.324, 'epoch': 19.0}\n",
            " 76% 2033/2675 [25:25<06:34,  1.63it/s]\n",
            "100% 59/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:38:53,861 >> Saving model checkpoint to models/OneShot/1/checkpoint-2033\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:38:53,862 >> Configuration saved in models/OneShot/1/checkpoint-2033/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:38:54,860 >> Model weights saved in models/OneShot/1/checkpoint-2033/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:38:54,861 >> tokenizer config file saved in models/OneShot/1/checkpoint-2033/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:38:54,861 >> Special tokens file saved in models/OneShot/1/checkpoint-2033/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:38:58,440 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1926] due to args.save_total_limit\n",
            " 80% 2140/2675 [26:41<05:28,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:40:10,398 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:40:10,400 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:40:10,400 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:40:10,400 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.04it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 16.95it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.26it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.01it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.46it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.08it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.38it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.62it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.61it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.50it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.27it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.08it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.31it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.03it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.98it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.06it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.16it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.20it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            "{'eval_loss': 1.0542094707489014, 'eval_accuracy': 0.8583691120147705, 'eval_f1': 0.850329939851673, 'eval_runtime': 3.8434, 'eval_samples_per_second': 121.246, 'eval_steps_per_second': 15.351, 'epoch': 20.0}\n",
            "\n",
            " 80% 2140/2675 [26:45<05:28,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:40:14,245 >> Saving model checkpoint to models/OneShot/1/checkpoint-2140\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:40:14,247 >> Configuration saved in models/OneShot/1/checkpoint-2140/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:40:15,311 >> Model weights saved in models/OneShot/1/checkpoint-2140/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:40:15,312 >> tokenizer config file saved in models/OneShot/1/checkpoint-2140/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:40:15,312 >> Special tokens file saved in models/OneShot/1/checkpoint-2140/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:40:18,916 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2033] due to args.save_total_limit\n",
            " 84% 2247/2675 [28:01<04:24,  1.62it/s][INFO|trainer.py:723] 2022-08-25 00:41:30,816 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:41:30,818 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:41:30,818 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:41:30,818 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.00it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.38it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.64it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.07it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.66it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.32it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.33it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.47it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.21it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.11it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.01it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.12it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.15it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.97it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.11it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.14it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.22it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.10it/s]\u001b[A\n",
            "{'eval_loss': 1.0615886449813843, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8527517294015274, 'eval_runtime': 3.843, 'eval_samples_per_second': 121.259, 'eval_steps_per_second': 15.353, 'epoch': 21.0}\n",
            "\n",
            " 84% 2247/2675 [28:05<04:24,  1.62it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:41:34,662 >> Saving model checkpoint to models/OneShot/1/checkpoint-2247\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:41:34,663 >> Configuration saved in models/OneShot/1/checkpoint-2247/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:41:35,680 >> Model weights saved in models/OneShot/1/checkpoint-2247/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:41:35,681 >> tokenizer config file saved in models/OneShot/1/checkpoint-2247/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:41:35,681 >> Special tokens file saved in models/OneShot/1/checkpoint-2247/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:41:39,189 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2140] due to args.save_total_limit\n",
            " 88% 2354/2675 [29:22<03:17,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:42:51,349 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:42:51,351 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:42:51,351 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:42:51,351 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 22.07it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.39it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.49it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.85it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.46it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.32it/s]\u001b[A\n",
            " 27% 16/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.40it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.44it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.35it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.36it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.17it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.25it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.17it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.25it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.23it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.16it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 14.98it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.99it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.05it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.28it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.12it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.14it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.04it/s]\u001b[A\n",
            "{'eval_loss': 1.0680363178253174, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8527517294015274, 'eval_runtime': 3.8479, 'eval_samples_per_second': 121.106, 'eval_steps_per_second': 15.333, 'epoch': 22.0}\n",
            "\n",
            " 88% 2354/2675 [29:26<03:17,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:42:55,201 >> Saving model checkpoint to models/OneShot/1/checkpoint-2354\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:42:55,202 >> Configuration saved in models/OneShot/1/checkpoint-2354/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:42:56,312 >> Model weights saved in models/OneShot/1/checkpoint-2354/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:42:56,312 >> tokenizer config file saved in models/OneShot/1/checkpoint-2354/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:42:56,313 >> Special tokens file saved in models/OneShot/1/checkpoint-2354/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:42:59,844 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2247] due to args.save_total_limit\n",
            " 92% 2461/2675 [30:43<02:11,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:44:11,978 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:44:11,980 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:44:11,980 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:44:11,980 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.74it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.11it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.30it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 15.96it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.61it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.45it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.45it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.42it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.14it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.06it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.04it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 14.98it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.04it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.13it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.28it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.21it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.24it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.21it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.28it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.24it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.23it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.19it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.17it/s]\u001b[A\n",
            "{'eval_loss': 1.072234869003296, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8527517294015274, 'eval_runtime': 3.8434, 'eval_samples_per_second': 121.245, 'eval_steps_per_second': 15.351, 'epoch': 23.0}\n",
            "\n",
            " 92% 2461/2675 [30:46<02:11,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:44:15,825 >> Saving model checkpoint to models/OneShot/1/checkpoint-2461\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:44:15,826 >> Configuration saved in models/OneShot/1/checkpoint-2461/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:44:16,849 >> Model weights saved in models/OneShot/1/checkpoint-2461/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:44:16,850 >> tokenizer config file saved in models/OneShot/1/checkpoint-2461/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:44:16,850 >> Special tokens file saved in models/OneShot/1/checkpoint-2461/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:44:20,351 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2354] due to args.save_total_limit\n",
            "{'loss': 0.0001, 'learning_rate': 1.308411214953271e-06, 'epoch': 23.36}\n",
            " 96% 2568/2675 [32:03<01:05,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:45:32,572 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:45:32,575 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:45:32,575 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:45:32,575 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.80it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.18it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.51it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.03it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:03, 15.55it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.40it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.42it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.22it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.31it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.37it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.19it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.08it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.15it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.17it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.19it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.18it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.14it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.10it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:01, 14.99it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.23it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.31it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.43it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.32it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.27it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.21it/s]\u001b[A\n",
            "{'eval_loss': 1.0759996175765991, 'eval_accuracy': 0.8583691120147705, 'eval_f1': 0.850329939851673, 'eval_runtime': 3.8379, 'eval_samples_per_second': 121.421, 'eval_steps_per_second': 15.373, 'epoch': 24.0}\n",
            "\n",
            " 96% 2568/2675 [32:07<01:05,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:45:36,414 >> Saving model checkpoint to models/OneShot/1/checkpoint-2568\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:45:36,415 >> Configuration saved in models/OneShot/1/checkpoint-2568/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:45:37,490 >> Model weights saved in models/OneShot/1/checkpoint-2568/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:45:37,491 >> tokenizer config file saved in models/OneShot/1/checkpoint-2568/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:45:37,491 >> Special tokens file saved in models/OneShot/1/checkpoint-2568/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:45:41,025 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2461] due to args.save_total_limit\n",
            "100% 2675/2675 [33:24<00:00,  1.63it/s][INFO|trainer.py:723] 2022-08-25 00:46:53,244 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:46:53,246 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:46:53,246 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:46:53,246 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 21.76it/s]\u001b[A\n",
            " 10% 6/59 [00:00<00:03, 17.14it/s]\u001b[A\n",
            " 14% 8/59 [00:00<00:03, 16.50it/s]\u001b[A\n",
            " 17% 10/59 [00:00<00:03, 16.03it/s]\u001b[A\n",
            " 20% 12/59 [00:00<00:02, 15.74it/s]\u001b[A\n",
            " 24% 14/59 [00:00<00:02, 15.40it/s]\u001b[A\n",
            " 27% 16/59 [00:00<00:02, 15.39it/s]\u001b[A\n",
            " 31% 18/59 [00:01<00:02, 15.39it/s]\u001b[A\n",
            " 34% 20/59 [00:01<00:02, 15.41it/s]\u001b[A\n",
            " 37% 22/59 [00:01<00:02, 15.32it/s]\u001b[A\n",
            " 41% 24/59 [00:01<00:02, 15.34it/s]\u001b[A\n",
            " 44% 26/59 [00:01<00:02, 15.18it/s]\u001b[A\n",
            " 47% 28/59 [00:01<00:02, 15.20it/s]\u001b[A\n",
            " 51% 30/59 [00:01<00:01, 15.11it/s]\u001b[A\n",
            " 54% 32/59 [00:02<00:01, 15.32it/s]\u001b[A\n",
            " 58% 34/59 [00:02<00:01, 15.33it/s]\u001b[A\n",
            " 61% 36/59 [00:02<00:01, 15.34it/s]\u001b[A\n",
            " 64% 38/59 [00:02<00:01, 15.52it/s]\u001b[A\n",
            " 68% 40/59 [00:02<00:01, 15.34it/s]\u001b[A\n",
            " 71% 42/59 [00:02<00:01, 15.39it/s]\u001b[A\n",
            " 75% 44/59 [00:02<00:00, 15.22it/s]\u001b[A\n",
            " 78% 46/59 [00:02<00:00, 15.29it/s]\u001b[A\n",
            " 81% 48/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 85% 50/59 [00:03<00:00, 15.30it/s]\u001b[A\n",
            " 88% 52/59 [00:03<00:00, 15.44it/s]\u001b[A\n",
            " 92% 54/59 [00:03<00:00, 15.36it/s]\u001b[A\n",
            " 95% 56/59 [00:03<00:00, 15.29it/s]\u001b[A\n",
            " 98% 58/59 [00:03<00:00, 15.18it/s]\u001b[A\n",
            "{'eval_loss': 1.077115774154663, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8527517294015274, 'eval_runtime': 3.8202, 'eval_samples_per_second': 121.982, 'eval_steps_per_second': 15.444, 'epoch': 25.0}\n",
            "\n",
            "100% 2675/2675 [33:28<00:00,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 00:46:57,068 >> Saving model checkpoint to models/OneShot/1/checkpoint-2675\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:46:57,069 >> Configuration saved in models/OneShot/1/checkpoint-2675/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:46:58,089 >> Model weights saved in models/OneShot/1/checkpoint-2675/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:46:58,090 >> tokenizer config file saved in models/OneShot/1/checkpoint-2675/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:46:58,090 >> Special tokens file saved in models/OneShot/1/checkpoint-2675/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 00:47:01,647 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2568] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2022-08-25 00:47:01,807 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1942] 2022-08-25 00:47:01,808 >> Loading best model from models/OneShot/1/checkpoint-749 (score: 0.8627393225331369).\n",
            "{'train_runtime': 2015.2885, 'train_samples_per_second': 42.351, 'train_steps_per_second': 1.327, 'train_loss': 0.024066307666215384, 'epoch': 25.0}\n",
            "100% 2675/2675 [33:35<00:00,  1.33it/s]\n",
            "[INFO|trainer.py:2640] 2022-08-25 00:47:04,128 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 00:47:04,129 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 00:47:05,212 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 00:47:05,213 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 00:47:05,213 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0241\n",
            "  train_runtime            = 0:33:35.28\n",
            "  train_samples            =       3414\n",
            "  train_samples_per_second =     42.351\n",
            "  train_steps_per_second   =      1.327\n",
            "08/25/2022 00:47:05 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:723] 2022-08-25 00:47:05,252 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 00:47:05,254 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 00:47:05,254 >>   Num examples = 466\n",
            "[INFO|trainer.py:2896] 2022-08-25 00:47:05,254 >>   Batch size = 8\n",
            "100% 59/59 [00:03<00:00, 16.10it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.8712\n",
            "  eval_f1                 =     0.8627\n",
            "  eval_loss               =     0.7954\n",
            "  eval_runtime            = 0:00:03.74\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    124.284\n",
            "  eval_steps_per_second   =     15.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline PT : Monolingual BERT for one-shot portuguese idiomaticity detection"
      ],
      "metadata": {
        "id": "HlsdRUjAGAMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Portuguese language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 25 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "id": "7G_xky56FjRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d813c5-2868-4f5d-afb5-7898cd8e0600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 01:05:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 01:05:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_01-05-26_93ca22c7e237,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 01:05:26 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "08/25/2022 01:05:26 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "08/25/2022 01:05:27 - WARNING - datasets.builder -   Using custom data configuration default-bf3d9e40ae93e3e2\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-bf3d9e40ae93e3e2/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 8962.19it/s]\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1284.43it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-bf3d9e40ae93e3e2/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 354.29it/s]\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 01:05:27,278 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 01:05:27,285 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 01:05:27,548 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 01:05:27,549 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 01:05:28,296 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 01:05:28,296 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 01:05:28,296 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 01:05:28,296 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 01:05:28,296 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 01:05:28,429 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 01:05:28,429 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2022-08-25 01:05:28,669 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:2426] 2022-08-25 01:05:31,688 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2438] 2022-08-25 01:05:31,688 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 2/2 [00:00<00:00, 11.17ba/s]\n",
            "100% 1/1 [00:00<00:00, 24.18ba/s]\n",
            "08/25/2022 01:05:31 - INFO - __main__ -   Sample 275 of the training set: {'label': 0, 'sentence1': 'Já o Boeing 787-9 que a Lufthansa encomendou cerca de 20 aeronaves tem programada entrega para 2022, coincidindo com a previsão da Lufthansa em lançar a nova classe executiva.', 'sentence2': 'classe executiva', 'input_ids': [101, 147, 5589, 184, 10178, 5603, 1559, 118, 130, 15027, 170, 14557, 4964, 3822, 3202, 4035, 19191, 13645, 1358, 172, 1200, 2599, 1260, 1406, 170, 28032, 13799, 21359, 1306, 1788, 7971, 4035, 7877, 2571, 18311, 17881, 1477, 117, 9584, 16388, 24704, 1186, 3254, 170, 3073, 9356, 9290, 5358, 14557, 4964, 3822, 3202, 9712, 2495, 1179, 15331, 1197, 170, 1185, 2497, 1705, 1162, 4252, 10294, 16065, 2497, 119, 102, 1705, 1162, 4252, 10294, 16065, 2497, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 01:05:31 - INFO - __main__ -   Sample 1165 of the training set: {'label': 0, 'sentence1': 'Assim, Bibi continuará ao lado marido trambiqueiro, sem saber que seu verdadeiro príncipe encantado é o advogado.', 'sentence2': 'príncipe encantado', 'input_ids': [101, 1249, 5053, 1306, 117, 139, 21883, 14255, 6105, 6718, 1197, 5589, 170, 1186, 19122, 1186, 12477, 10132, 1186, 14172, 5567, 3530, 9992, 117, 14516, 1306, 21718, 3169, 15027, 14516, 1358, 1396, 18484, 2007, 9992, 185, 1197, 7326, 6617, 3186, 4035, 7804, 18518, 1186, 255, 184, 8050, 6005, 23224, 119, 102, 185, 1197, 7326, 6617, 3186, 4035, 7804, 18518, 1186, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 01:05:31 - INFO - __main__ -   Sample 129 of the training set: {'label': 0, 'sentence1': 'O amigo secreto parece nunca sair de moda entre os brasileiros.', 'sentence2': 'amigo secreto', 'input_ids': [101, 152, 1821, 11466, 3318, 1186, 14247, 10294, 1162, 22108, 2599, 21718, 3161, 1260, 182, 16848, 4035, 7877, 184, 1116, 12418, 5053, 1513, 9992, 1116, 119, 102, 1821, 11466, 3318, 1186, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:723] 2022-08-25 01:05:35,911 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-08-25 01:05:35,920 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-08-25 01:05:35,920 >>   Num examples = 1217\n",
            "[INFO|trainer.py:1607] 2022-08-25 01:05:35,920 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1608] 2022-08-25 01:05:35,920 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1609] 2022-08-25 01:05:35,920 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1610] 2022-08-25 01:05:35,920 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-08-25 01:05:35,920 >>   Total optimization steps = 975\n",
            "  4% 38/975 [00:26<11:00,  1.42it/s][INFO|trainer.py:723] 2022-08-25 01:06:02,673 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:06:02,675 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:06:02,675 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:06:02,675 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.55it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.60it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.84it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.31it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 14.76it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.51it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.52it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.63it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.73it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.63it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.55it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.27it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.24it/s]\u001b[A\n",
            " 86% 30/35 [00:02<00:00, 14.23it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.33it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.7086381912231445, 'eval_accuracy': 0.66300368309021, 'eval_f1': 0.5962577160493827, 'eval_runtime': 2.3738, 'eval_samples_per_second': 115.006, 'eval_steps_per_second': 14.744, 'epoch': 1.0}\n",
            "  4% 39/975 [00:29<10:59,  1.42it/s]\n",
            "100% 35/35 [00:02<00:00, 14.43it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:06:05,050 >> Saving model checkpoint to models/OneShot/1/checkpoint-39\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:06:05,051 >> Configuration saved in models/OneShot/1/checkpoint-39/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:06:06,121 >> Model weights saved in models/OneShot/1/checkpoint-39/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:06:06,121 >> tokenizer config file saved in models/OneShot/1/checkpoint-39/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:06:06,122 >> Special tokens file saved in models/OneShot/1/checkpoint-39/special_tokens_map.json\n",
            "  8% 77/975 [01:00<10:19,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:06:36,045 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:06:36,047 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:06:36,047 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:06:36,047 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.16it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.07it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.09it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.76it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.34it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.98it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.23it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.32it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.24it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.13it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.15it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 15.06it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.94it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 15.00it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 15.26it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.7218104004859924, 'eval_accuracy': 0.732600748538971, 'eval_f1': 0.7046374105197635, 'eval_runtime': 2.2727, 'eval_samples_per_second': 120.119, 'eval_steps_per_second': 15.4, 'epoch': 2.0}\n",
            "  8% 78/975 [01:02<10:19,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 15.24it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:06:38,321 >> Saving model checkpoint to models/OneShot/1/checkpoint-78\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:06:38,322 >> Configuration saved in models/OneShot/1/checkpoint-78/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:06:39,412 >> Model weights saved in models/OneShot/1/checkpoint-78/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:06:39,412 >> tokenizer config file saved in models/OneShot/1/checkpoint-78/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:06:39,413 >> Special tokens file saved in models/OneShot/1/checkpoint-78/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:06:42,824 >> Deleting older checkpoint [models/OneShot/1/checkpoint-39] due to args.save_total_limit\n",
            " 12% 116/975 [01:32<09:49,  1.46it/s][INFO|trainer.py:723] 2022-08-25 01:07:08,849 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:07:08,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:07:08,851 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:07:08,851 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.35it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.64it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.27it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.77it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.61it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.22it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.41it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.32it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.33it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.27it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.11it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.98it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.90it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 15.03it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 15.01it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.5887289047241211, 'eval_accuracy': 0.7655677795410156, 'eval_f1': 0.765034965034965, 'eval_runtime': 2.2678, 'eval_samples_per_second': 120.382, 'eval_steps_per_second': 15.434, 'epoch': 3.0}\n",
            " 12% 117/975 [01:35<09:48,  1.46it/s]\n",
            "100% 35/35 [00:02<00:00, 15.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:07:11,120 >> Saving model checkpoint to models/OneShot/1/checkpoint-117\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:07:11,120 >> Configuration saved in models/OneShot/1/checkpoint-117/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:07:12,120 >> Model weights saved in models/OneShot/1/checkpoint-117/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:07:12,121 >> tokenizer config file saved in models/OneShot/1/checkpoint-117/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:07:12,121 >> Special tokens file saved in models/OneShot/1/checkpoint-117/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:07:15,600 >> Deleting older checkpoint [models/OneShot/1/checkpoint-78] due to args.save_total_limit\n",
            " 16% 155/975 [02:06<09:33,  1.43it/s][INFO|trainer.py:723] 2022-08-25 01:07:42,174 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:07:42,176 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:07:42,176 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:07:42,176 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.08it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.59it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.76it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.40it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.14it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.62it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.86it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.93it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.76it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.74it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.40it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.55it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.67it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8167409300804138, 'eval_accuracy': 0.7545787692070007, 'eval_f1': 0.7437984116089812, 'eval_runtime': 2.3339, 'eval_samples_per_second': 116.97, 'eval_steps_per_second': 14.996, 'epoch': 4.0}\n",
            " 16% 156/975 [02:08<09:32,  1.43it/s]\n",
            "100% 35/35 [00:02<00:00, 14.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:07:44,512 >> Saving model checkpoint to models/OneShot/1/checkpoint-156\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:07:44,513 >> Configuration saved in models/OneShot/1/checkpoint-156/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:07:45,572 >> Model weights saved in models/OneShot/1/checkpoint-156/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:07:45,573 >> tokenizer config file saved in models/OneShot/1/checkpoint-156/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:07:45,573 >> Special tokens file saved in models/OneShot/1/checkpoint-156/special_tokens_map.json\n",
            " 20% 194/975 [02:39<08:57,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:08:15,119 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:08:15,120 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:08:15,121 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:08:15,121 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.99it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.33it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.31it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.70it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.46it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.12it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.22it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.20it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.17it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.22it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.98it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.96it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.95it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.94it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 15.07it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.5616040229797363, 'eval_accuracy': 0.8461538553237915, 'eval_f1': 0.8438725490196078, 'eval_runtime': 2.2807, 'eval_samples_per_second': 119.7, 'eval_steps_per_second': 15.346, 'epoch': 5.0}\n",
            " 20% 195/975 [02:41<08:57,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 15.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:08:17,403 >> Saving model checkpoint to models/OneShot/1/checkpoint-195\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:08:17,403 >> Configuration saved in models/OneShot/1/checkpoint-195/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:08:18,569 >> Model weights saved in models/OneShot/1/checkpoint-195/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:08:18,570 >> tokenizer config file saved in models/OneShot/1/checkpoint-195/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:08:18,570 >> Special tokens file saved in models/OneShot/1/checkpoint-195/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:08:21,993 >> Deleting older checkpoint [models/OneShot/1/checkpoint-117] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:08:22,071 >> Deleting older checkpoint [models/OneShot/1/checkpoint-156] due to args.save_total_limit\n",
            " 24% 233/975 [03:12<08:34,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:08:48,455 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:08:48,457 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:08:48,457 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:08:48,457 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.56it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.71it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.52it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.42it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.19it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.68it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.88it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.87it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.84it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.76it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.67it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.61it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.54it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.71it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.77it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.5766980648040771, 'eval_accuracy': 0.8534798622131348, 'eval_f1': 0.8507381082558776, 'eval_runtime': 2.331, 'eval_samples_per_second': 117.116, 'eval_steps_per_second': 15.015, 'epoch': 6.0}\n",
            " 24% 234/975 [03:14<08:34,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:08:50,789 >> Saving model checkpoint to models/OneShot/1/checkpoint-234\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:08:50,790 >> Configuration saved in models/OneShot/1/checkpoint-234/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:08:51,794 >> Model weights saved in models/OneShot/1/checkpoint-234/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:08:51,795 >> tokenizer config file saved in models/OneShot/1/checkpoint-234/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:08:51,795 >> Special tokens file saved in models/OneShot/1/checkpoint-234/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:08:55,371 >> Deleting older checkpoint [models/OneShot/1/checkpoint-195] due to args.save_total_limit\n",
            " 28% 272/975 [03:45<08:07,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:09:21,835 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:09:21,837 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:09:21,837 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:09:21,837 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.76it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.84it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.89it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.52it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.20it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.04it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.95it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.10it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.02it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.96it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.88it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.73it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.60it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.85it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.88it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.756409764289856, 'eval_accuracy': 0.8168498277664185, 'eval_f1': 0.8107531055900621, 'eval_runtime': 2.3078, 'eval_samples_per_second': 118.294, 'eval_steps_per_second': 15.166, 'epoch': 7.0}\n",
            " 28% 273/975 [03:48<08:07,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:09:24,146 >> Saving model checkpoint to models/OneShot/1/checkpoint-273\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:09:24,147 >> Configuration saved in models/OneShot/1/checkpoint-273/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:09:25,137 >> Model weights saved in models/OneShot/1/checkpoint-273/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:09:25,138 >> tokenizer config file saved in models/OneShot/1/checkpoint-273/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:09:25,138 >> Special tokens file saved in models/OneShot/1/checkpoint-273/special_tokens_map.json\n",
            " 32% 311/975 [04:18<07:37,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:09:54,884 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:09:54,886 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:09:54,886 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:09:54,886 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.79it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.59it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 16.03it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.37it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.20it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.85it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.91it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.93it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 14.98it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.91it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.54it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.49it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.79it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8918111324310303, 'eval_accuracy': 0.8205128312110901, 'eval_f1': 0.8114029522480226, 'eval_runtime': 2.3111, 'eval_samples_per_second': 118.125, 'eval_steps_per_second': 15.144, 'epoch': 8.0}\n",
            " 32% 312/975 [04:21<07:37,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 14.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:09:57,199 >> Saving model checkpoint to models/OneShot/1/checkpoint-312\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:09:57,200 >> Configuration saved in models/OneShot/1/checkpoint-312/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:09:58,481 >> Model weights saved in models/OneShot/1/checkpoint-312/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:09:58,481 >> tokenizer config file saved in models/OneShot/1/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:09:58,482 >> Special tokens file saved in models/OneShot/1/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:10:02,022 >> Deleting older checkpoint [models/OneShot/1/checkpoint-273] due to args.save_total_limit\n",
            " 36% 350/975 [04:52<07:15,  1.43it/s][INFO|trainer.py:723] 2022-08-25 01:10:28,414 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:10:28,416 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:10:28,416 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:10:28,417 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.10it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 16.66it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.48it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.17it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.09it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.41it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.67it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.79it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 14.93it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 14.92it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.90it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.78it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.45it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.48it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.63it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8692466020584106, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8273730063935923, 'eval_runtime': 2.3393, 'eval_samples_per_second': 116.701, 'eval_steps_per_second': 14.962, 'epoch': 9.0}\n",
            " 36% 351/975 [04:54<07:15,  1.43it/s]\n",
            "100% 35/35 [00:02<00:00, 14.83it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:10:30,757 >> Saving model checkpoint to models/OneShot/1/checkpoint-351\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:10:30,758 >> Configuration saved in models/OneShot/1/checkpoint-351/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:10:31,801 >> Model weights saved in models/OneShot/1/checkpoint-351/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:10:31,802 >> tokenizer config file saved in models/OneShot/1/checkpoint-351/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:10:31,802 >> Special tokens file saved in models/OneShot/1/checkpoint-351/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:10:35,358 >> Deleting older checkpoint [models/OneShot/1/checkpoint-312] due to args.save_total_limit\n",
            " 40% 389/975 [05:25<06:45,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:11:01,754 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:11:01,755 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:11:01,756 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:11:01,756 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.88it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.49it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 16.26it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.74it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.46it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.83it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.90it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 15.03it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.99it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 15.03it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.72it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.62it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.88it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.695095419883728, 'eval_accuracy': 0.8571428656578064, 'eval_f1': 0.8540086933867187, 'eval_runtime': 2.3008, 'eval_samples_per_second': 118.657, 'eval_steps_per_second': 15.212, 'epoch': 10.0}\n",
            " 40% 390/975 [05:28<06:44,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.92it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:11:04,058 >> Saving model checkpoint to models/OneShot/1/checkpoint-390\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:11:04,059 >> Configuration saved in models/OneShot/1/checkpoint-390/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:11:05,072 >> Model weights saved in models/OneShot/1/checkpoint-390/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:11:05,072 >> tokenizer config file saved in models/OneShot/1/checkpoint-390/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:11:05,073 >> Special tokens file saved in models/OneShot/1/checkpoint-390/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:11:08,662 >> Deleting older checkpoint [models/OneShot/1/checkpoint-234] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:11:08,683 >> Deleting older checkpoint [models/OneShot/1/checkpoint-351] due to args.save_total_limit\n",
            " 44% 428/975 [05:59<06:19,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:11:35,075 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:11:35,076 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:11:35,076 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:11:35,077 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.10it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.02it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.95it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.43it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.20it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.79it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.85it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.86it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 14.94it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 15.02it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.91it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.65it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.52it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.65it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7629103064537048, 'eval_accuracy': 0.8534798622131348, 'eval_f1': 0.8520325203252033, 'eval_runtime': 2.317, 'eval_samples_per_second': 117.822, 'eval_steps_per_second': 15.105, 'epoch': 11.0}\n",
            " 44% 429/975 [06:01<06:18,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:11:37,395 >> Saving model checkpoint to models/OneShot/1/checkpoint-429\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:11:37,396 >> Configuration saved in models/OneShot/1/checkpoint-429/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:11:38,493 >> Model weights saved in models/OneShot/1/checkpoint-429/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:11:38,493 >> tokenizer config file saved in models/OneShot/1/checkpoint-429/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:11:38,493 >> Special tokens file saved in models/OneShot/1/checkpoint-429/special_tokens_map.json\n",
            " 48% 467/975 [06:32<05:56,  1.43it/s][INFO|trainer.py:723] 2022-08-25 01:12:08,368 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:12:08,371 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:12:08,373 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:12:08,373 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.60it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.40it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 16.34it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.83it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.44it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.62it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.62it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.97it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.02it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 15.17it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 15.16it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 15.10it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.74it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.49it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.46it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9372543096542358, 'eval_accuracy': 0.831501841545105, 'eval_f1': 0.8249023982152817, 'eval_runtime': 2.3178, 'eval_samples_per_second': 117.782, 'eval_steps_per_second': 15.1, 'epoch': 12.0}\n",
            " 48% 468/975 [06:34<05:55,  1.43it/s]\n",
            "100% 35/35 [00:02<00:00, 14.71it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:12:10,690 >> Saving model checkpoint to models/OneShot/1/checkpoint-468\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:12:10,691 >> Configuration saved in models/OneShot/1/checkpoint-468/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:12:11,839 >> Model weights saved in models/OneShot/1/checkpoint-468/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:12:11,839 >> tokenizer config file saved in models/OneShot/1/checkpoint-468/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:12:11,840 >> Special tokens file saved in models/OneShot/1/checkpoint-468/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:12:15,176 >> Deleting older checkpoint [models/OneShot/1/checkpoint-429] due to args.save_total_limit\n",
            "{'loss': 0.1193, 'learning_rate': 9.743589743589744e-06, 'epoch': 12.82}\n",
            " 52% 506/975 [07:05<05:25,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:12:41,519 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:12:41,521 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:12:41,521 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:12:41,521 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.50it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.37it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.98it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.43it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.53it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.78it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.83it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.99it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.09it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 15.20it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 15.04it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 15.16it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.92it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.69it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.79it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8490657806396484, 'eval_accuracy': 0.8498168587684631, 'eval_f1': 0.8484421756732969, 'eval_runtime': 2.2994, 'eval_samples_per_second': 118.726, 'eval_steps_per_second': 15.221, 'epoch': 13.0}\n",
            " 52% 507/975 [07:07<05:24,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:12:43,822 >> Saving model checkpoint to models/OneShot/1/checkpoint-507\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:12:43,823 >> Configuration saved in models/OneShot/1/checkpoint-507/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:12:44,979 >> Model weights saved in models/OneShot/1/checkpoint-507/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:12:44,980 >> tokenizer config file saved in models/OneShot/1/checkpoint-507/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:12:44,980 >> Special tokens file saved in models/OneShot/1/checkpoint-507/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:12:48,425 >> Deleting older checkpoint [models/OneShot/1/checkpoint-468] due to args.save_total_limit\n",
            " 56% 545/975 [07:38<04:55,  1.46it/s][INFO|trainer.py:723] 2022-08-25 01:13:14,805 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:13:14,807 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:13:14,807 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:13:14,807 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.42it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.89it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.15it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.66it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.39it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.84it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.10it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.13it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.25it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.29it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.05it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 15.08it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.86it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.96it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9723426699638367, 'eval_accuracy': 0.8424908518791199, 'eval_f1': 0.8370239764539282, 'eval_runtime': 2.2896, 'eval_samples_per_second': 119.237, 'eval_steps_per_second': 15.287, 'epoch': 14.0}\n",
            " 56% 546/975 [07:41<04:54,  1.46it/s]\n",
            "100% 35/35 [00:02<00:00, 14.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:13:17,098 >> Saving model checkpoint to models/OneShot/1/checkpoint-546\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:13:17,099 >> Configuration saved in models/OneShot/1/checkpoint-546/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:13:18,337 >> Model weights saved in models/OneShot/1/checkpoint-546/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:13:18,338 >> tokenizer config file saved in models/OneShot/1/checkpoint-546/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:13:18,338 >> Special tokens file saved in models/OneShot/1/checkpoint-546/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:13:21,735 >> Deleting older checkpoint [models/OneShot/1/checkpoint-507] due to args.save_total_limit\n",
            " 60% 584/975 [08:12<04:32,  1.43it/s][INFO|trainer.py:723] 2022-08-25 01:13:48,150 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:13:48,152 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:13:48,152 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:13:48,152 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.73it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.18it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.92it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.49it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.36it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.77it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.77it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.80it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 14.77it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.72it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.87it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.65it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.58it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.72it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8980807065963745, 'eval_accuracy': 0.8424908518791199, 'eval_f1': 0.8393788055004447, 'eval_runtime': 2.3278, 'eval_samples_per_second': 117.278, 'eval_steps_per_second': 15.036, 'epoch': 15.0}\n",
            " 60% 585/975 [08:14<04:32,  1.43it/s]\n",
            "100% 35/35 [00:02<00:00, 14.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:13:50,482 >> Saving model checkpoint to models/OneShot/1/checkpoint-585\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:13:50,483 >> Configuration saved in models/OneShot/1/checkpoint-585/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:13:51,628 >> Model weights saved in models/OneShot/1/checkpoint-585/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:13:51,629 >> tokenizer config file saved in models/OneShot/1/checkpoint-585/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:13:51,629 >> Special tokens file saved in models/OneShot/1/checkpoint-585/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:13:55,086 >> Deleting older checkpoint [models/OneShot/1/checkpoint-546] due to args.save_total_limit\n",
            " 64% 623/975 [08:45<04:02,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:14:21,464 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:14:21,466 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:14:21,466 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:14:21,467 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.56it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.41it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.20it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.81it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.56it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 15.00it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.06it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.23it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.38it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.15it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.36it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 15.16it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.87it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.93it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9510946273803711, 'eval_accuracy': 0.831501841545105, 'eval_f1': 0.8263550884955753, 'eval_runtime': 2.2732, 'eval_samples_per_second': 120.093, 'eval_steps_per_second': 15.397, 'epoch': 16.0}\n",
            " 64% 624/975 [08:47<04:01,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 15.09it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:14:23,741 >> Saving model checkpoint to models/OneShot/1/checkpoint-624\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:14:23,742 >> Configuration saved in models/OneShot/1/checkpoint-624/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:14:24,963 >> Model weights saved in models/OneShot/1/checkpoint-624/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:14:24,964 >> tokenizer config file saved in models/OneShot/1/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:14:24,964 >> Special tokens file saved in models/OneShot/1/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:14:28,441 >> Deleting older checkpoint [models/OneShot/1/checkpoint-585] due to args.save_total_limit\n",
            " 68% 662/975 [09:18<03:37,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:14:54,831 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:14:54,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:14:54,833 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:14:54,833 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.17it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.51it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.76it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.36it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.17it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.76it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.85it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.07it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.96it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.86it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.72it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.54it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.76it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.78it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0001945495605469, 'eval_accuracy': 0.8388278484344482, 'eval_f1': 0.8334627329192545, 'eval_runtime': 2.3157, 'eval_samples_per_second': 117.893, 'eval_steps_per_second': 15.114, 'epoch': 17.0}\n",
            " 68% 663/975 [09:21<03:36,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 15.03it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:14:57,150 >> Saving model checkpoint to models/OneShot/1/checkpoint-663\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:14:57,151 >> Configuration saved in models/OneShot/1/checkpoint-663/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:14:58,277 >> Model weights saved in models/OneShot/1/checkpoint-663/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:14:58,278 >> tokenizer config file saved in models/OneShot/1/checkpoint-663/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:14:58,278 >> Special tokens file saved in models/OneShot/1/checkpoint-663/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:15:01,688 >> Deleting older checkpoint [models/OneShot/1/checkpoint-624] due to args.save_total_limit\n",
            " 72% 701/975 [09:52<03:10,  1.43it/s][INFO|trainer.py:723] 2022-08-25 01:15:28,186 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:15:28,188 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:15:28,188 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:15:28,188 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.74it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.26it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.89it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.42it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.26it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.61it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.76it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.83it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 14.91it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 14.72it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.76it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.79it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.51it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.49it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.57it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9301682710647583, 'eval_accuracy': 0.8424908518791199, 'eval_f1': 0.8390352260417667, 'eval_runtime': 2.335, 'eval_samples_per_second': 116.918, 'eval_steps_per_second': 14.99, 'epoch': 18.0}\n",
            " 72% 702/975 [09:54<03:10,  1.43it/s]\n",
            "100% 35/35 [00:02<00:00, 14.76it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:15:30,525 >> Saving model checkpoint to models/OneShot/1/checkpoint-702\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:15:30,526 >> Configuration saved in models/OneShot/1/checkpoint-702/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:15:31,750 >> Model weights saved in models/OneShot/1/checkpoint-702/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:15:31,751 >> tokenizer config file saved in models/OneShot/1/checkpoint-702/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:15:31,751 >> Special tokens file saved in models/OneShot/1/checkpoint-702/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:15:35,153 >> Deleting older checkpoint [models/OneShot/1/checkpoint-663] due to args.save_total_limit\n",
            " 76% 740/975 [10:25<02:42,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:16:01,474 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:16:01,476 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:16:01,476 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:16:01,476 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.44it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.43it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 16.27it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.69it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.36it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.66it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.78it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 15.02it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.08it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 15.23it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 15.20it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 15.12it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.78it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.65it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.78it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9659397006034851, 'eval_accuracy': 0.8388278484344482, 'eval_f1': 0.8347275729223995, 'eval_runtime': 2.296, 'eval_samples_per_second': 118.904, 'eval_steps_per_second': 15.244, 'epoch': 19.0}\n",
            " 76% 741/975 [10:27<02:41,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 14.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:16:03,774 >> Saving model checkpoint to models/OneShot/1/checkpoint-741\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:16:03,774 >> Configuration saved in models/OneShot/1/checkpoint-741/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:16:04,926 >> Model weights saved in models/OneShot/1/checkpoint-741/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:16:04,926 >> tokenizer config file saved in models/OneShot/1/checkpoint-741/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:16:04,926 >> Special tokens file saved in models/OneShot/1/checkpoint-741/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:16:08,500 >> Deleting older checkpoint [models/OneShot/1/checkpoint-702] due to args.save_total_limit\n",
            " 80% 779/975 [10:58<02:16,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:16:34,840 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:16:34,842 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:16:34,842 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:16:34,842 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.42it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 16.96it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.51it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.32it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.03it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.46it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.69it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.88it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 15.03it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 15.09it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.93it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.85it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.50it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.46it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.68it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9899982810020447, 'eval_accuracy': 0.831501841545105, 'eval_f1': 0.8267957629923866, 'eval_runtime': 2.3342, 'eval_samples_per_second': 116.956, 'eval_steps_per_second': 14.994, 'epoch': 20.0}\n",
            " 80% 780/975 [11:01<02:15,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.73it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:16:37,178 >> Saving model checkpoint to models/OneShot/1/checkpoint-780\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:16:37,179 >> Configuration saved in models/OneShot/1/checkpoint-780/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:16:38,338 >> Model weights saved in models/OneShot/1/checkpoint-780/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:16:38,339 >> tokenizer config file saved in models/OneShot/1/checkpoint-780/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:16:38,339 >> Special tokens file saved in models/OneShot/1/checkpoint-780/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:16:41,794 >> Deleting older checkpoint [models/OneShot/1/checkpoint-741] due to args.save_total_limit\n",
            " 84% 818/975 [11:32<01:49,  1.43it/s][INFO|trainer.py:723] 2022-08-25 01:17:08,234 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:17:08,236 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:17:08,236 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:17:08,236 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.41it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.77it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.88it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.40it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.19it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.78it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.80it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 14.88it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:01, 14.96it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.87it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.93it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.72it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.62it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.59it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.81it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0403813123703003, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8303481419082208, 'eval_runtime': 2.3258, 'eval_samples_per_second': 117.378, 'eval_steps_per_second': 15.048, 'epoch': 21.0}\n",
            " 84% 819/975 [11:34<01:48,  1.43it/s]\n",
            "100% 35/35 [00:02<00:00, 14.75it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:17:10,563 >> Saving model checkpoint to models/OneShot/1/checkpoint-819\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:17:10,564 >> Configuration saved in models/OneShot/1/checkpoint-819/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:17:11,591 >> Model weights saved in models/OneShot/1/checkpoint-819/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:17:11,592 >> tokenizer config file saved in models/OneShot/1/checkpoint-819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:17:11,592 >> Special tokens file saved in models/OneShot/1/checkpoint-819/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:17:15,111 >> Deleting older checkpoint [models/OneShot/1/checkpoint-780] due to args.save_total_limit\n",
            " 88% 857/975 [12:05<01:21,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:17:41,399 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:17:41,401 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:17:41,401 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:17:41,401 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.85it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.97it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.06it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.68it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.25it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.88it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.13it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.10it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.11it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 14.94it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.00it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.90it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.70it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.76it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.96it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0365650653839111, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8303481419082208, 'eval_runtime': 2.2985, 'eval_samples_per_second': 118.772, 'eval_steps_per_second': 15.227, 'epoch': 22.0}\n",
            " 88% 858/975 [12:07<01:20,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 14.97it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:17:43,700 >> Saving model checkpoint to models/OneShot/1/checkpoint-858\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:17:43,701 >> Configuration saved in models/OneShot/1/checkpoint-858/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:17:44,737 >> Model weights saved in models/OneShot/1/checkpoint-858/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:17:44,737 >> tokenizer config file saved in models/OneShot/1/checkpoint-858/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:17:44,738 >> Special tokens file saved in models/OneShot/1/checkpoint-858/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:17:48,314 >> Deleting older checkpoint [models/OneShot/1/checkpoint-819] due to args.save_total_limit\n",
            " 92% 896/975 [12:38<00:54,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:18:14,624 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:18:14,626 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:18:14,626 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:18:14,626 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 19.78it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 17.12it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:01, 15.96it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 15.57it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 15.34it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 14.83it/s]\u001b[A\n",
            " 43% 15/35 [00:00<00:01, 14.84it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 14.88it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 14.95it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:00, 14.80it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 14.81it/s]\u001b[A\n",
            " 77% 27/35 [00:01<00:00, 14.51it/s]\u001b[A\n",
            " 83% 29/35 [00:01<00:00, 14.52it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 14.68it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0376038551330566, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8303481419082208, 'eval_runtime': 2.3281, 'eval_samples_per_second': 117.262, 'eval_steps_per_second': 15.034, 'epoch': 23.0}\n",
            " 92% 897/975 [12:41<00:54,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:18:16,956 >> Saving model checkpoint to models/OneShot/1/checkpoint-897\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:18:16,957 >> Configuration saved in models/OneShot/1/checkpoint-897/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:18:17,979 >> Model weights saved in models/OneShot/1/checkpoint-897/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:18:17,980 >> tokenizer config file saved in models/OneShot/1/checkpoint-897/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:18:17,980 >> Special tokens file saved in models/OneShot/1/checkpoint-897/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:18:21,501 >> Deleting older checkpoint [models/OneShot/1/checkpoint-858] due to args.save_total_limit\n",
            " 96% 935/975 [13:11<00:27,  1.45it/s][INFO|trainer.py:723] 2022-08-25 01:18:47,875 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:18:47,877 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:18:47,877 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:18:47,877 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 20.88it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 16.96it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 15.93it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.41it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.33it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.86it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 14.98it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.12it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.07it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.02it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 14.92it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.79it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.54it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.67it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.71it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0478408336639404, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8303481419082208, 'eval_runtime': 2.3095, 'eval_samples_per_second': 118.206, 'eval_steps_per_second': 15.155, 'epoch': 24.0}\n",
            " 96% 936/975 [13:14<00:26,  1.45it/s]\n",
            "100% 35/35 [00:02<00:00, 14.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:18:50,188 >> Saving model checkpoint to models/OneShot/1/checkpoint-936\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:18:50,189 >> Configuration saved in models/OneShot/1/checkpoint-936/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:18:51,232 >> Model weights saved in models/OneShot/1/checkpoint-936/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:18:51,233 >> tokenizer config file saved in models/OneShot/1/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:18:51,233 >> Special tokens file saved in models/OneShot/1/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:18:54,814 >> Deleting older checkpoint [models/OneShot/1/checkpoint-897] due to args.save_total_limit\n",
            "100% 974/975 [13:45<00:00,  1.44it/s][INFO|trainer.py:723] 2022-08-25 01:19:21,081 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:19:21,083 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:19:21,083 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:19:21,083 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 21.11it/s]\u001b[A\n",
            " 17% 6/35 [00:00<00:01, 17.41it/s]\u001b[A\n",
            " 23% 8/35 [00:00<00:01, 16.21it/s]\u001b[A\n",
            " 29% 10/35 [00:00<00:01, 15.75it/s]\u001b[A\n",
            " 34% 12/35 [00:00<00:01, 15.49it/s]\u001b[A\n",
            " 40% 14/35 [00:00<00:01, 14.82it/s]\u001b[A\n",
            " 46% 16/35 [00:01<00:01, 15.00it/s]\u001b[A\n",
            " 51% 18/35 [00:01<00:01, 15.01it/s]\u001b[A\n",
            " 57% 20/35 [00:01<00:00, 15.18it/s]\u001b[A\n",
            " 63% 22/35 [00:01<00:00, 15.11it/s]\u001b[A\n",
            " 69% 24/35 [00:01<00:00, 15.00it/s]\u001b[A\n",
            " 74% 26/35 [00:01<00:00, 14.89it/s]\u001b[A\n",
            " 80% 28/35 [00:01<00:00, 14.65it/s]\u001b[A\n",
            " 86% 30/35 [00:01<00:00, 14.75it/s]\u001b[A\n",
            " 91% 32/35 [00:02<00:00, 14.89it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0489397048950195, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8303481419082208, 'eval_runtime': 2.2948, 'eval_samples_per_second': 118.964, 'eval_steps_per_second': 15.252, 'epoch': 25.0}\n",
            "100% 975/975 [13:47<00:00,  1.44it/s]\n",
            "100% 35/35 [00:02<00:00, 14.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 01:19:23,379 >> Saving model checkpoint to models/OneShot/1/checkpoint-975\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:19:23,380 >> Configuration saved in models/OneShot/1/checkpoint-975/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:19:24,426 >> Model weights saved in models/OneShot/1/checkpoint-975/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:19:24,426 >> tokenizer config file saved in models/OneShot/1/checkpoint-975/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:19:24,426 >> Special tokens file saved in models/OneShot/1/checkpoint-975/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 01:19:27,965 >> Deleting older checkpoint [models/OneShot/1/checkpoint-936] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2022-08-25 01:19:28,141 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1942] 2022-08-25 01:19:28,141 >> Loading best model from models/OneShot/1/checkpoint-390 (score: 0.8540086933867187).\n",
            "{'train_runtime': 834.9856, 'train_samples_per_second': 36.438, 'train_steps_per_second': 1.168, 'train_loss': 0.06277974495520959, 'epoch': 25.0}\n",
            "100% 975/975 [13:54<00:00,  1.17it/s]\n",
            "[INFO|trainer.py:2640] 2022-08-25 01:19:30,908 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 01:19:30,909 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 01:19:32,331 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 01:19:32,331 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 01:19:32,331 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0628\n",
            "  train_runtime            = 0:13:54.98\n",
            "  train_samples            =       1217\n",
            "  train_samples_per_second =     36.438\n",
            "  train_steps_per_second   =      1.168\n",
            "08/25/2022 01:19:32 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:723] 2022-08-25 01:19:32,383 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 01:19:32,385 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 01:19:32,385 >>   Num examples = 273\n",
            "[INFO|trainer.py:2896] 2022-08-25 01:19:32,385 >>   Batch size = 8\n",
            "100% 35/35 [00:02<00:00, 16.16it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.8571\n",
            "  eval_f1                 =      0.854\n",
            "  eval_loss               =     0.6951\n",
            "  eval_runtime            = 0:00:02.24\n",
            "  eval_samples            =        273\n",
            "  eval_samples_per_second =    121.367\n",
            "  eval_steps_per_second   =      15.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Multi : Multilingual BERT for one-shot multilingual idiomaticity detection "
      ],
      "metadata": {
        "id": "LgoRNxSD2ygG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 25 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "id": "ob4gOl0VZtSz",
        "outputId": "737bc6c3-1010-4da3-950f-71cca3778bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 02:56:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 02:56:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_02-56-56_93ca22c7e237,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 02:56:56 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "08/25/2022 02:56:56 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "08/25/2022 02:56:57 - WARNING - datasets.builder -   Using custom data configuration default-48fc7ef7a91f0654\n",
            "08/25/2022 02:56:57 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-48fc7ef7a91f0654/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "100% 2/2 [00:00<00:00, 194.12it/s]\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 02:56:57,275 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 02:56:57,282 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 02:56:57,533 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 02:56:57,534 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:56:58,317 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:56:58,317 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:56:58,318 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:56:58,318 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-08-25 02:56:58,318 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:681] 2022-08-25 02:56:58,445 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:730] 2022-08-25 02:56:58,445 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2022-08-25 02:56:58,790 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2426] 2022-08-25 02:57:03,614 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2438] 2022-08-25 02:57:03,614 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "08/25/2022 02:57:03 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-48fc7ef7a91f0654/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0126b820da8273b6.arrow\n",
            "100% 1/1 [00:00<00:00, 10.40ba/s]\n",
            "08/25/2022 02:57:03 - INFO - __main__ -   Sample 1100 of the training set: {'label': 0, 'sentence1': 'Given how many hours of beauty sleep most kitties like to clock up, choosing the right place for them to lay their heads is critical.', 'sentence2': 'beauty sleep', 'input_ids': [101, 90491, 14796, 11299, 19573, 10108, 54883, 63658, 10992, 72812, 14197, 11850, 10114, 52843, 10741, 117, 11257, 90739, 10105, 13448, 11192, 10142, 11345, 10114, 47413, 10455, 42399, 10124, 24523, 119, 102, 54883, 63658, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 02:57:03 - INFO - __main__ -   Sample 516 of the training set: {'label': 0, 'sentence1': 'Yet Sergeant Mark Brady, who oversees major collision investigations for South Yorkshire Police, told the inquiry, “Had there been a hard shoulder, had Jason and Alexandru pulled on to the hard shoulder, my opinion is that Mr Szuba would have driven clean past them.”', 'sentence2': 'hard shoulder', 'input_ids': [101, 71547, 54118, 11997, 45982, 117, 10479, 10491, 20262, 10107, 11922, 94460, 87748, 10142, 11056, 27577, 18051, 117, 21937, 10105, 10106, 56914, 117, 100, 66434, 11155, 10590, 169, 19118, 78681, 117, 10374, 16796, 10111, 43816, 65884, 10135, 10114, 10105, 19118, 78681, 117, 15127, 32282, 10124, 10189, 12916, 156, 13078, 10537, 10894, 10529, 39803, 55911, 17781, 11345, 119, 100, 102, 19118, 78681, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 02:57:03 - INFO - __main__ -   Sample 2089 of the training set: {'label': 0, 'sentence1': 'While the family fun hub holds back from turning into a ghost town for Japanese Yokai (demons) to roam around this year, Downtown East is bringing its scary stories online with a series of episodes on Instagram Stories.', 'sentence2': 'ghost town', 'input_ids': [101, 14600, 10105, 11365, 41807, 65896, 28278, 12014, 10188, 48448, 10708, 169, 100766, 12221, 10142, 13847, 30665, 18511, 113, 30776, 10891, 114, 10114, 25470, 11008, 12166, 10531, 10924, 117, 68339, 11830, 10124, 45749, 10474, 187, 15983, 10157, 21158, 13893, 10169, 169, 11366, 10108, 23604, 10135, 83019, 25955, 119, 102, 100766, 12221, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:723] 2022-08-25 02:57:05,988 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-08-25 02:57:05,996 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-08-25 02:57:05,996 >>   Num examples = 4631\n",
            "[INFO|trainer.py:1607] 2022-08-25 02:57:05,996 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1608] 2022-08-25 02:57:05,996 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1609] 2022-08-25 02:57:05,996 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1610] 2022-08-25 02:57:05,996 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-08-25 02:57:05,996 >>   Total optimization steps = 3625\n",
            "  4% 145/3625 [01:43<38:08,  1.52it/s][INFO|trainer.py:723] 2022-08-25 02:58:49,226 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 02:58:49,228 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 02:58:49,228 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 02:58:49,228 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.47it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.70it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.27it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.52it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.22it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.82it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.05it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.22it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.02it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.82it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.92it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.26it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.87it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.90it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.93it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.99it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.83it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.92it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.95it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.95it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.09it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.83it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.74it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7757744193077087, 'eval_accuracy': 0.7225980758666992, 'eval_f1': 0.7216991790314078, 'eval_runtime': 6.1823, 'eval_samples_per_second': 119.536, 'eval_steps_per_second': 15.043, 'epoch': 1.0}\n",
            "  4% 145/3625 [01:49<38:08,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.89it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 02:58:55,412 >> Saving model checkpoint to models/OneShot/1/checkpoint-145\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 02:58:55,412 >> Configuration saved in models/OneShot/1/checkpoint-145/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 02:58:58,006 >> Model weights saved in models/OneShot/1/checkpoint-145/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 02:58:58,007 >> tokenizer config file saved in models/OneShot/1/checkpoint-145/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 02:58:58,007 >> Special tokens file saved in models/OneShot/1/checkpoint-145/special_tokens_map.json\n",
            "  8% 290/3625 [03:39<36:28,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:00:45,736 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:00:45,738 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:00:45,739 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:00:45,739 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.07it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.02it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.54it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.73it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.46it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.20it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.25it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.40it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.18it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.23it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.94it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.12it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.22it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.97it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.93it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.77it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.89it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.92it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 15.01it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8404209613800049, 'eval_accuracy': 0.7726657390594482, 'eval_f1': 0.7726653482751044, 'eval_runtime': 6.1564, 'eval_samples_per_second': 120.037, 'eval_steps_per_second': 15.106, 'epoch': 2.0}\n",
            "  8% 290/3625 [03:45<36:28,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:00:51,897 >> Saving model checkpoint to models/OneShot/1/checkpoint-290\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:00:51,898 >> Configuration saved in models/OneShot/1/checkpoint-290/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:00:54,214 >> Model weights saved in models/OneShot/1/checkpoint-290/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:00:54,215 >> tokenizer config file saved in models/OneShot/1/checkpoint-290/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:00:54,215 >> Special tokens file saved in models/OneShot/1/checkpoint-290/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:00:59,933 >> Deleting older checkpoint [models/OneShot/1/checkpoint-145] due to args.save_total_limit\n",
            " 12% 435/3625 [05:36<34:50,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:02:42,210 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:02:42,212 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:02:42,213 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:02:42,213 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.27it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.39it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.16it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.63it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.50it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.23it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.32it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.90it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.10it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.12it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.94it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.09it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.10it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.04it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.00it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 15.07it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.10it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9476714730262756, 'eval_accuracy': 0.7902570962905884, 'eval_f1': 0.7900356524209735, 'eval_runtime': 6.1662, 'eval_samples_per_second': 119.846, 'eval_steps_per_second': 15.082, 'epoch': 3.0}\n",
            " 12% 435/3625 [05:42<34:50,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 14.90it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:02:48,381 >> Saving model checkpoint to models/OneShot/1/checkpoint-435\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:02:48,382 >> Configuration saved in models/OneShot/1/checkpoint-435/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:02:50,661 >> Model weights saved in models/OneShot/1/checkpoint-435/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:02:50,662 >> tokenizer config file saved in models/OneShot/1/checkpoint-435/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:02:50,662 >> Special tokens file saved in models/OneShot/1/checkpoint-435/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:02:56,472 >> Deleting older checkpoint [models/OneShot/1/checkpoint-290] due to args.save_total_limit\n",
            "{'loss': 0.1577, 'learning_rate': 1.7241379310344828e-05, 'epoch': 3.45}\n",
            " 16% 580/3625 [07:32<33:16,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:04:38,845 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:04:38,847 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:04:38,847 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:04:38,847 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.81it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.46it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.16it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.68it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.44it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.98it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.07it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.35it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.22it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.88it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.02it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.96it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.94it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.05it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.00it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.10it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.98it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.96it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7577977776527405, 'eval_accuracy': 0.8362652063369751, 'eval_f1': 0.8339100667376207, 'eval_runtime': 6.1624, 'eval_samples_per_second': 119.921, 'eval_steps_per_second': 15.091, 'epoch': 4.0}\n",
            " 16% 580/3625 [07:39<33:16,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 14.90it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:04:45,011 >> Saving model checkpoint to models/OneShot/1/checkpoint-580\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:04:45,012 >> Configuration saved in models/OneShot/1/checkpoint-580/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:04:47,385 >> Model weights saved in models/OneShot/1/checkpoint-580/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:04:47,386 >> tokenizer config file saved in models/OneShot/1/checkpoint-580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:04:47,386 >> Special tokens file saved in models/OneShot/1/checkpoint-580/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:04:53,050 >> Deleting older checkpoint [models/OneShot/1/checkpoint-435] due to args.save_total_limit\n",
            " 20% 725/3625 [09:29<31:44,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:06:35,340 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:06:35,342 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:06:35,342 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:06:35,342 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.09it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.94it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.55it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.96it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.68it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.13it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.22it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.40it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 15.01it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.10it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.15it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.22it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.04it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.10it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.17it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.89it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.06it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.17it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.05it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.18it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.07it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.20it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.875864565372467, 'eval_accuracy': 0.8497970104217529, 'eval_f1': 0.8481914332509786, 'eval_runtime': 6.1405, 'eval_samples_per_second': 120.349, 'eval_steps_per_second': 15.145, 'epoch': 5.0}\n",
            " 20% 725/3625 [09:35<31:44,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.03it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:06:41,485 >> Saving model checkpoint to models/OneShot/1/checkpoint-725\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:06:41,486 >> Configuration saved in models/OneShot/1/checkpoint-725/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:06:44,028 >> Model weights saved in models/OneShot/1/checkpoint-725/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:06:44,029 >> tokenizer config file saved in models/OneShot/1/checkpoint-725/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:06:44,030 >> Special tokens file saved in models/OneShot/1/checkpoint-725/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:06:49,675 >> Deleting older checkpoint [models/OneShot/1/checkpoint-580] due to args.save_total_limit\n",
            " 24% 870/3625 [11:25<30:06,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:08:31,912 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:08:31,915 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:08:31,915 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:08:31,915 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.16it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.75it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.31it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.65it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.45it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.06it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.12it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.36it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.02it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.83it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.87it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.97it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.91it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.93it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.96it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.17it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.07it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.27it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.20it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.17it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.05it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.11it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7766631245613098, 'eval_accuracy': 0.8606224656105042, 'eval_f1': 0.8601708070255365, 'eval_runtime': 6.1546, 'eval_samples_per_second': 120.072, 'eval_steps_per_second': 15.111, 'epoch': 6.0}\n",
            " 24% 870/3625 [11:32<30:06,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 15.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:08:38,071 >> Saving model checkpoint to models/OneShot/1/checkpoint-870\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:08:38,072 >> Configuration saved in models/OneShot/1/checkpoint-870/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:08:40,449 >> Model weights saved in models/OneShot/1/checkpoint-870/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:08:40,450 >> tokenizer config file saved in models/OneShot/1/checkpoint-870/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:08:40,450 >> Special tokens file saved in models/OneShot/1/checkpoint-870/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:08:46,141 >> Deleting older checkpoint [models/OneShot/1/checkpoint-725] due to args.save_total_limit\n",
            "{'loss': 0.021, 'learning_rate': 1.4482758620689657e-05, 'epoch': 6.9}\n",
            " 28% 1015/3625 [13:22<28:32,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:10:28,353 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:10:28,355 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:10:28,355 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:10:28,355 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.49it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.61it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.18it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.45it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.36it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.96it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.11it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.31it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.32it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.18it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.05it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.04it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.92it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.85it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.15it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.16it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.21it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.99it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.05it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.92it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.86it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.71it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.83it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.90it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2676063776016235, 'eval_accuracy': 0.8349120616912842, 'eval_f1': 0.8300496139461938, 'eval_runtime': 6.1651, 'eval_samples_per_second': 119.868, 'eval_steps_per_second': 15.085, 'epoch': 7.0}\n",
            " 28% 1015/3625 [13:28<28:32,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:10:34,522 >> Saving model checkpoint to models/OneShot/1/checkpoint-1015\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:10:34,523 >> Configuration saved in models/OneShot/1/checkpoint-1015/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:10:36,887 >> Model weights saved in models/OneShot/1/checkpoint-1015/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:10:36,887 >> tokenizer config file saved in models/OneShot/1/checkpoint-1015/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:10:36,888 >> Special tokens file saved in models/OneShot/1/checkpoint-1015/special_tokens_map.json\n",
            " 32% 1160/3625 [15:18<26:56,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:12:24,648 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:12:24,650 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:12:24,650 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:12:24,650 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.24it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.83it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.24it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.42it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.33it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.36it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.53it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.34it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.99it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:03, 15.27it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.26it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.24it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.22it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.20it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.18it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.31it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.16it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.85it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.86it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.08it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.82it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.94it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.86it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0438505411148071, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.855876508684133, 'eval_runtime': 6.1529, 'eval_samples_per_second': 120.106, 'eval_steps_per_second': 15.115, 'epoch': 8.0}\n",
            " 32% 1160/3625 [15:24<26:56,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:12:30,804 >> Saving model checkpoint to models/OneShot/1/checkpoint-1160\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:12:30,805 >> Configuration saved in models/OneShot/1/checkpoint-1160/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:12:33,216 >> Model weights saved in models/OneShot/1/checkpoint-1160/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:12:33,217 >> tokenizer config file saved in models/OneShot/1/checkpoint-1160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:12:33,217 >> Special tokens file saved in models/OneShot/1/checkpoint-1160/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:12:38,926 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1015] due to args.save_total_limit\n",
            " 36% 1305/3625 [17:15<25:25,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:14:21,254 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:14:21,256 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:14:21,256 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:14:21,256 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.77it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.98it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.51it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.32it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.95it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.16it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.41it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.18it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.04it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.92it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.95it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.87it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.12it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 14.99it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.20it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.03it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.91it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.87it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.97it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.17it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.32it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.15it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.19it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.08it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.14it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.10it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.88it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.06it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.23it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.10it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.13it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.103362798690796, 'eval_accuracy': 0.8511502146720886, 'eval_f1': 0.8500604996384777, 'eval_runtime': 6.1484, 'eval_samples_per_second': 120.194, 'eval_steps_per_second': 15.126, 'epoch': 9.0}\n",
            " 36% 1305/3625 [17:21<25:25,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:14:27,406 >> Saving model checkpoint to models/OneShot/1/checkpoint-1305\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:14:27,407 >> Configuration saved in models/OneShot/1/checkpoint-1305/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:14:29,782 >> Model weights saved in models/OneShot/1/checkpoint-1305/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:14:29,783 >> tokenizer config file saved in models/OneShot/1/checkpoint-1305/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:14:29,783 >> Special tokens file saved in models/OneShot/1/checkpoint-1305/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:14:35,481 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1160] due to args.save_total_limit\n",
            " 40% 1450/3625 [19:11<23:48,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:16:17,671 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:16:17,673 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:16:17,673 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:16:17,673 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.63it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.54it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.25it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.73it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.50it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.30it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.36it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.28it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.92it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.73it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.01it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.15it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.01it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.85it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.95it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.10it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.04it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.90it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.05it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.10it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.87it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.08it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1605181694030762, 'eval_accuracy': 0.8525033593177795, 'eval_f1': 0.8524860957738845, 'eval_runtime': 6.1541, 'eval_samples_per_second': 120.082, 'eval_steps_per_second': 15.112, 'epoch': 10.0}\n",
            " 40% 1450/3625 [19:17<23:48,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:16:23,829 >> Saving model checkpoint to models/OneShot/1/checkpoint-1450\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:16:23,830 >> Configuration saved in models/OneShot/1/checkpoint-1450/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:16:26,226 >> Model weights saved in models/OneShot/1/checkpoint-1450/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:16:26,227 >> tokenizer config file saved in models/OneShot/1/checkpoint-1450/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:16:26,227 >> Special tokens file saved in models/OneShot/1/checkpoint-1450/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:16:31,879 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1305] due to args.save_total_limit\n",
            "{'loss': 0.0045, 'learning_rate': 1.1724137931034483e-05, 'epoch': 10.34}\n",
            " 44% 1595/3625 [21:08<22:06,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:18:14,158 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:18:14,160 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:18:14,160 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:18:14,160 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.79it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.41it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.19it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.87it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.65it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.19it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.34it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.36it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.20it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.00it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.10it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 15.10it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:03, 15.27it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.07it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.00it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.12it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.31it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.20it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.07it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.16it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.14it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.19it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.08it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 15.14it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.23it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.08it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.16it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.08it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1376429796218872, 'eval_accuracy': 0.8525033593177795, 'eval_f1': 0.8508417015871135, 'eval_runtime': 6.1284, 'eval_samples_per_second': 120.586, 'eval_steps_per_second': 15.175, 'epoch': 11.0}\n",
            " 44% 1595/3625 [21:14<22:06,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 15.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:18:20,291 >> Saving model checkpoint to models/OneShot/1/checkpoint-1595\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:18:20,292 >> Configuration saved in models/OneShot/1/checkpoint-1595/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:18:22,680 >> Model weights saved in models/OneShot/1/checkpoint-1595/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:18:22,681 >> tokenizer config file saved in models/OneShot/1/checkpoint-1595/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:18:22,681 >> Special tokens file saved in models/OneShot/1/checkpoint-1595/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:18:28,314 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1450] due to args.save_total_limit\n",
            " 48% 1740/3625 [23:04<20:36,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:20:10,633 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:20:10,635 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:20:10,635 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:20:10,635 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.93it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.66it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.22it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.76it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.54it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.03it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.23it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.99it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.74it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.76it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.99it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.91it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.80it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.79it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.93it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.13it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.85it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.87it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.00it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.86it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.83it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1203279495239258, 'eval_accuracy': 0.8606224656105042, 'eval_f1': 0.8602909191024641, 'eval_runtime': 6.178, 'eval_samples_per_second': 119.618, 'eval_steps_per_second': 15.053, 'epoch': 12.0}\n",
            " 48% 1740/3625 [23:10<20:36,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:20:16,814 >> Saving model checkpoint to models/OneShot/1/checkpoint-1740\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:20:16,815 >> Configuration saved in models/OneShot/1/checkpoint-1740/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:20:19,189 >> Model weights saved in models/OneShot/1/checkpoint-1740/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:20:19,190 >> tokenizer config file saved in models/OneShot/1/checkpoint-1740/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:20:19,190 >> Special tokens file saved in models/OneShot/1/checkpoint-1740/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:20:24,868 >> Deleting older checkpoint [models/OneShot/1/checkpoint-870] due to args.save_total_limit\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:20:24,900 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1595] due to args.save_total_limit\n",
            " 52% 1885/3625 [25:01<18:59,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:22:07,207 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:22:07,209 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:22:07,209 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:22:07,209 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.46it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.81it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.35it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.63it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.40it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.10it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.21it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.23it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.82it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.90it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.96it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.03it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.92it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.88it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.92it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.00it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.15it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.17it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.82it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.80it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.85it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.85it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.80it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.90it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.85it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.80it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2687989473342896, 'eval_accuracy': 0.8525033593177795, 'eval_f1': 0.8515892010605113, 'eval_runtime': 6.1858, 'eval_samples_per_second': 119.467, 'eval_steps_per_second': 15.034, 'epoch': 13.0}\n",
            " 52% 1885/3625 [25:07<18:59,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 14.86it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:22:13,396 >> Saving model checkpoint to models/OneShot/1/checkpoint-1885\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:22:13,397 >> Configuration saved in models/OneShot/1/checkpoint-1885/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:22:15,818 >> Model weights saved in models/OneShot/1/checkpoint-1885/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:22:15,819 >> tokenizer config file saved in models/OneShot/1/checkpoint-1885/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:22:15,819 >> Special tokens file saved in models/OneShot/1/checkpoint-1885/special_tokens_map.json\n",
            "{'loss': 0.002, 'learning_rate': 8.965517241379312e-06, 'epoch': 13.79}\n",
            " 56% 2030/3625 [26:57<17:34,  1.51it/s][INFO|trainer.py:723] 2022-08-25 03:24:03,650 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:24:03,652 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:24:03,652 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:24:03,652 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.39it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.48it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.25it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.65it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.38it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.14it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.16it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.96it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.03it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.01it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.97it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.88it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.80it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.98it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.91it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.91it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.94it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.96it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.97it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4814687967300415, 'eval_accuracy': 0.8362652063369751, 'eval_f1': 0.8321290116677462, 'eval_runtime': 6.1826, 'eval_samples_per_second': 119.528, 'eval_steps_per_second': 15.042, 'epoch': 14.0}\n",
            " 56% 2030/3625 [27:03<17:34,  1.51it/s]\n",
            "100% 93/93 [00:06<00:00, 14.90it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:24:09,836 >> Saving model checkpoint to models/OneShot/1/checkpoint-2030\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:24:09,837 >> Configuration saved in models/OneShot/1/checkpoint-2030/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:24:12,241 >> Model weights saved in models/OneShot/1/checkpoint-2030/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:24:12,241 >> tokenizer config file saved in models/OneShot/1/checkpoint-2030/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:24:12,241 >> Special tokens file saved in models/OneShot/1/checkpoint-2030/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:24:17,900 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1885] due to args.save_total_limit\n",
            " 60% 2175/3625 [28:54<15:55,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:26:00,308 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:26:00,310 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:26:00,310 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:26:00,311 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.20it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.02it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.61it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.25it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.99it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.22it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.37it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.35it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.12it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.88it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.81it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.86it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 14.96it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.16it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.86it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.88it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.01it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.00it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.81it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.89it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.99it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.07it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2913707494735718, 'eval_accuracy': 0.8579161167144775, 'eval_f1': 0.8576491890434179, 'eval_runtime': 6.1699, 'eval_samples_per_second': 119.775, 'eval_steps_per_second': 15.073, 'epoch': 15.0}\n",
            " 60% 2175/3625 [29:00<15:55,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.90it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:26:06,483 >> Saving model checkpoint to models/OneShot/1/checkpoint-2175\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:26:06,484 >> Configuration saved in models/OneShot/1/checkpoint-2175/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:26:08,898 >> Model weights saved in models/OneShot/1/checkpoint-2175/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:26:08,899 >> tokenizer config file saved in models/OneShot/1/checkpoint-2175/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:26:08,899 >> Special tokens file saved in models/OneShot/1/checkpoint-2175/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:26:14,642 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2030] due to args.save_total_limit\n",
            " 64% 2320/3625 [30:50<14:15,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:27:57,007 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:27:57,009 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:27:57,009 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:27:57,009 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.76it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.75it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.35it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.69it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.50it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.02it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.11it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.28it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.32it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.16it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.99it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.13it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:03, 15.30it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.20it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.92it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.83it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.86it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.10it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.12it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.95it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.87it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.99it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.90it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.76it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.86it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.92it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.81it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.80it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4544600248336792, 'eval_accuracy': 0.8403247594833374, 'eval_f1': 0.8374067720763723, 'eval_runtime': 6.1666, 'eval_samples_per_second': 119.84, 'eval_steps_per_second': 15.081, 'epoch': 16.0}\n",
            " 64% 2320/3625 [30:57<14:15,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:28:03,177 >> Saving model checkpoint to models/OneShot/1/checkpoint-2320\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:28:03,178 >> Configuration saved in models/OneShot/1/checkpoint-2320/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:28:05,565 >> Model weights saved in models/OneShot/1/checkpoint-2320/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:28:05,566 >> tokenizer config file saved in models/OneShot/1/checkpoint-2320/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:28:05,566 >> Special tokens file saved in models/OneShot/1/checkpoint-2320/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:28:11,226 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2175] due to args.save_total_limit\n",
            " 68% 2465/3625 [32:47<12:39,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:29:53,447 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:29:53,449 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:29:53,449 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:29:53,449 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.70it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.46it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.23it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.65it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.44it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.97it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.11it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.34it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.15it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.12it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 15.01it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.16it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.92it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 15.00it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.07it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.12it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.04it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.18it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.09it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 15.04it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.14it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.12it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.10it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.98it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3255122900009155, 'eval_accuracy': 0.8579161167144775, 'eval_f1': 0.8575781214151332, 'eval_runtime': 6.15, 'eval_samples_per_second': 120.163, 'eval_steps_per_second': 15.122, 'epoch': 17.0}\n",
            " 68% 2465/3625 [32:53<12:39,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 14.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:29:59,601 >> Saving model checkpoint to models/OneShot/1/checkpoint-2465\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:29:59,602 >> Configuration saved in models/OneShot/1/checkpoint-2465/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:30:02,007 >> Model weights saved in models/OneShot/1/checkpoint-2465/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:30:02,008 >> tokenizer config file saved in models/OneShot/1/checkpoint-2465/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:30:02,008 >> Special tokens file saved in models/OneShot/1/checkpoint-2465/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:30:07,649 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2320] due to args.save_total_limit\n",
            "{'loss': 0.002, 'learning_rate': 6.206896551724138e-06, 'epoch': 17.24}\n",
            " 72% 2610/3625 [34:43<11:06,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:31:49,905 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:31:49,907 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:31:49,907 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:31:49,907 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 20.87it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.71it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.36it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.72it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.12it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.23it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.11it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.19it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.99it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.21it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.19it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.96it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.13it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.14it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.11it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.90it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.02it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.18it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.76it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.77it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.91it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.94it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.93it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.00it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 15.03it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 15.04it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.329761266708374, 'eval_accuracy': 0.8552097678184509, 'eval_f1': 0.8542489599480942, 'eval_runtime': 6.1478, 'eval_samples_per_second': 120.205, 'eval_steps_per_second': 15.127, 'epoch': 18.0}\n",
            " 72% 2610/3625 [34:50<11:06,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:31:56,056 >> Saving model checkpoint to models/OneShot/1/checkpoint-2610\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:31:56,057 >> Configuration saved in models/OneShot/1/checkpoint-2610/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:31:58,469 >> Model weights saved in models/OneShot/1/checkpoint-2610/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:31:58,470 >> tokenizer config file saved in models/OneShot/1/checkpoint-2610/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:31:58,470 >> Special tokens file saved in models/OneShot/1/checkpoint-2610/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:32:04,362 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2465] due to args.save_total_limit\n",
            " 76% 2755/3625 [36:40<09:31,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:33:46,639 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:33:46,641 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:33:46,641 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:33:46,641 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.63it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.79it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.22it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.68it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.43it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.16it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.40it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.53it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.29it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.34it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.04it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.96it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.99it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.20it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.31it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.12it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.01it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.88it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.00it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.17it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.20it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.96it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.94it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.90it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.17it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.99it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 15.04it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 15.00it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.06it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3481707572937012, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.8555789085545724, 'eval_runtime': 6.1435, 'eval_samples_per_second': 120.29, 'eval_steps_per_second': 15.138, 'epoch': 19.0}\n",
            " 76% 2755/3625 [36:46<09:31,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:33:52,786 >> Saving model checkpoint to models/OneShot/1/checkpoint-2755\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:33:52,786 >> Configuration saved in models/OneShot/1/checkpoint-2755/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:33:55,184 >> Model weights saved in models/OneShot/1/checkpoint-2755/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:33:55,185 >> tokenizer config file saved in models/OneShot/1/checkpoint-2755/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:33:55,185 >> Special tokens file saved in models/OneShot/1/checkpoint-2755/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:34:00,856 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2610] due to args.save_total_limit\n",
            " 80% 2900/3625 [38:37<07:56,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:35:43,123 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:35:43,125 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:35:43,125 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:35:43,125 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 22.58it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.00it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.46it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.66it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.31it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.97it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.15it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.36it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.31it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.26it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.07it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.86it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.06it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.81it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.84it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.89it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:03, 14.95it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.88it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.04it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.02it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.86it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.88it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.87it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.87it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.07it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.11it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.99it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.06it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3569477796554565, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.8555789085545724, 'eval_runtime': 6.1644, 'eval_samples_per_second': 119.882, 'eval_steps_per_second': 15.087, 'epoch': 20.0}\n",
            " 80% 2900/3625 [38:43<07:56,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.03it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:35:49,291 >> Saving model checkpoint to models/OneShot/1/checkpoint-2900\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:35:49,292 >> Configuration saved in models/OneShot/1/checkpoint-2900/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:35:51,743 >> Model weights saved in models/OneShot/1/checkpoint-2900/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:35:51,743 >> tokenizer config file saved in models/OneShot/1/checkpoint-2900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:35:51,743 >> Special tokens file saved in models/OneShot/1/checkpoint-2900/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:35:57,374 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2755] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 3.448275862068966e-06, 'epoch': 20.69}\n",
            " 84% 3045/3625 [40:33<06:20,  1.53it/s][INFO|trainer.py:723] 2022-08-25 03:37:39,644 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:37:39,645 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:37:39,646 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:37:39,646 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.01it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.78it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.18it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.54it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.30it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 14.99it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.16it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.24it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.12it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.05it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.92it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.03it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.10it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.03it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.05it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.87it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.93it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.95it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.12it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.06it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.07it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.05it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.78it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.74it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.91it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.96it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.92it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.83it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3651947975158691, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.8555789085545724, 'eval_runtime': 6.1682, 'eval_samples_per_second': 119.808, 'eval_steps_per_second': 15.077, 'epoch': 21.0}\n",
            " 84% 3045/3625 [40:39<06:20,  1.53it/s]\n",
            "100% 93/93 [00:06<00:00, 15.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:37:45,815 >> Saving model checkpoint to models/OneShot/1/checkpoint-3045\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:37:45,817 >> Configuration saved in models/OneShot/1/checkpoint-3045/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:37:48,224 >> Model weights saved in models/OneShot/1/checkpoint-3045/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:37:48,225 >> tokenizer config file saved in models/OneShot/1/checkpoint-3045/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:37:48,225 >> Special tokens file saved in models/OneShot/1/checkpoint-3045/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:37:53,887 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2900] due to args.save_total_limit\n",
            " 88% 3190/3625 [42:30<04:45,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:39:36,200 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:39:36,202 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:39:36,202 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:39:36,202 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.68it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.95it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.41it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.62it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.26it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.16it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.34it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.34it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.25it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 14.99it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.86it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.91it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.10it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.26it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.13it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.18it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 14.95it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.73it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.84it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 14.94it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.07it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.86it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.85it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.93it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.92it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.10it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.11it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.08it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.91it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 14.93it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.02it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.11it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.97it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.82it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.85it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.3715019226074219, 'eval_accuracy': 0.8565629124641418, 'eval_f1': 0.8555789085545724, 'eval_runtime': 6.167, 'eval_samples_per_second': 119.832, 'eval_steps_per_second': 15.08, 'epoch': 22.0}\n",
            " 88% 3190/3625 [42:36<04:45,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:39:42,371 >> Saving model checkpoint to models/OneShot/1/checkpoint-3190\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:39:42,372 >> Configuration saved in models/OneShot/1/checkpoint-3190/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:39:44,789 >> Model weights saved in models/OneShot/1/checkpoint-3190/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:39:44,790 >> tokenizer config file saved in models/OneShot/1/checkpoint-3190/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:39:44,790 >> Special tokens file saved in models/OneShot/1/checkpoint-3190/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:39:50,442 >> Deleting older checkpoint [models/OneShot/1/checkpoint-3045] due to args.save_total_limit\n",
            " 92% 3335/3625 [44:26<03:10,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:41:32,797 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:41:32,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:41:32,799 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:41:32,799 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.81it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.31it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.64it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.67it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.36it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.08it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.21it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.37it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.28it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.30it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.14it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.09it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.89it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.86it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.02it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.12it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.17it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.03it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.83it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 14.89it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.04it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.25it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.12it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.18it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.05it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 15.02it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 15.03it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.97it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 15.01it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.05it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.19it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.10it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.08it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.93it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.89it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.97it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.91it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.89it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.84it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4155559539794922, 'eval_accuracy': 0.8511502146720886, 'eval_f1': 0.8496834272696341, 'eval_runtime': 6.1538, 'eval_samples_per_second': 120.088, 'eval_steps_per_second': 15.113, 'epoch': 23.0}\n",
            " 92% 3335/3625 [44:32<03:10,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:41:38,954 >> Saving model checkpoint to models/OneShot/1/checkpoint-3335\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:41:38,955 >> Configuration saved in models/OneShot/1/checkpoint-3335/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:41:41,354 >> Model weights saved in models/OneShot/1/checkpoint-3335/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:41:41,354 >> tokenizer config file saved in models/OneShot/1/checkpoint-3335/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:41:41,354 >> Special tokens file saved in models/OneShot/1/checkpoint-3335/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:41:47,057 >> Deleting older checkpoint [models/OneShot/1/checkpoint-3190] due to args.save_total_limit\n",
            " 96% 3480/3625 [46:23<01:35,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:43:29,360 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:43:29,362 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:43:29,362 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:43:29,362 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 22.19it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 17.26it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.67it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.81it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.59it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.18it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.19it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.33it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.15it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.03it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.90it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 14.92it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.07it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.26it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.12it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.11it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 14.93it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 15.02it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:03, 15.09it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.11it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.12it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 15.11it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 15.08it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.99it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.89it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 15.07it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.99it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.07it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.01it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 14.97it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 15.00it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 14.86it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.80it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 14.95it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:01, 14.98it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.08it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 15.18it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 15.07it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 15.01it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.95it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.88it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4181339740753174, 'eval_accuracy': 0.8511502146720886, 'eval_f1': 0.8496834272696341, 'eval_runtime': 6.1492, 'eval_samples_per_second': 120.177, 'eval_steps_per_second': 15.124, 'epoch': 24.0}\n",
            " 96% 3480/3625 [46:29<01:35,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 14.96it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:43:35,513 >> Saving model checkpoint to models/OneShot/1/checkpoint-3480\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:43:35,514 >> Configuration saved in models/OneShot/1/checkpoint-3480/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:43:37,898 >> Model weights saved in models/OneShot/1/checkpoint-3480/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:43:37,899 >> tokenizer config file saved in models/OneShot/1/checkpoint-3480/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:43:37,899 >> Special tokens file saved in models/OneShot/1/checkpoint-3480/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:43:43,563 >> Deleting older checkpoint [models/OneShot/1/checkpoint-3335] due to args.save_total_limit\n",
            "{'loss': 0.0001, 'learning_rate': 6.896551724137931e-07, 'epoch': 24.14}\n",
            "100% 3625/3625 [48:19<00:00,  1.52it/s][INFO|trainer.py:723] 2022-08-25 03:45:25,944 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:45:25,946 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:45:25,946 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:45:25,946 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:04, 21.53it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:05, 16.51it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:05, 16.15it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:05, 15.46it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:05, 15.44it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:05, 15.09it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:05, 15.24it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 15.27it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 15.21it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 15.10it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 15.06it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 15.05it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:04, 14.94it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:04, 15.08it/s]\u001b[A\n",
            " 34% 32/93 [00:02<00:04, 15.05it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 14.98it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 15.06it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 15.08it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:03, 15.14it/s]\u001b[A\n",
            " 49% 46/93 [00:03<00:03, 15.21it/s]\u001b[A\n",
            " 52% 48/93 [00:03<00:02, 15.09it/s]\u001b[A\n",
            " 54% 50/93 [00:03<00:02, 15.00it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 14.98it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 14.74it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 14.68it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 14.71it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 14.92it/s]\u001b[A\n",
            " 67% 62/93 [00:04<00:02, 14.96it/s]\u001b[A\n",
            " 69% 64/93 [00:04<00:01, 15.08it/s]\u001b[A\n",
            " 71% 66/93 [00:04<00:01, 15.06it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 15.03it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 14.94it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 15.12it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 14.98it/s]\u001b[A\n",
            " 82% 76/93 [00:05<00:01, 15.17it/s]\u001b[A\n",
            " 84% 78/93 [00:05<00:00, 15.05it/s]\u001b[A\n",
            " 86% 80/93 [00:05<00:00, 15.13it/s]\u001b[A\n",
            " 88% 82/93 [00:05<00:00, 14.96it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 14.82it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 14.78it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 14.87it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 14.94it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.4020642042160034, 'eval_accuracy': 0.8525033593177795, 'eval_f1': 0.8511680071541939, 'eval_runtime': 6.1668, 'eval_samples_per_second': 119.834, 'eval_steps_per_second': 15.081, 'epoch': 25.0}\n",
            "100% 3625/3625 [48:26<00:00,  1.52it/s]\n",
            "100% 93/93 [00:06<00:00, 15.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2022-08-25 03:45:32,114 >> Saving model checkpoint to models/OneShot/1/checkpoint-3625\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:45:32,115 >> Configuration saved in models/OneShot/1/checkpoint-3625/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:45:34,506 >> Model weights saved in models/OneShot/1/checkpoint-3625/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:45:34,506 >> tokenizer config file saved in models/OneShot/1/checkpoint-3625/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:45:34,506 >> Special tokens file saved in models/OneShot/1/checkpoint-3625/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2022-08-25 03:45:40,160 >> Deleting older checkpoint [models/OneShot/1/checkpoint-3480] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2022-08-25 03:45:40,333 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1942] 2022-08-25 03:45:40,333 >> Loading best model from models/OneShot/1/checkpoint-1740 (score: 0.8602909191024641).\n",
            "{'train_runtime': 2918.2817, 'train_samples_per_second': 39.672, 'train_steps_per_second': 1.242, 'train_loss': 0.025873314917858305, 'epoch': 25.0}\n",
            "100% 3625/3625 [48:38<00:00,  1.24it/s]\n",
            "[INFO|trainer.py:2640] 2022-08-25 03:45:44,284 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:451] 2022-08-25 03:45:44,285 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-08-25 03:45:46,696 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-08-25 03:45:46,697 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-08-25 03:45:46,697 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0259\n",
            "  train_runtime            = 0:48:38.28\n",
            "  train_samples            =       4631\n",
            "  train_samples_per_second =     39.672\n",
            "  train_steps_per_second   =      1.242\n",
            "08/25/2022 03:45:46 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:723] 2022-08-25 03:45:46,967 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2022-08-25 03:45:46,969 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-08-25 03:45:46,969 >>   Num examples = 739\n",
            "[INFO|trainer.py:2896] 2022-08-25 03:45:46,969 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 15.83it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.8606\n",
            "  eval_f1                 =     0.8603\n",
            "  eval_loss               =     1.1203\n",
            "  eval_runtime            = 0:00:05.95\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    124.074\n",
            "  eval_steps_per_second   =     15.614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for one-shot english idiomaticity detection"
      ],
      "metadata": {
        "id": "AzC-WNAbREEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install adapter-transformers\n",
        "!pip install git+https://github.com/adapter-hub/adapter-transformers.git\n",
        "\n",
        "# https://adapterhub.ml/explore/text_lang/"
      ],
      "metadata": {
        "id": "8XpLBCTUSuup",
        "outputId": "2f6e1d11-6364-40bb-8cf7-6129beadf1ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/adapter-hub/adapter-transformers.git\n",
            "  Cloning https://github.com/adapter-hub/adapter-transformers.git to /tmp/pip-req-build-91j13268\n",
            "  Running command git clone -q https://github.com/adapter-hub/adapter-transformers.git /tmp/pip-req-build-91j13268\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<0.8.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==3.1.0a0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.8.0,>=0.1.0->adapter-transformers==3.1.0a0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->adapter-transformers==3.1.0a0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers==3.1.0a0) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==3.1.0a0) (1.25.11)\n",
            "Building wheels for collected packages: adapter-transformers\n",
            "  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adapter-transformers: filename=adapter_transformers-3.1.0a0-py3-none-any.whl size=4268660 sha256=10c8024a801067a376d61baaf1e404eaafdef4a506d203aa19d0229c33ceed1c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c1brb_3v/wheels/0e/10/59/2e0642953eade6187066db14e31def5345c4ed7d757d32de99\n",
            "Successfully built adapter-transformers\n",
            "Installing collected packages: huggingface-hub, adapter-transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.9.0\n",
            "    Uninstalling huggingface-hub-0.9.0:\n",
            "      Successfully uninstalled huggingface-hub-0.9.0\n",
            "Successfully installed adapter-transformers-3.1.0a0 huggingface-hub-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/OneShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/OneShot/EN/train.csv \\\n",
        "  --validation_file Data/OneShot/EN/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "id": "CX4cERiTRCjn",
        "outputId": "9d761a12-3930-4dbd-c2dd-ce07259895f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 04:27:09 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 04:27:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_04-27-09_42ef7f7e098f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 04:27:09 - INFO - __main__ - load a local file for train: Data/OneShot/EN/train.csv\n",
            "08/25/2022 04:27:09 - INFO - __main__ - load a local file for validation: Data/OneShot/EN/dev.csv\n",
            "08/25/2022 04:27:09 - WARNING - datasets.builder - Using custom data configuration default-7b1b30830fbd45bb\n",
            "08/25/2022 04:27:09 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-7b1b30830fbd45bb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7b1b30830fbd45bb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 11781.75it/s]\n",
            "08/25/2022 04:27:09 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 04:27:09 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 877.93it/s]\n",
            "08/25/2022 04:27:09 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 04:27:09 - INFO - datasets.builder - Generating train split\n",
            "08/25/2022 04:27:09 - INFO - datasets.builder - Generating validation split\n",
            "08/25/2022 04:27:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7b1b30830fbd45bb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 947.44it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 04:27:09,965 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 04:27:09,966 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 04:27:10,226 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 04:27:10,227 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 04:27:10,987 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 04:27:10,988 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 04:27:10,988 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 04:27:10,988 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 04:27:10,988 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 04:27:11,113 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 04:27:11,114 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 04:27:11,412 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 04:27:16,346 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 04:27:16,346 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 04:27:16,356 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 04:27:16,405 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 04:27:16,603 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:327] 2022-08-25 04:27:16,732 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 04:27:16,852 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 04:27:17,707 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 04:27:17,709 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:27:17,947 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 04:27:17,959 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 04:27:17,959 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 04:27:18,008 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 04:27:18,137 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 04:27:19,208 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 04:27:19,210 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:27:19,495 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:27:19,508 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 04:27:19,509 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 04:27:20,688 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 04:27:22,373 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 04:27:22,440 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with EN language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]08/25/2022 04:27:22 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7b1b30830fbd45bb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-ea4b4eec4f3bc05c.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  9.82ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 04:27:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7b1b30830fbd45bb/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-13efe05a0542cc45.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 18.40ba/s]\n",
            "08/25/2022 04:27:23 - INFO - __main__ - Sample 550 of the training set: {'label': 0, 'sentence1': 'When in a current bull market it can be difficult to ask the question: when will the bull market end?', 'sentence2': 'bull market', 'input_ids': [101, 12242, 10106, 169, 14978, 11499, 11231, 17313, 10271, 10944, 10347, 25232, 10114, 63001, 10105, 20210, 131, 10841, 11337, 10105, 11499, 11231, 17313, 11572, 136, 102, 11499, 11231, 17313, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 04:27:23 - INFO - __main__ - Sample 2331 of the training set: {'label': 1, 'sentence1': 'The Pain Killer consists of light and dark rums, pineapple and orange juices, cream of coconut and nutmeg — it’s blended, making it the ideal drink to sip on the beach at the lakefront resort.', 'sentence2': 'pain killer', 'input_ids': [101, 10117, 44170, 38066, 20963, 10108, 15765, 10111, 25100, 52522, 10107, 117, 102150, 102295, 10284, 10111, 41435, 23005, 39801, 117, 93461, 10108, 11170, 23486, 11159, 10111, 11085, 10123, 10627, 10240, 100, 10271, 100, 187, 10718, 53556, 117, 14293, 10271, 10105, 29580, 69423, 10114, 10294, 10410, 10135, 10105, 45405, 10160, 10105, 24923, 31044, 60637, 119, 102, 38576, 61976, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 04:27:23 - INFO - __main__ - Sample 3286 of the training set: {'label': 1, 'sentence1': 'Renovations are scheduled to begin Monday for the P4 parking lot in Colonial Williamsburg, located behind the Merchants Square bus stop on South Henry Street, which will reopen as paid parking.', 'sentence2': 'parking lot', 'input_ids': [101, 46965, 39784, 10107, 10301, 34081, 10114, 16135, 40714, 10142, 10105, 153, 11011, 23975, 19826, 10106, 50422, 12494, 12248, 117, 11954, 17155, 10105, 91101, 10107, 19465, 19369, 20517, 10135, 11056, 11601, 11962, 117, 10319, 11337, 11639, 47656, 10146, 25938, 23975, 119, 102, 23975, 19826, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 04:27:28,282 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 04:27:28,297 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 04:27:28,297 >>   Num examples = 3414\n",
            "[INFO|trainer.py:1421] 2022-08-25 04:27:28,297 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 04:27:28,297 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 04:27:28,297 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 04:27:28,297 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 04:27:28,297 >>   Total optimization steps = 2675\n",
            "  4% 107/2675 [00:59<21:52,  1.96it/s][INFO|trainer.py:623] 2022-08-25 04:28:28,037 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:28:28,039 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:28:28,039 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:28:28,039 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.55it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.76it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.54it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.99it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.61it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.28it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.26it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.19it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 13.22it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 13.16it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 13.15it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 13.09it/s]\u001b[A\n",
            " 46% 27/59 [00:01<00:02, 13.11it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 13.07it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 13.08it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:01, 13.07it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 13.08it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 13.08it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 13.07it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 13.07it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 13.05it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 13.09it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 13.07it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 13.07it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 13.05it/s]\u001b[A\n",
            " 90% 53/59 [00:03<00:00, 13.05it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 13.03it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 13.07it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7402487397193909, 'eval_accuracy': 0.7682403326034546, 'eval_f1': 0.7655411449016101, 'eval_runtime': 4.4627, 'eval_samples_per_second': 104.421, 'eval_steps_per_second': 13.221, 'epoch': 1.0}\n",
            "  4% 107/2675 [01:04<21:52,  1.96it/s]\n",
            "100% 59/59 [00:04<00:00, 14.53it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:28:32,503 >> Saving model checkpoint to models/OneShot/1/checkpoint-107\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:32,504 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:32,643 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:32,643 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:32,652 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:32,653 >> Configuration saved in models/OneShot/1/checkpoint-107/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:32,755 >> Module weights saved in models/OneShot/1/checkpoint-107/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:32,756 >> Configuration saved in models/OneShot/1/checkpoint-107/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:32,897 >> Module weights saved in models/OneShot/1/checkpoint-107/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:32,898 >> Configuration saved in models/OneShot/1/checkpoint-107/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:34,483 >> Module weights saved in models/OneShot/1/checkpoint-107/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:34,573 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:34,589 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:28:34,590 >> Configuration saved in models/OneShot/1/checkpoint-107/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:28:35,959 >> Module weights saved in models/OneShot/1/checkpoint-107/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:28:35,959 >> tokenizer config file saved in models/OneShot/1/checkpoint-107/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:28:35,959 >> Special tokens file saved in models/OneShot/1/checkpoint-107/special_tokens_map.json\n",
            "  8% 214/2675 [02:08<21:26,  1.91it/s][INFO|trainer.py:623] 2022-08-25 04:29:36,418 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:29:36,420 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:29:36,420 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:29:36,420 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.31it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.56it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.26it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.69it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.40it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.16it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.06it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 13.00it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.95it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.89it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.83it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.83it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.78it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.75it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.79it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.81it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.71it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.63it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.64it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.72it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.73it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.74it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.78it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.78it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.77it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.66it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.75it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.77it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7899565696716309, 'eval_accuracy': 0.7553648352622986, 'eval_f1': 0.7347195589645255, 'eval_runtime': 4.566, 'eval_samples_per_second': 102.058, 'eval_steps_per_second': 12.921, 'epoch': 2.0}\n",
            "  8% 214/2675 [02:12<21:26,  1.91it/s]\n",
            "100% 59/59 [00:04<00:00, 14.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:29:40,987 >> Saving model checkpoint to models/OneShot/1/checkpoint-214\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:40,988 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:41,078 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:41,078 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:41,087 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:41,088 >> Configuration saved in models/OneShot/1/checkpoint-214/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:41,142 >> Module weights saved in models/OneShot/1/checkpoint-214/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:41,143 >> Configuration saved in models/OneShot/1/checkpoint-214/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:41,202 >> Module weights saved in models/OneShot/1/checkpoint-214/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:41,203 >> Configuration saved in models/OneShot/1/checkpoint-214/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:42,534 >> Module weights saved in models/OneShot/1/checkpoint-214/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:42,566 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:42,640 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:29:42,640 >> Configuration saved in models/OneShot/1/checkpoint-214/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:29:43,980 >> Module weights saved in models/OneShot/1/checkpoint-214/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:29:43,981 >> tokenizer config file saved in models/OneShot/1/checkpoint-214/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:29:43,981 >> Special tokens file saved in models/OneShot/1/checkpoint-214/special_tokens_map.json\n",
            " 12% 321/2675 [03:16<20:25,  1.92it/s][INFO|trainer.py:623] 2022-08-25 04:30:44,943 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:30:44,945 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:30:44,945 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:30:44,945 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.16it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.29it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.98it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.56it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.24it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.09it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 13.01it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.95it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.88it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.83it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.80it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.83it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.83it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.80it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.86it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.84it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.76it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.82it/s]\u001b[A\n",
            " 66% 39/59 [00:02<00:01, 12.81it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.73it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.75it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.81it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.79it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.78it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.81it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.80it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.75it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.81it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8075490593910217, 'eval_accuracy': 0.8047210574150085, 'eval_f1': 0.802161915025636, 'eval_runtime': 4.5612, 'eval_samples_per_second': 102.167, 'eval_steps_per_second': 12.935, 'epoch': 3.0}\n",
            " 12% 321/2675 [03:21<20:25,  1.92it/s]\n",
            "100% 59/59 [00:04<00:00, 14.29it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:30:49,507 >> Saving model checkpoint to models/OneShot/1/checkpoint-321\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:49,508 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:49,595 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:49,595 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:49,604 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:49,605 >> Configuration saved in models/OneShot/1/checkpoint-321/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:49,663 >> Module weights saved in models/OneShot/1/checkpoint-321/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:49,663 >> Configuration saved in models/OneShot/1/checkpoint-321/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:49,724 >> Module weights saved in models/OneShot/1/checkpoint-321/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:49,725 >> Configuration saved in models/OneShot/1/checkpoint-321/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:51,078 >> Module weights saved in models/OneShot/1/checkpoint-321/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:51,212 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:51,236 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:30:51,236 >> Configuration saved in models/OneShot/1/checkpoint-321/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:30:52,611 >> Module weights saved in models/OneShot/1/checkpoint-321/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:30:52,612 >> tokenizer config file saved in models/OneShot/1/checkpoint-321/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:30:52,612 >> Special tokens file saved in models/OneShot/1/checkpoint-321/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:30:53,090 >> Deleting older checkpoint [models/OneShot/1/checkpoint-107] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:30:53,165 >> Deleting older checkpoint [models/OneShot/1/checkpoint-214] due to args.save_total_limit\n",
            " 16% 428/2675 [04:25<19:41,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:31:54,043 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:31:54,045 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:31:54,045 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:31:54,045 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.87it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.90it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.65it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.17it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.74it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.66it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.68it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.69it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.63it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.56it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.56it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.51it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.48it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.44it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.47it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.62it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.59it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.60it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.52it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.63it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5650470852851868, 'eval_accuracy': 0.8047210574150085, 'eval_f1': 0.8008462835378598, 'eval_runtime': 4.6469, 'eval_samples_per_second': 100.281, 'eval_steps_per_second': 12.697, 'epoch': 4.0}\n",
            " 16% 428/2675 [04:30<19:41,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:31:58,693 >> Saving model checkpoint to models/OneShot/1/checkpoint-428\n",
            "[INFO|loading.py:60] 2022-08-25 04:31:58,694 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:31:58,784 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:31:58,785 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:31:58,793 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:31:58,794 >> Configuration saved in models/OneShot/1/checkpoint-428/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:31:58,852 >> Module weights saved in models/OneShot/1/checkpoint-428/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:31:58,853 >> Configuration saved in models/OneShot/1/checkpoint-428/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:31:58,911 >> Module weights saved in models/OneShot/1/checkpoint-428/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:31:58,912 >> Configuration saved in models/OneShot/1/checkpoint-428/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:32:00,214 >> Module weights saved in models/OneShot/1/checkpoint-428/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:32:00,222 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:32:00,326 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:32:00,326 >> Configuration saved in models/OneShot/1/checkpoint-428/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:32:01,904 >> Module weights saved in models/OneShot/1/checkpoint-428/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:32:01,905 >> tokenizer config file saved in models/OneShot/1/checkpoint-428/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:32:01,905 >> Special tokens file saved in models/OneShot/1/checkpoint-428/special_tokens_map.json\n",
            "{'loss': 0.1414, 'learning_rate': 8.130841121495327e-05, 'epoch': 4.67}\n",
            " 20% 535/2675 [05:35<18:44,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:33:03,392 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:33:03,394 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:33:03,394 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:33:03,394 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.36it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.31it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.00it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.49it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.00it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.77it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.78it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.79it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.76it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.76it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.66it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.63it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.74it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.72it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.67it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.72it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.74it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.70it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.70it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.76it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.75it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.72it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.71it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.71it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.75it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.70it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.69it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7803714871406555, 'eval_accuracy': 0.832617998123169, 'eval_f1': 0.8313756054111229, 'eval_runtime': 4.5966, 'eval_samples_per_second': 101.379, 'eval_steps_per_second': 12.836, 'epoch': 5.0}\n",
            " 20% 535/2675 [05:39<18:44,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.19it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:33:07,992 >> Saving model checkpoint to models/OneShot/1/checkpoint-535\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:07,993 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:08,083 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:08,083 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:08,090 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:08,091 >> Configuration saved in models/OneShot/1/checkpoint-535/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:08,148 >> Module weights saved in models/OneShot/1/checkpoint-535/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:08,148 >> Configuration saved in models/OneShot/1/checkpoint-535/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:08,209 >> Module weights saved in models/OneShot/1/checkpoint-535/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:08,209 >> Configuration saved in models/OneShot/1/checkpoint-535/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:09,579 >> Module weights saved in models/OneShot/1/checkpoint-535/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:09,580 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:09,595 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:33:09,595 >> Configuration saved in models/OneShot/1/checkpoint-535/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:33:11,214 >> Module weights saved in models/OneShot/1/checkpoint-535/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:33:11,215 >> tokenizer config file saved in models/OneShot/1/checkpoint-535/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:33:11,215 >> Special tokens file saved in models/OneShot/1/checkpoint-535/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:33:11,681 >> Deleting older checkpoint [models/OneShot/1/checkpoint-321] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:33:11,752 >> Deleting older checkpoint [models/OneShot/1/checkpoint-428] due to args.save_total_limit\n",
            " 24% 642/2675 [06:44<17:50,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:34:12,855 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:34:12,857 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:34:12,857 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:34:12,857 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.13it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.25it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.09it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.61it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.16it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.86it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.79it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.55it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.64it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.62it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.44it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.36it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.39it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.43it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.47it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.49it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.47it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.44it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.43it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.52it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8603980541229248, 'eval_accuracy': 0.8390557765960693, 'eval_f1': 0.8366112375939077, 'eval_runtime': 4.6469, 'eval_samples_per_second': 100.283, 'eval_steps_per_second': 12.697, 'epoch': 6.0}\n",
            " 24% 642/2675 [06:49<17:50,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 13.99it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:34:17,505 >> Saving model checkpoint to models/OneShot/1/checkpoint-642\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:17,506 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:17,601 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:17,602 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:17,610 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:17,610 >> Configuration saved in models/OneShot/1/checkpoint-642/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:17,667 >> Module weights saved in models/OneShot/1/checkpoint-642/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:17,668 >> Configuration saved in models/OneShot/1/checkpoint-642/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:17,725 >> Module weights saved in models/OneShot/1/checkpoint-642/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:17,726 >> Configuration saved in models/OneShot/1/checkpoint-642/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:19,158 >> Module weights saved in models/OneShot/1/checkpoint-642/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:19,261 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:19,289 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:34:19,290 >> Configuration saved in models/OneShot/1/checkpoint-642/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:34:20,595 >> Module weights saved in models/OneShot/1/checkpoint-642/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:34:20,596 >> tokenizer config file saved in models/OneShot/1/checkpoint-642/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:34:20,596 >> Special tokens file saved in models/OneShot/1/checkpoint-642/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:34:21,097 >> Deleting older checkpoint [models/OneShot/1/checkpoint-535] due to args.save_total_limit\n",
            " 28% 749/2675 [07:53<16:59,  1.89it/s][INFO|trainer.py:623] 2022-08-25 04:35:22,168 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:35:22,170 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:35:22,170 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:35:22,170 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.34it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.27it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.94it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.45it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.95it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.77it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.74it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.79it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.73it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.73it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.57it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.47it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.49it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.45it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.36it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.40it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.42it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.50it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.56it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.70it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.68it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.59it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7558469772338867, 'eval_accuracy': 0.8540772795677185, 'eval_f1': 0.8447884012539184, 'eval_runtime': 4.6413, 'eval_samples_per_second': 100.402, 'eval_steps_per_second': 12.712, 'epoch': 7.0}\n",
            " 28% 749/2675 [07:58<16:59,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 14.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:35:26,813 >> Saving model checkpoint to models/OneShot/1/checkpoint-749\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:26,814 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:26,905 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:26,906 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:26,913 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:26,913 >> Configuration saved in models/OneShot/1/checkpoint-749/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:26,964 >> Module weights saved in models/OneShot/1/checkpoint-749/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:26,965 >> Configuration saved in models/OneShot/1/checkpoint-749/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:27,027 >> Module weights saved in models/OneShot/1/checkpoint-749/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:27,027 >> Configuration saved in models/OneShot/1/checkpoint-749/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:28,412 >> Module weights saved in models/OneShot/1/checkpoint-749/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:28,455 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:28,468 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:35:28,474 >> Configuration saved in models/OneShot/1/checkpoint-749/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:35:29,812 >> Module weights saved in models/OneShot/1/checkpoint-749/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:35:29,813 >> tokenizer config file saved in models/OneShot/1/checkpoint-749/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:35:29,813 >> Special tokens file saved in models/OneShot/1/checkpoint-749/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:35:30,333 >> Deleting older checkpoint [models/OneShot/1/checkpoint-642] due to args.save_total_limit\n",
            " 32% 856/2675 [09:03<16:02,  1.89it/s][INFO|trainer.py:623] 2022-08-25 04:36:31,511 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:36:31,513 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:36:31,513 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:36:31,514 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.96it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.41it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.08it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.55it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.12it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.81it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.73it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.68it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.70it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.60it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.60it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.66it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.57it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.50it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.42it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.41it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.43it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.52it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.47it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.9650758504867554, 'eval_accuracy': 0.8347639441490173, 'eval_f1': 0.8232370255425011, 'eval_runtime': 4.6388, 'eval_samples_per_second': 100.456, 'eval_steps_per_second': 12.719, 'epoch': 8.0}\n",
            " 32% 856/2675 [09:07<16:02,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 13.92it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:36:36,154 >> Saving model checkpoint to models/OneShot/1/checkpoint-856\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:36,154 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:36,244 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:36,244 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:36,252 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:36,252 >> Configuration saved in models/OneShot/1/checkpoint-856/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:36,305 >> Module weights saved in models/OneShot/1/checkpoint-856/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:36,305 >> Configuration saved in models/OneShot/1/checkpoint-856/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:36,369 >> Module weights saved in models/OneShot/1/checkpoint-856/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:36,370 >> Configuration saved in models/OneShot/1/checkpoint-856/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:37,772 >> Module weights saved in models/OneShot/1/checkpoint-856/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:37,806 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:37,823 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:36:37,824 >> Configuration saved in models/OneShot/1/checkpoint-856/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:36:39,162 >> Module weights saved in models/OneShot/1/checkpoint-856/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:36:39,163 >> tokenizer config file saved in models/OneShot/1/checkpoint-856/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:36:39,163 >> Special tokens file saved in models/OneShot/1/checkpoint-856/special_tokens_map.json\n",
            " 36% 963/2675 [10:12<14:59,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:37:40,641 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:37:40,643 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:37:40,643 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:37:40,643 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.42it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.50it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.12it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.57it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.11it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.01it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.93it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.87it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.88it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.85it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.75it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.70it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.51it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.53it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.53it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.60it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.67it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.65it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.66it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.63it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8829788565635681, 'eval_accuracy': 0.8497853875160217, 'eval_f1': 0.8474217931447388, 'eval_runtime': 4.6071, 'eval_samples_per_second': 101.148, 'eval_steps_per_second': 12.806, 'epoch': 9.0}\n",
            " 36% 963/2675 [10:16<14:59,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:37:45,251 >> Saving model checkpoint to models/OneShot/1/checkpoint-963\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:45,252 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:45,340 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:45,341 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:45,349 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:45,350 >> Configuration saved in models/OneShot/1/checkpoint-963/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:45,473 >> Module weights saved in models/OneShot/1/checkpoint-963/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:45,474 >> Configuration saved in models/OneShot/1/checkpoint-963/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:45,530 >> Module weights saved in models/OneShot/1/checkpoint-963/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:45,530 >> Configuration saved in models/OneShot/1/checkpoint-963/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:46,642 >> Module weights saved in models/OneShot/1/checkpoint-963/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:46,804 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:46,821 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:37:46,822 >> Configuration saved in models/OneShot/1/checkpoint-963/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:37:48,255 >> Module weights saved in models/OneShot/1/checkpoint-963/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:37:48,255 >> tokenizer config file saved in models/OneShot/1/checkpoint-963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:37:48,256 >> Special tokens file saved in models/OneShot/1/checkpoint-963/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:37:48,734 >> Deleting older checkpoint [models/OneShot/1/checkpoint-749] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:37:48,802 >> Deleting older checkpoint [models/OneShot/1/checkpoint-856] due to args.save_total_limit\n",
            "{'loss': 0.0225, 'learning_rate': 6.261682242990654e-05, 'epoch': 9.35}\n",
            " 40% 1070/2675 [11:21<14:02,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:38:50,009 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:38:50,011 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:38:50,011 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:38:50,011 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.02it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.04it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.82it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.41it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.14it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.04it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.97it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.88it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.80it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.74it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.61it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.68it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.65it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.55it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.60it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.56it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.62it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.68it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.70it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.71it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8469817042350769, 'eval_accuracy': 0.8669527769088745, 'eval_f1': 0.862032739289057, 'eval_runtime': 4.6064, 'eval_samples_per_second': 101.163, 'eval_steps_per_second': 12.808, 'epoch': 10.0}\n",
            " 40% 1070/2675 [11:26<14:02,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.18it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:38:54,619 >> Saving model checkpoint to models/OneShot/1/checkpoint-1070\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:54,620 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:54,711 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:54,711 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:54,719 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:54,719 >> Configuration saved in models/OneShot/1/checkpoint-1070/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:54,772 >> Module weights saved in models/OneShot/1/checkpoint-1070/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:54,772 >> Configuration saved in models/OneShot/1/checkpoint-1070/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:54,838 >> Module weights saved in models/OneShot/1/checkpoint-1070/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:54,838 >> Configuration saved in models/OneShot/1/checkpoint-1070/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:56,249 >> Module weights saved in models/OneShot/1/checkpoint-1070/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:56,313 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:56,334 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:38:56,334 >> Configuration saved in models/OneShot/1/checkpoint-1070/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:38:57,906 >> Module weights saved in models/OneShot/1/checkpoint-1070/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:38:57,907 >> tokenizer config file saved in models/OneShot/1/checkpoint-1070/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:38:57,907 >> Special tokens file saved in models/OneShot/1/checkpoint-1070/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:38:58,380 >> Deleting older checkpoint [models/OneShot/1/checkpoint-963] due to args.save_total_limit\n",
            " 44% 1177/2675 [12:31<13:08,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:39:59,565 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:39:59,567 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:39:59,567 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:39:59,567 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.42it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.33it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.99it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.52it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.03it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.69it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.68it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.58it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.56it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.58it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.51it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.57it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.60it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.60it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.65it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.63it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.68it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.68it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.864337682723999, 'eval_accuracy': 0.8712446093559265, 'eval_f1': 0.8689243924392439, 'eval_runtime': 4.6318, 'eval_samples_per_second': 100.608, 'eval_steps_per_second': 12.738, 'epoch': 11.0}\n",
            " 44% 1177/2675 [12:35<13:08,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:40:04,200 >> Saving model checkpoint to models/OneShot/1/checkpoint-1177\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:04,200 >> Configuration saved in models/OneShot/1/checkpoint-1177/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:04,291 >> Module weights saved in models/OneShot/1/checkpoint-1177/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:04,292 >> Configuration saved in models/OneShot/1/checkpoint-1177/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:04,299 >> Module weights saved in models/OneShot/1/checkpoint-1177/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:04,300 >> Configuration saved in models/OneShot/1/checkpoint-1177/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:04,352 >> Module weights saved in models/OneShot/1/checkpoint-1177/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:04,352 >> Configuration saved in models/OneShot/1/checkpoint-1177/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:04,429 >> Module weights saved in models/OneShot/1/checkpoint-1177/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:04,429 >> Configuration saved in models/OneShot/1/checkpoint-1177/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:05,892 >> Module weights saved in models/OneShot/1/checkpoint-1177/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:05,893 >> Configuration saved in models/OneShot/1/checkpoint-1177/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:05,903 >> Module weights saved in models/OneShot/1/checkpoint-1177/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:40:05,903 >> Configuration saved in models/OneShot/1/checkpoint-1177/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:40:07,476 >> Module weights saved in models/OneShot/1/checkpoint-1177/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:40:07,476 >> tokenizer config file saved in models/OneShot/1/checkpoint-1177/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:40:07,477 >> Special tokens file saved in models/OneShot/1/checkpoint-1177/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:40:07,937 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1070] due to args.save_total_limit\n",
            " 48% 1284/2675 [13:40<12:14,  1.89it/s][INFO|trainer.py:623] 2022-08-25 04:41:09,070 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:41:09,071 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:41:09,072 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:41:09,072 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.17it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.31it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.14it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.64it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.18it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.85it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.78it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.63it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.53it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.59it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.65it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.61it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.59it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.47it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.49it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.53it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.68it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.70it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.62it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.64it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.66it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0474989414215088, 'eval_accuracy': 0.8605149984359741, 'eval_f1': 0.8536396801236984, 'eval_runtime': 4.6273, 'eval_samples_per_second': 100.707, 'eval_steps_per_second': 12.75, 'epoch': 12.0}\n",
            " 48% 1284/2675 [13:45<12:14,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 14.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:41:13,700 >> Saving model checkpoint to models/OneShot/1/checkpoint-1284\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:13,701 >> Configuration saved in models/OneShot/1/checkpoint-1284/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:13,794 >> Module weights saved in models/OneShot/1/checkpoint-1284/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:13,795 >> Configuration saved in models/OneShot/1/checkpoint-1284/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:13,802 >> Module weights saved in models/OneShot/1/checkpoint-1284/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:13,803 >> Configuration saved in models/OneShot/1/checkpoint-1284/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:13,855 >> Module weights saved in models/OneShot/1/checkpoint-1284/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:13,856 >> Configuration saved in models/OneShot/1/checkpoint-1284/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:13,923 >> Module weights saved in models/OneShot/1/checkpoint-1284/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:13,924 >> Configuration saved in models/OneShot/1/checkpoint-1284/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:15,407 >> Module weights saved in models/OneShot/1/checkpoint-1284/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:15,414 >> Configuration saved in models/OneShot/1/checkpoint-1284/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:15,490 >> Module weights saved in models/OneShot/1/checkpoint-1284/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:41:15,504 >> Configuration saved in models/OneShot/1/checkpoint-1284/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:41:16,953 >> Module weights saved in models/OneShot/1/checkpoint-1284/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:41:16,954 >> tokenizer config file saved in models/OneShot/1/checkpoint-1284/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:41:16,954 >> Special tokens file saved in models/OneShot/1/checkpoint-1284/special_tokens_map.json\n",
            " 52% 1391/2675 [14:50<11:13,  1.91it/s][INFO|trainer.py:623] 2022-08-25 04:42:18,590 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:42:18,606 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:42:18,606 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:42:18,606 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 18.61it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.81it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.53it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.23it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.91it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.60it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.70it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.60it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.67it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.67it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.60it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.66it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.62it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.61it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.70it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.60it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.57it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.47it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.45it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.41it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.33it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8521931767463684, 'eval_accuracy': 0.8841201663017273, 'eval_f1': 0.8806012640209531, 'eval_runtime': 4.6676, 'eval_samples_per_second': 99.837, 'eval_steps_per_second': 12.64, 'epoch': 13.0}\n",
            " 52% 1391/2675 [14:54<11:13,  1.91it/s]\n",
            "100% 59/59 [00:04<00:00, 13.48it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:42:23,275 >> Saving model checkpoint to models/OneShot/1/checkpoint-1391\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:23,276 >> Configuration saved in models/OneShot/1/checkpoint-1391/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:23,405 >> Module weights saved in models/OneShot/1/checkpoint-1391/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:23,406 >> Configuration saved in models/OneShot/1/checkpoint-1391/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:23,421 >> Module weights saved in models/OneShot/1/checkpoint-1391/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:23,422 >> Configuration saved in models/OneShot/1/checkpoint-1391/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:23,499 >> Module weights saved in models/OneShot/1/checkpoint-1391/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:23,500 >> Configuration saved in models/OneShot/1/checkpoint-1391/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:23,644 >> Module weights saved in models/OneShot/1/checkpoint-1391/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:23,645 >> Configuration saved in models/OneShot/1/checkpoint-1391/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:24,982 >> Module weights saved in models/OneShot/1/checkpoint-1391/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:24,983 >> Configuration saved in models/OneShot/1/checkpoint-1391/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:25,188 >> Module weights saved in models/OneShot/1/checkpoint-1391/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:42:25,188 >> Configuration saved in models/OneShot/1/checkpoint-1391/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:42:26,589 >> Module weights saved in models/OneShot/1/checkpoint-1391/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:42:26,590 >> tokenizer config file saved in models/OneShot/1/checkpoint-1391/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:42:26,590 >> Special tokens file saved in models/OneShot/1/checkpoint-1391/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:42:27,079 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1177] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:42:27,139 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1284] due to args.save_total_limit\n",
            " 56% 1498/2675 [15:59<10:18,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:43:28,202 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:43:28,204 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:43:28,204 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:43:28,204 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.33it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.47it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.05it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.11it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.90it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.91it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.89it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.81it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.84it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.75it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.67it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.63it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.74it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.71it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.60it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.52it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.50it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.52it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.65it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.58it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.63it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.68it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8869900107383728, 'eval_accuracy': 0.8905579447746277, 'eval_f1': 0.8867913743087015, 'eval_runtime': 4.6115, 'eval_samples_per_second': 101.051, 'eval_steps_per_second': 12.794, 'epoch': 14.0}\n",
            " 56% 1498/2675 [16:04<10:18,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.08it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:43:32,817 >> Saving model checkpoint to models/OneShot/1/checkpoint-1498\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:32,817 >> Configuration saved in models/OneShot/1/checkpoint-1498/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:32,913 >> Module weights saved in models/OneShot/1/checkpoint-1498/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:32,914 >> Configuration saved in models/OneShot/1/checkpoint-1498/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:32,922 >> Module weights saved in models/OneShot/1/checkpoint-1498/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:32,922 >> Configuration saved in models/OneShot/1/checkpoint-1498/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:32,989 >> Module weights saved in models/OneShot/1/checkpoint-1498/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:32,989 >> Configuration saved in models/OneShot/1/checkpoint-1498/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:33,064 >> Module weights saved in models/OneShot/1/checkpoint-1498/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:33,065 >> Configuration saved in models/OneShot/1/checkpoint-1498/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:34,545 >> Module weights saved in models/OneShot/1/checkpoint-1498/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:34,546 >> Configuration saved in models/OneShot/1/checkpoint-1498/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:34,556 >> Module weights saved in models/OneShot/1/checkpoint-1498/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:43:34,557 >> Configuration saved in models/OneShot/1/checkpoint-1498/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:43:36,066 >> Module weights saved in models/OneShot/1/checkpoint-1498/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:43:36,067 >> tokenizer config file saved in models/OneShot/1/checkpoint-1498/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:43:36,067 >> Special tokens file saved in models/OneShot/1/checkpoint-1498/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:43:36,546 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1391] due to args.save_total_limit\n",
            "{'loss': 0.005, 'learning_rate': 4.392523364485982e-05, 'epoch': 14.02}\n",
            " 60% 1605/2675 [17:09<09:23,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:44:37,837 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:44:37,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:44:37,838 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:44:37,838 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.26it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.41it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.03it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.97it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.80it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.71it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.70it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.74it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.56it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.46it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.41it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.42it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.45it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.56it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.52it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.48it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.41it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.43it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.38it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.41it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.50it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.58it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9211282730102539, 'eval_accuracy': 0.8884119987487793, 'eval_f1': 0.8844774980930588, 'eval_runtime': 4.6532, 'eval_samples_per_second': 100.146, 'eval_steps_per_second': 12.679, 'epoch': 15.0}\n",
            " 60% 1605/2675 [17:14<09:23,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.01it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:44:42,493 >> Saving model checkpoint to models/OneShot/1/checkpoint-1605\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:42,494 >> Configuration saved in models/OneShot/1/checkpoint-1605/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:42,584 >> Module weights saved in models/OneShot/1/checkpoint-1605/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:42,585 >> Configuration saved in models/OneShot/1/checkpoint-1605/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:42,592 >> Module weights saved in models/OneShot/1/checkpoint-1605/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:42,593 >> Configuration saved in models/OneShot/1/checkpoint-1605/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:42,646 >> Module weights saved in models/OneShot/1/checkpoint-1605/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:42,647 >> Configuration saved in models/OneShot/1/checkpoint-1605/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:42,715 >> Module weights saved in models/OneShot/1/checkpoint-1605/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:42,716 >> Configuration saved in models/OneShot/1/checkpoint-1605/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:44,235 >> Module weights saved in models/OneShot/1/checkpoint-1605/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:44,235 >> Configuration saved in models/OneShot/1/checkpoint-1605/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:44,246 >> Module weights saved in models/OneShot/1/checkpoint-1605/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:44:44,246 >> Configuration saved in models/OneShot/1/checkpoint-1605/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:44:45,800 >> Module weights saved in models/OneShot/1/checkpoint-1605/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:44:45,801 >> tokenizer config file saved in models/OneShot/1/checkpoint-1605/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:44:45,801 >> Special tokens file saved in models/OneShot/1/checkpoint-1605/special_tokens_map.json\n",
            " 64% 1712/2675 [18:19<08:26,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:45:47,370 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:45:47,372 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:45:47,372 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:45:47,372 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.25it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.41it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.12it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.11it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.91it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.88it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.78it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.77it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.70it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.69it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.59it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.62it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.62it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.65it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.58it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.55it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.56it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.46it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9423291087150574, 'eval_accuracy': 0.8776823878288269, 'eval_f1': 0.8723784539544805, 'eval_runtime': 4.6253, 'eval_samples_per_second': 100.751, 'eval_steps_per_second': 12.756, 'epoch': 16.0}\n",
            " 64% 1712/2675 [18:23<08:26,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 13.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:45:51,999 >> Saving model checkpoint to models/OneShot/1/checkpoint-1712\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:51,999 >> Configuration saved in models/OneShot/1/checkpoint-1712/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:52,088 >> Module weights saved in models/OneShot/1/checkpoint-1712/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:52,089 >> Configuration saved in models/OneShot/1/checkpoint-1712/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:52,097 >> Module weights saved in models/OneShot/1/checkpoint-1712/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:52,097 >> Configuration saved in models/OneShot/1/checkpoint-1712/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:52,151 >> Module weights saved in models/OneShot/1/checkpoint-1712/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:52,152 >> Configuration saved in models/OneShot/1/checkpoint-1712/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:52,242 >> Module weights saved in models/OneShot/1/checkpoint-1712/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:52,243 >> Configuration saved in models/OneShot/1/checkpoint-1712/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:53,767 >> Module weights saved in models/OneShot/1/checkpoint-1712/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:53,870 >> Configuration saved in models/OneShot/1/checkpoint-1712/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:53,885 >> Module weights saved in models/OneShot/1/checkpoint-1712/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:45:53,886 >> Configuration saved in models/OneShot/1/checkpoint-1712/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:45:55,222 >> Module weights saved in models/OneShot/1/checkpoint-1712/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:45:55,222 >> tokenizer config file saved in models/OneShot/1/checkpoint-1712/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:45:55,223 >> Special tokens file saved in models/OneShot/1/checkpoint-1712/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:45:55,718 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1605] due to args.save_total_limit\n",
            " 68% 1819/2675 [19:28<07:31,  1.89it/s][INFO|trainer.py:623] 2022-08-25 04:46:56,905 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:46:56,907 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:46:56,907 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:46:56,907 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.25it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.14it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.90it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.40it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.11it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.98it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.93it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.83it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.78it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.81it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.73it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.59it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.58it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.63it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.63it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.66it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.67it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.66it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.61it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.67it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.62it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.67it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.65it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9498847723007202, 'eval_accuracy': 0.8841201663017273, 'eval_f1': 0.880780018192844, 'eval_runtime': 4.6162, 'eval_samples_per_second': 100.949, 'eval_steps_per_second': 12.781, 'epoch': 17.0}\n",
            " 68% 1819/2675 [19:33<07:31,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 14.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:47:01,525 >> Saving model checkpoint to models/OneShot/1/checkpoint-1819\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:01,526 >> Configuration saved in models/OneShot/1/checkpoint-1819/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:01,614 >> Module weights saved in models/OneShot/1/checkpoint-1819/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:01,614 >> Configuration saved in models/OneShot/1/checkpoint-1819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:01,622 >> Module weights saved in models/OneShot/1/checkpoint-1819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:01,622 >> Configuration saved in models/OneShot/1/checkpoint-1819/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:01,675 >> Module weights saved in models/OneShot/1/checkpoint-1819/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:01,676 >> Configuration saved in models/OneShot/1/checkpoint-1819/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:01,743 >> Module weights saved in models/OneShot/1/checkpoint-1819/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:01,743 >> Configuration saved in models/OneShot/1/checkpoint-1819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:02,983 >> Module weights saved in models/OneShot/1/checkpoint-1819/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:03,120 >> Configuration saved in models/OneShot/1/checkpoint-1819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:03,133 >> Module weights saved in models/OneShot/1/checkpoint-1819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:47:03,134 >> Configuration saved in models/OneShot/1/checkpoint-1819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:47:04,591 >> Module weights saved in models/OneShot/1/checkpoint-1819/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:47:04,592 >> tokenizer config file saved in models/OneShot/1/checkpoint-1819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:47:04,592 >> Special tokens file saved in models/OneShot/1/checkpoint-1819/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:47:05,064 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1712] due to args.save_total_limit\n",
            " 72% 1926/2675 [20:37<06:35,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:48:06,186 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:48:06,188 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:48:06,188 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:48:06,188 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 18.98it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.90it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.70it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.32it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.91it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.67it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.68it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.62it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.70it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.53it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.55it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.59it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.64it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.59it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.55it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.49it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.46it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.53it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.58it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.66it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8337849378585815, 'eval_accuracy': 0.8884119987487793, 'eval_f1': 0.8840884739017297, 'eval_runtime': 4.6426, 'eval_samples_per_second': 100.375, 'eval_steps_per_second': 12.708, 'epoch': 18.0}\n",
            " 72% 1926/2675 [20:42<06:35,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.06it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:48:10,831 >> Saving model checkpoint to models/OneShot/1/checkpoint-1926\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:10,832 >> Configuration saved in models/OneShot/1/checkpoint-1926/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:10,922 >> Module weights saved in models/OneShot/1/checkpoint-1926/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:10,922 >> Configuration saved in models/OneShot/1/checkpoint-1926/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:10,930 >> Module weights saved in models/OneShot/1/checkpoint-1926/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:10,931 >> Configuration saved in models/OneShot/1/checkpoint-1926/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:10,988 >> Module weights saved in models/OneShot/1/checkpoint-1926/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:10,989 >> Configuration saved in models/OneShot/1/checkpoint-1926/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:11,057 >> Module weights saved in models/OneShot/1/checkpoint-1926/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:11,057 >> Configuration saved in models/OneShot/1/checkpoint-1926/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:12,574 >> Module weights saved in models/OneShot/1/checkpoint-1926/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:12,574 >> Configuration saved in models/OneShot/1/checkpoint-1926/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:12,587 >> Module weights saved in models/OneShot/1/checkpoint-1926/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:48:12,587 >> Configuration saved in models/OneShot/1/checkpoint-1926/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:48:13,985 >> Module weights saved in models/OneShot/1/checkpoint-1926/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:48:13,985 >> tokenizer config file saved in models/OneShot/1/checkpoint-1926/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:48:13,986 >> Special tokens file saved in models/OneShot/1/checkpoint-1926/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:48:14,473 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1819] due to args.save_total_limit\n",
            "{'loss': 0.002, 'learning_rate': 2.5233644859813084e-05, 'epoch': 18.69}\n",
            " 76% 2033/2675 [21:47<05:36,  1.91it/s][INFO|trainer.py:623] 2022-08-25 04:49:15,621 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:49:15,623 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:49:15,623 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:49:15,623 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.08it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.27it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.93it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.46it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.17it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.99it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.92it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.82it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.76it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.59it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.68it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.60it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.54it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.64it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.63it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.66it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.64it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.67it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.65it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.60it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9334959387779236, 'eval_accuracy': 0.8755365014076233, 'eval_f1': 0.8681921036204743, 'eval_runtime': 4.6174, 'eval_samples_per_second': 100.922, 'eval_steps_per_second': 12.778, 'epoch': 19.0}\n",
            " 76% 2033/2675 [21:51<05:36,  1.91it/s]\n",
            "100% 59/59 [00:04<00:00, 14.04it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:49:20,242 >> Saving model checkpoint to models/OneShot/1/checkpoint-2033\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:20,242 >> Configuration saved in models/OneShot/1/checkpoint-2033/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:20,332 >> Module weights saved in models/OneShot/1/checkpoint-2033/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:20,333 >> Configuration saved in models/OneShot/1/checkpoint-2033/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:20,340 >> Module weights saved in models/OneShot/1/checkpoint-2033/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:20,341 >> Configuration saved in models/OneShot/1/checkpoint-2033/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:20,402 >> Module weights saved in models/OneShot/1/checkpoint-2033/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:20,403 >> Configuration saved in models/OneShot/1/checkpoint-2033/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:20,482 >> Module weights saved in models/OneShot/1/checkpoint-2033/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:20,483 >> Configuration saved in models/OneShot/1/checkpoint-2033/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:21,956 >> Module weights saved in models/OneShot/1/checkpoint-2033/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:21,956 >> Configuration saved in models/OneShot/1/checkpoint-2033/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:21,967 >> Module weights saved in models/OneShot/1/checkpoint-2033/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:49:21,968 >> Configuration saved in models/OneShot/1/checkpoint-2033/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:49:23,473 >> Module weights saved in models/OneShot/1/checkpoint-2033/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:49:23,474 >> tokenizer config file saved in models/OneShot/1/checkpoint-2033/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:49:23,474 >> Special tokens file saved in models/OneShot/1/checkpoint-2033/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:49:23,960 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1926] due to args.save_total_limit\n",
            " 80% 2140/2675 [22:56<04:42,  1.89it/s][INFO|trainer.py:623] 2022-08-25 04:50:25,079 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:50:25,081 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:50:25,081 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:50:25,081 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.30it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.44it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.05it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.51it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.98it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.73it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.74it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.77it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.77it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.59it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.53it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.61it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.64it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.68it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.65it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.61it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.65it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.70it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.76it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.68it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.65it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.56it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.64it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.994623064994812, 'eval_accuracy': 0.8755365014076233, 'eval_f1': 0.8687451437451438, 'eval_runtime': 4.6144, 'eval_samples_per_second': 100.987, 'eval_steps_per_second': 12.786, 'epoch': 20.0}\n",
            " 80% 2140/2675 [23:01<04:42,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 14.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:50:29,697 >> Saving model checkpoint to models/OneShot/1/checkpoint-2140\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:29,698 >> Configuration saved in models/OneShot/1/checkpoint-2140/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:29,786 >> Module weights saved in models/OneShot/1/checkpoint-2140/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:29,787 >> Configuration saved in models/OneShot/1/checkpoint-2140/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:29,794 >> Module weights saved in models/OneShot/1/checkpoint-2140/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:29,795 >> Configuration saved in models/OneShot/1/checkpoint-2140/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:29,853 >> Module weights saved in models/OneShot/1/checkpoint-2140/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:29,853 >> Configuration saved in models/OneShot/1/checkpoint-2140/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:29,925 >> Module weights saved in models/OneShot/1/checkpoint-2140/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:29,926 >> Configuration saved in models/OneShot/1/checkpoint-2140/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:31,437 >> Module weights saved in models/OneShot/1/checkpoint-2140/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:31,491 >> Configuration saved in models/OneShot/1/checkpoint-2140/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:31,506 >> Module weights saved in models/OneShot/1/checkpoint-2140/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:50:31,506 >> Configuration saved in models/OneShot/1/checkpoint-2140/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:50:32,927 >> Module weights saved in models/OneShot/1/checkpoint-2140/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:50:32,928 >> tokenizer config file saved in models/OneShot/1/checkpoint-2140/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:50:32,928 >> Special tokens file saved in models/OneShot/1/checkpoint-2140/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:50:33,413 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2033] due to args.save_total_limit\n",
            " 84% 2247/2675 [24:06<03:45,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:51:34,496 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:51:34,498 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:51:34,498 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:51:34,498 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.27it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.35it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.01it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.52it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.00it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.80it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.65it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.68it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.71it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.71it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.52it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.57it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.61it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.65it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.67it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.65it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.53it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.52it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.51it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.66it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.67it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.51it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.49it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.41it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.42it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.0232131481170654, 'eval_accuracy': 0.8733905553817749, 'eval_f1': 0.8666188006655962, 'eval_runtime': 4.6438, 'eval_samples_per_second': 100.349, 'eval_steps_per_second': 12.705, 'epoch': 21.0}\n",
            " 84% 2247/2675 [24:10<03:45,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 13.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:51:39,143 >> Saving model checkpoint to models/OneShot/1/checkpoint-2247\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:39,144 >> Configuration saved in models/OneShot/1/checkpoint-2247/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:39,236 >> Module weights saved in models/OneShot/1/checkpoint-2247/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:39,236 >> Configuration saved in models/OneShot/1/checkpoint-2247/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:39,244 >> Module weights saved in models/OneShot/1/checkpoint-2247/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:39,244 >> Configuration saved in models/OneShot/1/checkpoint-2247/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:39,296 >> Module weights saved in models/OneShot/1/checkpoint-2247/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:39,297 >> Configuration saved in models/OneShot/1/checkpoint-2247/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:39,364 >> Module weights saved in models/OneShot/1/checkpoint-2247/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:39,365 >> Configuration saved in models/OneShot/1/checkpoint-2247/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:40,863 >> Module weights saved in models/OneShot/1/checkpoint-2247/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:40,864 >> Configuration saved in models/OneShot/1/checkpoint-2247/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:40,988 >> Module weights saved in models/OneShot/1/checkpoint-2247/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:51:40,989 >> Configuration saved in models/OneShot/1/checkpoint-2247/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:51:42,365 >> Module weights saved in models/OneShot/1/checkpoint-2247/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:51:42,366 >> tokenizer config file saved in models/OneShot/1/checkpoint-2247/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:51:42,366 >> Special tokens file saved in models/OneShot/1/checkpoint-2247/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:51:42,849 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2140] due to args.save_total_limit\n",
            " 88% 2354/2675 [25:15<02:49,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:52:44,067 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:52:44,069 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:52:44,069 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:52:44,069 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.15it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.12it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.87it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.45it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.98it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.81it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.77it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.75it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.73it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.66it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.62it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.57it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.52it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.51it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.56it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.61it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.62it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.54it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.51it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.41it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.40it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.43it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.45it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.53it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.59it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.54it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9794402718544006, 'eval_accuracy': 0.8755365014076233, 'eval_f1': 0.8684717653241976, 'eval_runtime': 4.6491, 'eval_samples_per_second': 100.234, 'eval_steps_per_second': 12.691, 'epoch': 22.0}\n",
            " 88% 2354/2675 [25:20<02:49,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 13.95it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:52:48,719 >> Saving model checkpoint to models/OneShot/1/checkpoint-2354\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:48,720 >> Configuration saved in models/OneShot/1/checkpoint-2354/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:48,810 >> Module weights saved in models/OneShot/1/checkpoint-2354/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:48,810 >> Configuration saved in models/OneShot/1/checkpoint-2354/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:48,817 >> Module weights saved in models/OneShot/1/checkpoint-2354/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:48,818 >> Configuration saved in models/OneShot/1/checkpoint-2354/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:48,875 >> Module weights saved in models/OneShot/1/checkpoint-2354/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:48,876 >> Configuration saved in models/OneShot/1/checkpoint-2354/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:48,946 >> Module weights saved in models/OneShot/1/checkpoint-2354/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:48,946 >> Configuration saved in models/OneShot/1/checkpoint-2354/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:50,420 >> Module weights saved in models/OneShot/1/checkpoint-2354/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:50,423 >> Configuration saved in models/OneShot/1/checkpoint-2354/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:50,437 >> Module weights saved in models/OneShot/1/checkpoint-2354/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:52:50,437 >> Configuration saved in models/OneShot/1/checkpoint-2354/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:52:51,958 >> Module weights saved in models/OneShot/1/checkpoint-2354/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:52:51,958 >> tokenizer config file saved in models/OneShot/1/checkpoint-2354/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:52:51,959 >> Special tokens file saved in models/OneShot/1/checkpoint-2354/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:52:52,457 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2247] due to args.save_total_limit\n",
            " 92% 2461/2675 [26:25<01:52,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:53:53,574 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:53:53,576 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:53:53,576 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:53:53,576 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.35it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.24it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.01it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.53it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.10it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.99it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.95it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.82it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.72it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.74it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.67it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.65it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.67it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.67it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.70it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.71it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.64it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.54it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.58it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.62it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.65it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.58it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.56it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.56it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.53it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.56it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9707071781158447, 'eval_accuracy': 0.8776823878288269, 'eval_f1': 0.8708745934751857, 'eval_runtime': 4.6202, 'eval_samples_per_second': 100.861, 'eval_steps_per_second': 12.77, 'epoch': 23.0}\n",
            " 92% 2461/2675 [26:29<01:52,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 14.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:53:58,198 >> Saving model checkpoint to models/OneShot/1/checkpoint-2461\n",
            "[INFO|loading.py:60] 2022-08-25 04:53:58,199 >> Configuration saved in models/OneShot/1/checkpoint-2461/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:53:58,291 >> Module weights saved in models/OneShot/1/checkpoint-2461/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:53:58,292 >> Configuration saved in models/OneShot/1/checkpoint-2461/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:53:58,303 >> Module weights saved in models/OneShot/1/checkpoint-2461/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:53:58,304 >> Configuration saved in models/OneShot/1/checkpoint-2461/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:53:58,358 >> Module weights saved in models/OneShot/1/checkpoint-2461/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:53:58,359 >> Configuration saved in models/OneShot/1/checkpoint-2461/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:53:58,447 >> Module weights saved in models/OneShot/1/checkpoint-2461/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:53:58,448 >> Configuration saved in models/OneShot/1/checkpoint-2461/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:53:59,896 >> Module weights saved in models/OneShot/1/checkpoint-2461/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:54:00,048 >> Configuration saved in models/OneShot/1/checkpoint-2461/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:54:00,074 >> Module weights saved in models/OneShot/1/checkpoint-2461/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:54:00,074 >> Configuration saved in models/OneShot/1/checkpoint-2461/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:54:01,468 >> Module weights saved in models/OneShot/1/checkpoint-2461/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:54:01,468 >> tokenizer config file saved in models/OneShot/1/checkpoint-2461/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:54:01,469 >> Special tokens file saved in models/OneShot/1/checkpoint-2461/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:54:01,939 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2354] due to args.save_total_limit\n",
            "{'loss': 0.0008, 'learning_rate': 6.542056074766355e-06, 'epoch': 23.36}\n",
            " 96% 2568/2675 [27:34<00:56,  1.90it/s][INFO|trainer.py:623] 2022-08-25 04:55:03,117 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:55:03,120 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:55:03,120 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:55:03,120 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.46it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.43it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.11it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.55it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 13.22it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 13.06it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.98it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.84it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.79it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.83it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.64it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.66it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.67it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.71it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.71it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.67it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.58it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.56it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.59it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.67it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.71it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.65it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.61it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.67it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.67it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.66it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.62it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9161802530288696, 'eval_accuracy': 0.8884119987487793, 'eval_f1': 0.8832485353068147, 'eval_runtime': 4.6046, 'eval_samples_per_second': 101.204, 'eval_steps_per_second': 12.813, 'epoch': 24.0}\n",
            " 96% 2568/2675 [27:39<00:56,  1.90it/s]\n",
            "100% 59/59 [00:04<00:00, 13.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:55:07,725 >> Saving model checkpoint to models/OneShot/1/checkpoint-2568\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:07,726 >> Configuration saved in models/OneShot/1/checkpoint-2568/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:07,826 >> Module weights saved in models/OneShot/1/checkpoint-2568/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:07,826 >> Configuration saved in models/OneShot/1/checkpoint-2568/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:07,834 >> Module weights saved in models/OneShot/1/checkpoint-2568/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:07,834 >> Configuration saved in models/OneShot/1/checkpoint-2568/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:07,886 >> Module weights saved in models/OneShot/1/checkpoint-2568/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:07,887 >> Configuration saved in models/OneShot/1/checkpoint-2568/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:07,954 >> Module weights saved in models/OneShot/1/checkpoint-2568/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:07,954 >> Configuration saved in models/OneShot/1/checkpoint-2568/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:09,431 >> Module weights saved in models/OneShot/1/checkpoint-2568/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:09,435 >> Configuration saved in models/OneShot/1/checkpoint-2568/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:09,451 >> Module weights saved in models/OneShot/1/checkpoint-2568/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:55:09,451 >> Configuration saved in models/OneShot/1/checkpoint-2568/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:55:10,964 >> Module weights saved in models/OneShot/1/checkpoint-2568/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:55:10,964 >> tokenizer config file saved in models/OneShot/1/checkpoint-2568/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:55:10,965 >> Special tokens file saved in models/OneShot/1/checkpoint-2568/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:55:11,437 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2461] due to args.save_total_limit\n",
            "100% 2675/2675 [28:44<00:00,  1.89it/s][INFO|trainer.py:623] 2022-08-25 04:56:12,667 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:56:12,669 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:56:12,669 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:56:12,669 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:02, 19.31it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 15.38it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 14.07it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 13.52it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.99it/s]\u001b[A\n",
            " 22% 13/59 [00:00<00:03, 12.78it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.78it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.84it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.80it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:02, 12.76it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.72it/s]\u001b[A\n",
            " 42% 25/59 [00:01<00:02, 12.68it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.66it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.69it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.72it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.69it/s]\u001b[A\n",
            " 63% 37/59 [00:02<00:01, 12.61it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.56it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.55it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.53it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.57it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.64it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.64it/s]\u001b[A\n",
            " 86% 51/59 [00:03<00:00, 12.63it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.57it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.59it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.61it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9166648387908936, 'eval_accuracy': 0.8884119987487793, 'eval_f1': 0.8832485353068147, 'eval_runtime': 4.6177, 'eval_samples_per_second': 100.916, 'eval_steps_per_second': 12.777, 'epoch': 25.0}\n",
            "100% 2675/2675 [28:48<00:00,  1.89it/s]\n",
            "100% 59/59 [00:04<00:00, 14.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 04:56:17,288 >> Saving model checkpoint to models/OneShot/1/checkpoint-2675\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:17,289 >> Configuration saved in models/OneShot/1/checkpoint-2675/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:17,379 >> Module weights saved in models/OneShot/1/checkpoint-2675/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:17,380 >> Configuration saved in models/OneShot/1/checkpoint-2675/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:17,387 >> Module weights saved in models/OneShot/1/checkpoint-2675/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:17,388 >> Configuration saved in models/OneShot/1/checkpoint-2675/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:17,439 >> Module weights saved in models/OneShot/1/checkpoint-2675/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:17,439 >> Configuration saved in models/OneShot/1/checkpoint-2675/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:17,514 >> Module weights saved in models/OneShot/1/checkpoint-2675/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:17,515 >> Configuration saved in models/OneShot/1/checkpoint-2675/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:19,016 >> Module weights saved in models/OneShot/1/checkpoint-2675/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:19,113 >> Configuration saved in models/OneShot/1/checkpoint-2675/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:19,126 >> Module weights saved in models/OneShot/1/checkpoint-2675/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:19,127 >> Configuration saved in models/OneShot/1/checkpoint-2675/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:20,552 >> Module weights saved in models/OneShot/1/checkpoint-2675/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:56:20,553 >> tokenizer config file saved in models/OneShot/1/checkpoint-2675/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:56:20,553 >> Special tokens file saved in models/OneShot/1/checkpoint-2675/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 04:56:21,040 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2568] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 04:56:21,097 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 04:56:21,097 >> Loading best model from models/OneShot/1/checkpoint-1498 (score: 0.8867913743087015).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 04:56:21,097 >> Could not locate the best model at models/OneShot/1/checkpoint-1498/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 1732.8003, 'train_samples_per_second': 49.256, 'train_steps_per_second': 1.544, 'train_loss': 0.03210307888695361, 'epoch': 25.0}\n",
            "100% 2675/2675 [28:52<00:00,  1.89it/s][INFO|trainer.py:238] 2022-08-25 04:56:21,113 >> Loading best adapter(s) from models/OneShot/1/checkpoint-1498 (score: 0.8867913743087015).\n",
            "[INFO|loading.py:77] 2022-08-25 04:56:21,113 >> Loading module configuration from models/OneShot/1/checkpoint-1498/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 04:56:21,122 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:56:21,770 >> Loading module weights from models/OneShot/1/checkpoint-1498/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:56:21,793 >> Loading module configuration from models/OneShot/1/checkpoint-1498/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 04:56:21,794 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 04:56:21,803 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 04:56:21,822 >> Loading module weights from models/OneShot/1/checkpoint-1498/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:56:21,825 >> Loading module configuration from models/OneShot/1/checkpoint-1498/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 04:56:21,826 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:56:21,969 >> Loading module weights from models/OneShot/1/checkpoint-1498/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 04:56:21,987 >> No matching prediction head found in 'models/OneShot/1/checkpoint-1498/en'\n",
            "[INFO|loading.py:77] 2022-08-25 04:56:21,987 >> Loading module configuration from models/OneShot/1/checkpoint-1498/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 04:56:21,988 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 04:56:22,127 >> Loading module weights from models/OneShot/1/checkpoint-1498/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 04:56:22,145 >> Loading module configuration from models/OneShot/1/checkpoint-1498/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 04:56:22,146 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 04:56:23,303 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 04:56:24,815 >> Loading module weights from models/OneShot/1/checkpoint-1498/pt/pytorch_model_head.bin\n",
            "100% 2675/2675 [28:56<00:00,  1.54it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 04:56:24,902 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:24,903 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:25,028 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:25,029 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:25,036 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:25,037 >> Configuration saved in models/OneShot/1/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:25,102 >> Module weights saved in models/OneShot/1/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:25,103 >> Configuration saved in models/OneShot/1/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:25,175 >> Module weights saved in models/OneShot/1/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:25,175 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:26,639 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:26,773 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:26,787 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 04:56:26,787 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 04:56:28,156 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 04:56:28,157 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 04:56:28,157 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0321\n",
            "  train_runtime            = 0:28:52.80\n",
            "  train_samples            =       3414\n",
            "  train_samples_per_second =     49.256\n",
            "  train_steps_per_second   =      1.544\n",
            "08/25/2022 04:56:28 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:623] 2022-08-25 04:56:28,385 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 04:56:28,387 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 04:56:28,387 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 04:56:28,387 >>   Batch size = 8\n",
            "100% 59/59 [00:04<00:00, 13.13it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.8906\n",
            "  eval_f1                 =     0.8868\n",
            "  eval_loss               =      0.887\n",
            "  eval_runtime            = 0:00:04.58\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =    101.736\n",
            "  eval_steps_per_second   =     12.881\n",
            "[INFO|modelcard.py:460] 2022-08-25 04:56:33,159 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8905579447746277}, {'name': 'F1', 'type': 'f1', 'value': 0.8867913743087015}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for one-shot portuguese idiomaticity detection"
      ],
      "metadata": {
        "id": "smQkFLCLEV_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Portuguese language\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/OneShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/OneShot/PT/train.csv \\\n",
        "  --validation_file Data/OneShot/PT/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS1IvfpcLsCu",
        "outputId": "62d9ed5d-7a33-4256-ac9c-84cfa434d5a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 09:37:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 09:37:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_09-37-53_9b17356d7e44,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 09:37:53 - INFO - __main__ - load a local file for train: Data/OneShot/PT/train.csv\n",
            "08/25/2022 09:37:53 - INFO - __main__ - load a local file for validation: Data/OneShot/PT/dev.csv\n",
            "08/25/2022 09:37:53 - WARNING - datasets.builder - Using custom data configuration default-679db48a42af9ccc\n",
            "08/25/2022 09:37:53 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-679db48a42af9ccc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-679db48a42af9ccc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 11066.77it/s]\n",
            "08/25/2022 09:37:53 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 09:37:53 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1053.85it/s]\n",
            "08/25/2022 09:37:53 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 09:37:53 - INFO - datasets.builder - Generating train split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 09:37:53 - INFO - datasets.builder - Generating validation split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 09:37:53 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-679db48a42af9ccc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 976.67it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 09:37:53,935 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 09:37:53,936 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 09:37:53,982 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 09:37:53,983 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:37:54,123 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:37:54,123 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:37:54,123 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:37:54,123 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 09:37:54,123 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 09:37:54,144 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 09:37:54,145 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 09:37:54,326 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 09:37:59,227 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 09:37:59,227 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 09:37:59,238 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 09:37:59,287 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 09:37:59,483 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:327] 2022-08-25 09:37:59,577 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 09:37:59,660 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 09:38:00,238 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 09:38:00,238 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:38:00,487 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 09:38:00,499 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 09:38:00,500 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 09:38:00,520 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 09:38:00,669 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 09:38:02,235 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 09:38:02,236 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:38:02,500 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:38:02,513 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 09:38:02,514 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 09:38:03,677 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 09:38:05,309 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 09:38:05,378 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with PT language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]08/25/2022 09:38:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-679db48a42af9ccc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-1a3548c7a9875f49.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 12.10ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 09:38:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-679db48a42af9ccc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0ff77b88a3555b24.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 28.37ba/s]\n",
            "08/25/2022 09:38:05 - INFO - __main__ - Sample 275 of the training set: {'label': 0, 'sentence1': 'Já o Boeing 787-9 que a Lufthansa encomendou cerca de 20 aeronaves tem programada entrega para 2022, coincidindo com a previsão da Lufthansa em lançar a nova classe executiva.', 'sentence2': 'classe executiva', 'input_ids': [101, 48625, 183, 20172, 53172, 118, 130, 10121, 169, 101529, 10110, 61643, 10605, 10138, 13698, 10104, 10197, 82895, 12900, 13815, 10229, 47937, 10220, 75632, 117, 71770, 43275, 26104, 10212, 169, 12229, 100167, 10143, 101529, 10266, 110543, 10129, 169, 15656, 15702, 11419, 97038, 17403, 119, 102, 15702, 11419, 97038, 17403, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 09:38:05 - INFO - __main__ - Sample 1165 of the training set: {'label': 0, 'sentence1': 'Assim, Bibi continuará ao lado marido trambiqueiro, sem saber que seu verdadeiro príncipe encantado é o advogado.', 'sentence2': 'príncipe encantado', 'input_ids': [101, 43479, 117, 31156, 11645, 36055, 10661, 10610, 15776, 40407, 51127, 101349, 12772, 14213, 117, 11531, 33335, 10121, 10617, 58695, 14213, 32039, 10110, 62745, 11272, 263, 183, 10840, 95926, 11272, 119, 102, 32039, 10110, 62745, 11272, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 09:38:05 - INFO - __main__ - Sample 129 of the training set: {'label': 0, 'sentence1': 'O amigo secreto parece nunca sair de moda entre os brasileiros.', 'sentence2': 'amigo secreto', 'input_ids': [101, 152, 27989, 65234, 30395, 19096, 13410, 10129, 10104, 38231, 10402, 10427, 88442, 119, 102, 27989, 65234, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 09:38:11,024 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 09:38:11,038 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 09:38:11,038 >>   Num examples = 1217\n",
            "[INFO|trainer.py:1421] 2022-08-25 09:38:11,038 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 09:38:11,038 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 09:38:11,038 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 09:38:11,038 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 09:38:11,038 >>   Total optimization steps = 975\n",
            "  4% 38/975 [00:23<09:36,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:38:34,882 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:38:34,884 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:38:34,884 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:38:34,885 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.22it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.28it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.16it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.53it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.81it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.95it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.19it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.84it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.82it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.78it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8131228685379028, 'eval_accuracy': 0.6593406796455383, 'eval_f1': 0.6273393121871743, 'eval_runtime': 2.8996, 'eval_samples_per_second': 94.15, 'eval_steps_per_second': 12.071, 'epoch': 1.0}\n",
            "  4% 39/975 [00:26<09:36,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:38:37,786 >> Saving model checkpoint to models/OneShot/1/checkpoint-39\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:37,786 >> Configuration saved in models/OneShot/1/checkpoint-39/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:37,925 >> Module weights saved in models/OneShot/1/checkpoint-39/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:37,925 >> Configuration saved in models/OneShot/1/checkpoint-39/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:37,936 >> Module weights saved in models/OneShot/1/checkpoint-39/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:37,936 >> Configuration saved in models/OneShot/1/checkpoint-39/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:38,048 >> Module weights saved in models/OneShot/1/checkpoint-39/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:38,049 >> Configuration saved in models/OneShot/1/checkpoint-39/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:38,197 >> Module weights saved in models/OneShot/1/checkpoint-39/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:38,198 >> Configuration saved in models/OneShot/1/checkpoint-39/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:39,815 >> Module weights saved in models/OneShot/1/checkpoint-39/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:39,853 >> Configuration saved in models/OneShot/1/checkpoint-39/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:39,875 >> Module weights saved in models/OneShot/1/checkpoint-39/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:38:39,876 >> Configuration saved in models/OneShot/1/checkpoint-39/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:38:41,312 >> Module weights saved in models/OneShot/1/checkpoint-39/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:38:41,313 >> tokenizer config file saved in models/OneShot/1/checkpoint-39/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:38:41,313 >> Special tokens file saved in models/OneShot/1/checkpoint-39/special_tokens_map.json\n",
            "  8% 77/975 [00:54<09:22,  1.60it/s][INFO|trainer.py:623] 2022-08-25 09:39:05,710 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:39:05,712 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:39:05,712 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:39:05,712 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.52it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.08it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.20it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.53it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.89it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.87it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.86it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.84it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.59it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.58it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.66it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.73it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.7409284710884094, 'eval_accuracy': 0.6776556968688965, 'eval_f1': 0.6753513513513514, 'eval_runtime': 2.932, 'eval_samples_per_second': 93.111, 'eval_steps_per_second': 11.937, 'epoch': 2.0}\n",
            "  8% 78/975 [00:57<09:21,  1.60it/s]\n",
            "100% 35/35 [00:02<00:00, 13.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:39:08,645 >> Saving model checkpoint to models/OneShot/1/checkpoint-78\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:08,646 >> Configuration saved in models/OneShot/1/checkpoint-78/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:08,738 >> Module weights saved in models/OneShot/1/checkpoint-78/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:08,738 >> Configuration saved in models/OneShot/1/checkpoint-78/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:08,746 >> Module weights saved in models/OneShot/1/checkpoint-78/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:08,747 >> Configuration saved in models/OneShot/1/checkpoint-78/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:08,800 >> Module weights saved in models/OneShot/1/checkpoint-78/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:08,801 >> Configuration saved in models/OneShot/1/checkpoint-78/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:08,863 >> Module weights saved in models/OneShot/1/checkpoint-78/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:08,864 >> Configuration saved in models/OneShot/1/checkpoint-78/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:10,229 >> Module weights saved in models/OneShot/1/checkpoint-78/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:10,398 >> Configuration saved in models/OneShot/1/checkpoint-78/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:10,408 >> Module weights saved in models/OneShot/1/checkpoint-78/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:10,409 >> Configuration saved in models/OneShot/1/checkpoint-78/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:11,723 >> Module weights saved in models/OneShot/1/checkpoint-78/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:39:11,724 >> tokenizer config file saved in models/OneShot/1/checkpoint-78/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:39:11,724 >> Special tokens file saved in models/OneShot/1/checkpoint-78/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:39:12,211 >> Deleting older checkpoint [models/OneShot/1/checkpoint-39] due to args.save_total_limit\n",
            " 12% 116/975 [01:24<08:41,  1.65it/s][INFO|trainer.py:623] 2022-08-25 09:39:35,594 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:39:35,595 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:39:35,596 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:39:35,596 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.28it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.44it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.52it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.00it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.51it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.34it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.30it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.29it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.27it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.28it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.14it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.00it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.03it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.08it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.11it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7854936122894287, 'eval_accuracy': 0.7545787692070007, 'eval_f1': 0.7480335569545273, 'eval_runtime': 2.8299, 'eval_samples_per_second': 96.47, 'eval_steps_per_second': 12.368, 'epoch': 3.0}\n",
            " 12% 117/975 [01:27<08:41,  1.65it/s]\n",
            "100% 35/35 [00:02<00:00, 13.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:39:38,427 >> Saving model checkpoint to models/OneShot/1/checkpoint-117\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:38,427 >> Configuration saved in models/OneShot/1/checkpoint-117/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:38,513 >> Module weights saved in models/OneShot/1/checkpoint-117/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:38,513 >> Configuration saved in models/OneShot/1/checkpoint-117/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:38,520 >> Module weights saved in models/OneShot/1/checkpoint-117/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:38,521 >> Configuration saved in models/OneShot/1/checkpoint-117/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:38,586 >> Module weights saved in models/OneShot/1/checkpoint-117/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:38,587 >> Configuration saved in models/OneShot/1/checkpoint-117/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:38,646 >> Module weights saved in models/OneShot/1/checkpoint-117/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:38,646 >> Configuration saved in models/OneShot/1/checkpoint-117/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:40,032 >> Module weights saved in models/OneShot/1/checkpoint-117/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:40,032 >> Configuration saved in models/OneShot/1/checkpoint-117/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:40,043 >> Module weights saved in models/OneShot/1/checkpoint-117/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:39:40,043 >> Configuration saved in models/OneShot/1/checkpoint-117/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:39:41,604 >> Module weights saved in models/OneShot/1/checkpoint-117/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:39:41,605 >> tokenizer config file saved in models/OneShot/1/checkpoint-117/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:39:41,605 >> Special tokens file saved in models/OneShot/1/checkpoint-117/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:39:42,082 >> Deleting older checkpoint [models/OneShot/1/checkpoint-78] due to args.save_total_limit\n",
            " 16% 155/975 [01:54<08:24,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:40:05,425 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:40:05,427 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:40:05,427 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:40:05,427 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:02, 15.95it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 13.71it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.90it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.48it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.88it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.89it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.86it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.96it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.88it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7920272946357727, 'eval_accuracy': 0.7472527623176575, 'eval_f1': 0.733507816368395, 'eval_runtime': 2.9047, 'eval_samples_per_second': 93.987, 'eval_steps_per_second': 12.05, 'epoch': 4.0}\n",
            " 16% 156/975 [01:57<08:24,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:40:08,333 >> Saving model checkpoint to models/OneShot/1/checkpoint-156\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:08,333 >> Configuration saved in models/OneShot/1/checkpoint-156/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:08,424 >> Module weights saved in models/OneShot/1/checkpoint-156/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:08,424 >> Configuration saved in models/OneShot/1/checkpoint-156/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:08,432 >> Module weights saved in models/OneShot/1/checkpoint-156/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:08,433 >> Configuration saved in models/OneShot/1/checkpoint-156/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:08,487 >> Module weights saved in models/OneShot/1/checkpoint-156/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:08,488 >> Configuration saved in models/OneShot/1/checkpoint-156/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:08,550 >> Module weights saved in models/OneShot/1/checkpoint-156/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:08,550 >> Configuration saved in models/OneShot/1/checkpoint-156/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:09,830 >> Module weights saved in models/OneShot/1/checkpoint-156/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:09,853 >> Configuration saved in models/OneShot/1/checkpoint-156/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:09,909 >> Module weights saved in models/OneShot/1/checkpoint-156/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:09,909 >> Configuration saved in models/OneShot/1/checkpoint-156/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:11,414 >> Module weights saved in models/OneShot/1/checkpoint-156/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:40:11,414 >> tokenizer config file saved in models/OneShot/1/checkpoint-156/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:40:11,414 >> Special tokens file saved in models/OneShot/1/checkpoint-156/special_tokens_map.json\n",
            " 20% 194/975 [02:24<08:02,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:40:35,443 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:40:35,445 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:40:35,445 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:40:35,445 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.60it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.07it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.25it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.90it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.23it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.80it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.67it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.83it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1501641273498535, 'eval_accuracy': 0.7838827967643738, 'eval_f1': 0.7729137588292518, 'eval_runtime': 2.8824, 'eval_samples_per_second': 94.712, 'eval_steps_per_second': 12.143, 'epoch': 5.0}\n",
            " 20% 195/975 [02:27<08:01,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.31it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:40:38,328 >> Saving model checkpoint to models/OneShot/1/checkpoint-195\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:38,329 >> Configuration saved in models/OneShot/1/checkpoint-195/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:38,421 >> Module weights saved in models/OneShot/1/checkpoint-195/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:38,422 >> Configuration saved in models/OneShot/1/checkpoint-195/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:38,430 >> Module weights saved in models/OneShot/1/checkpoint-195/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:38,430 >> Configuration saved in models/OneShot/1/checkpoint-195/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:38,483 >> Module weights saved in models/OneShot/1/checkpoint-195/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:38,484 >> Configuration saved in models/OneShot/1/checkpoint-195/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:38,543 >> Module weights saved in models/OneShot/1/checkpoint-195/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:38,544 >> Configuration saved in models/OneShot/1/checkpoint-195/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:39,905 >> Module weights saved in models/OneShot/1/checkpoint-195/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:39,916 >> Configuration saved in models/OneShot/1/checkpoint-195/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:39,930 >> Module weights saved in models/OneShot/1/checkpoint-195/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:40:39,931 >> Configuration saved in models/OneShot/1/checkpoint-195/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:40:41,522 >> Module weights saved in models/OneShot/1/checkpoint-195/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:40:41,523 >> tokenizer config file saved in models/OneShot/1/checkpoint-195/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:40:41,523 >> Special tokens file saved in models/OneShot/1/checkpoint-195/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:40:42,004 >> Deleting older checkpoint [models/OneShot/1/checkpoint-117] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:40:42,072 >> Deleting older checkpoint [models/OneShot/1/checkpoint-156] due to args.save_total_limit\n",
            " 24% 233/975 [02:54<07:35,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:41:05,535 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:41:05,536 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:41:05,536 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:41:05,536 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.21it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.32it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.22it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.54it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.01it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.82it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.01it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.13it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.14it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.14it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7861567139625549, 'eval_accuracy': 0.8095238208770752, 'eval_f1': 0.8014656522712016, 'eval_runtime': 2.8704, 'eval_samples_per_second': 95.108, 'eval_steps_per_second': 12.193, 'epoch': 6.0}\n",
            " 24% 234/975 [02:57<07:34,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.40it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:41:08,408 >> Saving model checkpoint to models/OneShot/1/checkpoint-234\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:08,409 >> Configuration saved in models/OneShot/1/checkpoint-234/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:08,499 >> Module weights saved in models/OneShot/1/checkpoint-234/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:08,500 >> Configuration saved in models/OneShot/1/checkpoint-234/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:08,508 >> Module weights saved in models/OneShot/1/checkpoint-234/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:08,508 >> Configuration saved in models/OneShot/1/checkpoint-234/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:08,562 >> Module weights saved in models/OneShot/1/checkpoint-234/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:08,563 >> Configuration saved in models/OneShot/1/checkpoint-234/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:08,619 >> Module weights saved in models/OneShot/1/checkpoint-234/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:08,620 >> Configuration saved in models/OneShot/1/checkpoint-234/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:09,978 >> Module weights saved in models/OneShot/1/checkpoint-234/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:09,978 >> Configuration saved in models/OneShot/1/checkpoint-234/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:09,991 >> Module weights saved in models/OneShot/1/checkpoint-234/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:09,991 >> Configuration saved in models/OneShot/1/checkpoint-234/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:11,467 >> Module weights saved in models/OneShot/1/checkpoint-234/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:41:11,468 >> tokenizer config file saved in models/OneShot/1/checkpoint-234/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:41:11,468 >> Special tokens file saved in models/OneShot/1/checkpoint-234/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:41:11,932 >> Deleting older checkpoint [models/OneShot/1/checkpoint-195] due to args.save_total_limit\n",
            " 28% 272/975 [03:24<07:12,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:41:35,372 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:41:35,373 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:41:35,374 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:41:35,374 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.66it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.10it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.92it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.41it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.82it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.90it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.94it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 11.97it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.78it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.99it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8640701174736023, 'eval_accuracy': 0.7655677795410156, 'eval_f1': 0.7441424554826617, 'eval_runtime': 2.9098, 'eval_samples_per_second': 93.82, 'eval_steps_per_second': 12.028, 'epoch': 7.0}\n",
            " 28% 273/975 [03:27<07:12,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.03it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:41:38,285 >> Saving model checkpoint to models/OneShot/1/checkpoint-273\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:38,285 >> Configuration saved in models/OneShot/1/checkpoint-273/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:38,376 >> Module weights saved in models/OneShot/1/checkpoint-273/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:38,377 >> Configuration saved in models/OneShot/1/checkpoint-273/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:38,384 >> Module weights saved in models/OneShot/1/checkpoint-273/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:38,385 >> Configuration saved in models/OneShot/1/checkpoint-273/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:38,440 >> Module weights saved in models/OneShot/1/checkpoint-273/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:38,441 >> Configuration saved in models/OneShot/1/checkpoint-273/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:38,502 >> Module weights saved in models/OneShot/1/checkpoint-273/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:38,502 >> Configuration saved in models/OneShot/1/checkpoint-273/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:39,878 >> Module weights saved in models/OneShot/1/checkpoint-273/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:40,024 >> Configuration saved in models/OneShot/1/checkpoint-273/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:40,046 >> Module weights saved in models/OneShot/1/checkpoint-273/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:41:40,046 >> Configuration saved in models/OneShot/1/checkpoint-273/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:41:41,495 >> Module weights saved in models/OneShot/1/checkpoint-273/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:41:41,495 >> tokenizer config file saved in models/OneShot/1/checkpoint-273/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:41:41,496 >> Special tokens file saved in models/OneShot/1/checkpoint-273/special_tokens_map.json\n",
            " 32% 311/975 [03:54<06:48,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:42:05,370 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:42:05,372 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:42:05,372 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:42:05,372 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.15it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.30it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.21it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.37it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.97it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.04it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.02it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.11it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.09it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.99it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9962749481201172, 'eval_accuracy': 0.7948718070983887, 'eval_f1': 0.7817247287264419, 'eval_runtime': 2.868, 'eval_samples_per_second': 95.187, 'eval_steps_per_second': 12.204, 'epoch': 8.0}\n",
            " 32% 312/975 [03:57<06:47,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:42:08,241 >> Saving model checkpoint to models/OneShot/1/checkpoint-312\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:08,242 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:08,336 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:08,337 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:08,344 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:08,345 >> Configuration saved in models/OneShot/1/checkpoint-312/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:08,403 >> Module weights saved in models/OneShot/1/checkpoint-312/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:08,403 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:08,475 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:08,478 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:09,571 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:09,784 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:09,801 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:09,802 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:11,313 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:42:11,314 >> tokenizer config file saved in models/OneShot/1/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:42:11,314 >> Special tokens file saved in models/OneShot/1/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:42:11,788 >> Deleting older checkpoint [models/OneShot/1/checkpoint-273] due to args.save_total_limit\n",
            " 36% 350/975 [04:24<06:23,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:42:35,259 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:42:35,260 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:42:35,260 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:42:35,261 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.74it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.11it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.97it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.35it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.81it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.92it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.96it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.95it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.77it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.03it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.10it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.19it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.17it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8476775884628296, 'eval_accuracy': 0.8351648449897766, 'eval_f1': 0.8338809784592918, 'eval_runtime': 2.878, 'eval_samples_per_second': 94.857, 'eval_steps_per_second': 12.161, 'epoch': 9.0}\n",
            " 36% 351/975 [04:27<06:23,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.55it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:42:38,141 >> Saving model checkpoint to models/OneShot/1/checkpoint-351\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:38,141 >> Configuration saved in models/OneShot/1/checkpoint-351/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:38,238 >> Module weights saved in models/OneShot/1/checkpoint-351/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:38,239 >> Configuration saved in models/OneShot/1/checkpoint-351/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:38,247 >> Module weights saved in models/OneShot/1/checkpoint-351/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:38,247 >> Configuration saved in models/OneShot/1/checkpoint-351/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:38,300 >> Module weights saved in models/OneShot/1/checkpoint-351/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:38,301 >> Configuration saved in models/OneShot/1/checkpoint-351/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:38,362 >> Module weights saved in models/OneShot/1/checkpoint-351/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:38,363 >> Configuration saved in models/OneShot/1/checkpoint-351/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:39,750 >> Module weights saved in models/OneShot/1/checkpoint-351/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:39,850 >> Configuration saved in models/OneShot/1/checkpoint-351/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:39,864 >> Module weights saved in models/OneShot/1/checkpoint-351/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:42:39,864 >> Configuration saved in models/OneShot/1/checkpoint-351/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:42:41,384 >> Module weights saved in models/OneShot/1/checkpoint-351/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:42:41,385 >> tokenizer config file saved in models/OneShot/1/checkpoint-351/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:42:41,385 >> Special tokens file saved in models/OneShot/1/checkpoint-351/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:42:41,862 >> Deleting older checkpoint [models/OneShot/1/checkpoint-234] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:42:41,934 >> Deleting older checkpoint [models/OneShot/1/checkpoint-312] due to args.save_total_limit\n",
            " 40% 389/975 [04:54<06:02,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:43:05,447 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:43:05,449 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:43:05,449 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:43:05,449 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.67it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.28it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.14it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.38it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.90it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.99it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.81it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8218474388122559, 'eval_accuracy': 0.8498168587684631, 'eval_f1': 0.8465219597142425, 'eval_runtime': 2.8891, 'eval_samples_per_second': 94.492, 'eval_steps_per_second': 12.114, 'epoch': 10.0}\n",
            " 40% 390/975 [04:57<06:01,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:43:08,339 >> Saving model checkpoint to models/OneShot/1/checkpoint-390\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:08,340 >> Configuration saved in models/OneShot/1/checkpoint-390/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:08,432 >> Module weights saved in models/OneShot/1/checkpoint-390/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:08,432 >> Configuration saved in models/OneShot/1/checkpoint-390/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:08,440 >> Module weights saved in models/OneShot/1/checkpoint-390/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:08,441 >> Configuration saved in models/OneShot/1/checkpoint-390/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:08,495 >> Module weights saved in models/OneShot/1/checkpoint-390/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:08,495 >> Configuration saved in models/OneShot/1/checkpoint-390/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:08,563 >> Module weights saved in models/OneShot/1/checkpoint-390/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:08,563 >> Configuration saved in models/OneShot/1/checkpoint-390/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:09,968 >> Module weights saved in models/OneShot/1/checkpoint-390/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:09,969 >> Configuration saved in models/OneShot/1/checkpoint-390/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:09,978 >> Module weights saved in models/OneShot/1/checkpoint-390/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:09,979 >> Configuration saved in models/OneShot/1/checkpoint-390/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:11,487 >> Module weights saved in models/OneShot/1/checkpoint-390/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:43:11,488 >> tokenizer config file saved in models/OneShot/1/checkpoint-390/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:43:11,488 >> Special tokens file saved in models/OneShot/1/checkpoint-390/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:43:11,963 >> Deleting older checkpoint [models/OneShot/1/checkpoint-351] due to args.save_total_limit\n",
            " 44% 428/975 [05:24<05:36,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:43:35,420 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:43:35,422 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:43:35,422 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:43:35,422 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.14it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.18it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.16it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.61it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.03it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.86it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.03it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.07it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.01it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0543937683105469, 'eval_accuracy': 0.8388278484344482, 'eval_f1': 0.8343263819927176, 'eval_runtime': 2.8813, 'eval_samples_per_second': 94.75, 'eval_steps_per_second': 12.147, 'epoch': 11.0}\n",
            " 44% 429/975 [05:27<05:35,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:43:38,304 >> Saving model checkpoint to models/OneShot/1/checkpoint-429\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:38,305 >> Configuration saved in models/OneShot/1/checkpoint-429/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:38,395 >> Module weights saved in models/OneShot/1/checkpoint-429/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:38,395 >> Configuration saved in models/OneShot/1/checkpoint-429/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:38,403 >> Module weights saved in models/OneShot/1/checkpoint-429/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:38,404 >> Configuration saved in models/OneShot/1/checkpoint-429/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:38,456 >> Module weights saved in models/OneShot/1/checkpoint-429/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:38,457 >> Configuration saved in models/OneShot/1/checkpoint-429/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:38,522 >> Module weights saved in models/OneShot/1/checkpoint-429/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:38,522 >> Configuration saved in models/OneShot/1/checkpoint-429/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:39,945 >> Module weights saved in models/OneShot/1/checkpoint-429/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:39,958 >> Configuration saved in models/OneShot/1/checkpoint-429/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:40,035 >> Module weights saved in models/OneShot/1/checkpoint-429/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:43:40,036 >> Configuration saved in models/OneShot/1/checkpoint-429/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:43:41,477 >> Module weights saved in models/OneShot/1/checkpoint-429/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:43:41,478 >> tokenizer config file saved in models/OneShot/1/checkpoint-429/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:43:41,478 >> Special tokens file saved in models/OneShot/1/checkpoint-429/special_tokens_map.json\n",
            " 48% 467/975 [05:54<05:11,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:44:05,319 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:44:05,321 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:44:05,321 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:44:05,321 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.00it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.09it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.17it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.53it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.97it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.99it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.98it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.01it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.80it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.97it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.984338104724884, 'eval_accuracy': 0.8461538553237915, 'eval_f1': 0.8410326086956521, 'eval_runtime': 2.8956, 'eval_samples_per_second': 94.282, 'eval_steps_per_second': 12.087, 'epoch': 12.0}\n",
            " 48% 468/975 [05:57<05:10,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.23it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:44:08,218 >> Saving model checkpoint to models/OneShot/1/checkpoint-468\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:08,218 >> Configuration saved in models/OneShot/1/checkpoint-468/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:08,311 >> Module weights saved in models/OneShot/1/checkpoint-468/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:08,312 >> Configuration saved in models/OneShot/1/checkpoint-468/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:08,319 >> Module weights saved in models/OneShot/1/checkpoint-468/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:08,320 >> Configuration saved in models/OneShot/1/checkpoint-468/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:08,373 >> Module weights saved in models/OneShot/1/checkpoint-468/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:08,374 >> Configuration saved in models/OneShot/1/checkpoint-468/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:08,454 >> Module weights saved in models/OneShot/1/checkpoint-468/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:08,455 >> Configuration saved in models/OneShot/1/checkpoint-468/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:10,006 >> Module weights saved in models/OneShot/1/checkpoint-468/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:10,069 >> Configuration saved in models/OneShot/1/checkpoint-468/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:10,089 >> Module weights saved in models/OneShot/1/checkpoint-468/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:10,090 >> Configuration saved in models/OneShot/1/checkpoint-468/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:11,569 >> Module weights saved in models/OneShot/1/checkpoint-468/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:44:11,570 >> tokenizer config file saved in models/OneShot/1/checkpoint-468/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:44:11,570 >> Special tokens file saved in models/OneShot/1/checkpoint-468/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:44:12,039 >> Deleting older checkpoint [models/OneShot/1/checkpoint-429] due to args.save_total_limit\n",
            "{'loss': 0.112, 'learning_rate': 4.871794871794872e-05, 'epoch': 12.82}\n",
            " 52% 506/975 [06:24<04:49,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:44:35,669 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:44:35,681 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:44:35,681 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:44:35,681 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.45it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.48it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.42it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.74it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.91it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.94it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.15it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.91it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.97it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.05it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.12it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.11it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0412876605987549, 'eval_accuracy': 0.8461538553237915, 'eval_f1': 0.8418570009930487, 'eval_runtime': 2.8578, 'eval_samples_per_second': 95.527, 'eval_steps_per_second': 12.247, 'epoch': 13.0}\n",
            " 52% 507/975 [06:27<04:48,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.54it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:44:38,540 >> Saving model checkpoint to models/OneShot/1/checkpoint-507\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:38,541 >> Configuration saved in models/OneShot/1/checkpoint-507/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:38,632 >> Module weights saved in models/OneShot/1/checkpoint-507/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:38,632 >> Configuration saved in models/OneShot/1/checkpoint-507/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:38,640 >> Module weights saved in models/OneShot/1/checkpoint-507/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:38,640 >> Configuration saved in models/OneShot/1/checkpoint-507/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:38,692 >> Module weights saved in models/OneShot/1/checkpoint-507/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:38,692 >> Configuration saved in models/OneShot/1/checkpoint-507/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:38,764 >> Module weights saved in models/OneShot/1/checkpoint-507/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:38,764 >> Configuration saved in models/OneShot/1/checkpoint-507/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:40,267 >> Module weights saved in models/OneShot/1/checkpoint-507/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:40,382 >> Configuration saved in models/OneShot/1/checkpoint-507/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:40,409 >> Module weights saved in models/OneShot/1/checkpoint-507/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:44:40,410 >> Configuration saved in models/OneShot/1/checkpoint-507/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:44:41,849 >> Module weights saved in models/OneShot/1/checkpoint-507/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:44:41,850 >> tokenizer config file saved in models/OneShot/1/checkpoint-507/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:44:41,850 >> Special tokens file saved in models/OneShot/1/checkpoint-507/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:44:42,317 >> Deleting older checkpoint [models/OneShot/1/checkpoint-468] due to args.save_total_limit\n",
            " 56% 545/975 [06:54<04:24,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:45:05,773 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:45:05,775 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:45:05,775 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:45:05,775 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.26it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.24it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.15it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.56it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.94it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 11.99it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.89it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.80it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.97it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8866536021232605, 'eval_accuracy': 0.860805869102478, 'eval_f1': 0.8596287075124485, 'eval_runtime': 2.8931, 'eval_samples_per_second': 94.363, 'eval_steps_per_second': 12.098, 'epoch': 14.0}\n",
            " 56% 546/975 [06:57<04:23,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.17it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:45:08,670 >> Saving model checkpoint to models/OneShot/1/checkpoint-546\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:08,671 >> Configuration saved in models/OneShot/1/checkpoint-546/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:08,791 >> Module weights saved in models/OneShot/1/checkpoint-546/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:08,792 >> Configuration saved in models/OneShot/1/checkpoint-546/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:08,803 >> Module weights saved in models/OneShot/1/checkpoint-546/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:08,804 >> Configuration saved in models/OneShot/1/checkpoint-546/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:08,878 >> Module weights saved in models/OneShot/1/checkpoint-546/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:08,879 >> Configuration saved in models/OneShot/1/checkpoint-546/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:08,968 >> Module weights saved in models/OneShot/1/checkpoint-546/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:08,968 >> Configuration saved in models/OneShot/1/checkpoint-546/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:10,482 >> Module weights saved in models/OneShot/1/checkpoint-546/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:10,559 >> Configuration saved in models/OneShot/1/checkpoint-546/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:10,578 >> Module weights saved in models/OneShot/1/checkpoint-546/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:10,579 >> Configuration saved in models/OneShot/1/checkpoint-546/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:11,953 >> Module weights saved in models/OneShot/1/checkpoint-546/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:45:11,954 >> tokenizer config file saved in models/OneShot/1/checkpoint-546/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:45:11,954 >> Special tokens file saved in models/OneShot/1/checkpoint-546/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:45:12,431 >> Deleting older checkpoint [models/OneShot/1/checkpoint-390] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:45:12,466 >> Deleting older checkpoint [models/OneShot/1/checkpoint-507] due to args.save_total_limit\n",
            " 60% 584/975 [07:24<04:00,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:45:35,927 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:45:35,928 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:45:35,928 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:45:35,928 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.82it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.17it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.83it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.34it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.24it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.16it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.85it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.87it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.85it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.02it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0103482007980347, 'eval_accuracy': 0.8498168587684631, 'eval_f1': 0.8461760461760462, 'eval_runtime': 2.8703, 'eval_samples_per_second': 95.112, 'eval_steps_per_second': 12.194, 'epoch': 15.0}\n",
            " 60% 585/975 [07:27<03:59,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:45:38,800 >> Saving model checkpoint to models/OneShot/1/checkpoint-585\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:38,801 >> Configuration saved in models/OneShot/1/checkpoint-585/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:38,896 >> Module weights saved in models/OneShot/1/checkpoint-585/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:38,896 >> Configuration saved in models/OneShot/1/checkpoint-585/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:38,904 >> Module weights saved in models/OneShot/1/checkpoint-585/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:38,904 >> Configuration saved in models/OneShot/1/checkpoint-585/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:38,958 >> Module weights saved in models/OneShot/1/checkpoint-585/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:38,958 >> Configuration saved in models/OneShot/1/checkpoint-585/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:39,026 >> Module weights saved in models/OneShot/1/checkpoint-585/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:39,026 >> Configuration saved in models/OneShot/1/checkpoint-585/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:40,509 >> Module weights saved in models/OneShot/1/checkpoint-585/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:40,511 >> Configuration saved in models/OneShot/1/checkpoint-585/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:40,534 >> Module weights saved in models/OneShot/1/checkpoint-585/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:45:40,535 >> Configuration saved in models/OneShot/1/checkpoint-585/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:45:42,070 >> Module weights saved in models/OneShot/1/checkpoint-585/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:45:42,070 >> tokenizer config file saved in models/OneShot/1/checkpoint-585/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:45:42,071 >> Special tokens file saved in models/OneShot/1/checkpoint-585/special_tokens_map.json\n",
            " 64% 623/975 [07:54<03:36,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:46:05,890 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:46:05,892 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:46:05,892 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:46:05,892 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.78it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.08it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.22it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.60it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.98it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.86it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.00it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.04it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.93it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0481690168380737, 'eval_accuracy': 0.8424908518791199, 'eval_f1': 0.8397034123960511, 'eval_runtime': 2.8822, 'eval_samples_per_second': 94.72, 'eval_steps_per_second': 12.144, 'epoch': 16.0}\n",
            " 64% 624/975 [07:57<03:36,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:46:08,776 >> Saving model checkpoint to models/OneShot/1/checkpoint-624\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:08,777 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:08,875 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:08,875 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:08,884 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:08,885 >> Configuration saved in models/OneShot/1/checkpoint-624/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:08,943 >> Module weights saved in models/OneShot/1/checkpoint-624/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:08,944 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:09,040 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:09,041 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:10,627 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:10,685 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:10,699 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:10,706 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:12,153 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:46:12,154 >> tokenizer config file saved in models/OneShot/1/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:46:12,154 >> Special tokens file saved in models/OneShot/1/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:46:12,620 >> Deleting older checkpoint [models/OneShot/1/checkpoint-585] due to args.save_total_limit\n",
            " 68% 662/975 [08:24<03:12,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:46:36,069 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:46:36,071 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:46:36,071 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:46:36,071 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.12it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.32it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.17it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.46it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.99it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.04it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.98it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.76it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.162497639656067, 'eval_accuracy': 0.8571428656578064, 'eval_f1': 0.8521845367837954, 'eval_runtime': 2.8967, 'eval_samples_per_second': 94.244, 'eval_steps_per_second': 12.083, 'epoch': 17.0}\n",
            " 68% 663/975 [08:27<03:12,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:46:38,969 >> Saving model checkpoint to models/OneShot/1/checkpoint-663\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:38,970 >> Configuration saved in models/OneShot/1/checkpoint-663/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:39,062 >> Module weights saved in models/OneShot/1/checkpoint-663/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:39,062 >> Configuration saved in models/OneShot/1/checkpoint-663/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:39,070 >> Module weights saved in models/OneShot/1/checkpoint-663/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:39,071 >> Configuration saved in models/OneShot/1/checkpoint-663/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:39,123 >> Module weights saved in models/OneShot/1/checkpoint-663/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:39,124 >> Configuration saved in models/OneShot/1/checkpoint-663/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:39,193 >> Module weights saved in models/OneShot/1/checkpoint-663/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:39,194 >> Configuration saved in models/OneShot/1/checkpoint-663/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:40,709 >> Module weights saved in models/OneShot/1/checkpoint-663/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:40,766 >> Configuration saved in models/OneShot/1/checkpoint-663/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:40,780 >> Module weights saved in models/OneShot/1/checkpoint-663/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:46:40,781 >> Configuration saved in models/OneShot/1/checkpoint-663/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:46:42,158 >> Module weights saved in models/OneShot/1/checkpoint-663/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:46:42,159 >> tokenizer config file saved in models/OneShot/1/checkpoint-663/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:46:42,159 >> Special tokens file saved in models/OneShot/1/checkpoint-663/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:46:42,641 >> Deleting older checkpoint [models/OneShot/1/checkpoint-624] due to args.save_total_limit\n",
            " 72% 701/975 [08:54<02:48,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:47:06,101 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:47:06,103 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:47:06,103 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:47:06,103 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.03it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.18it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.64it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.00it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.10it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.85it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.04it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.10it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.08it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.96it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1697760820388794, 'eval_accuracy': 0.8534798622131348, 'eval_f1': 0.8486024844720497, 'eval_runtime': 2.8857, 'eval_samples_per_second': 94.604, 'eval_steps_per_second': 12.129, 'epoch': 18.0}\n",
            " 72% 702/975 [08:57<02:47,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:47:08,990 >> Saving model checkpoint to models/OneShot/1/checkpoint-702\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:08,991 >> Configuration saved in models/OneShot/1/checkpoint-702/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:09,080 >> Module weights saved in models/OneShot/1/checkpoint-702/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:09,081 >> Configuration saved in models/OneShot/1/checkpoint-702/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:09,088 >> Module weights saved in models/OneShot/1/checkpoint-702/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:09,089 >> Configuration saved in models/OneShot/1/checkpoint-702/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:09,145 >> Module weights saved in models/OneShot/1/checkpoint-702/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:09,146 >> Configuration saved in models/OneShot/1/checkpoint-702/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:09,214 >> Module weights saved in models/OneShot/1/checkpoint-702/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:09,215 >> Configuration saved in models/OneShot/1/checkpoint-702/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:10,704 >> Module weights saved in models/OneShot/1/checkpoint-702/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:10,716 >> Configuration saved in models/OneShot/1/checkpoint-702/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:10,729 >> Module weights saved in models/OneShot/1/checkpoint-702/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:10,730 >> Configuration saved in models/OneShot/1/checkpoint-702/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:12,250 >> Module weights saved in models/OneShot/1/checkpoint-702/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:47:12,250 >> tokenizer config file saved in models/OneShot/1/checkpoint-702/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:47:12,251 >> Special tokens file saved in models/OneShot/1/checkpoint-702/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:47:12,717 >> Deleting older checkpoint [models/OneShot/1/checkpoint-663] due to args.save_total_limit\n",
            " 76% 740/975 [09:25<02:24,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:47:36,194 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:47:36,195 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:47:36,196 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:47:36,196 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.94it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.31it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.63it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.89it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.97it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.93it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.77it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.85it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1657661199569702, 'eval_accuracy': 0.8498168587684631, 'eval_f1': 0.8450259605399792, 'eval_runtime': 2.8984, 'eval_samples_per_second': 94.19, 'eval_steps_per_second': 12.076, 'epoch': 19.0}\n",
            " 76% 741/975 [09:28<02:23,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.16it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:47:39,095 >> Saving model checkpoint to models/OneShot/1/checkpoint-741\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:39,096 >> Configuration saved in models/OneShot/1/checkpoint-741/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:39,184 >> Module weights saved in models/OneShot/1/checkpoint-741/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:39,185 >> Configuration saved in models/OneShot/1/checkpoint-741/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:39,193 >> Module weights saved in models/OneShot/1/checkpoint-741/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:39,193 >> Configuration saved in models/OneShot/1/checkpoint-741/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:39,244 >> Module weights saved in models/OneShot/1/checkpoint-741/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:39,245 >> Configuration saved in models/OneShot/1/checkpoint-741/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:39,314 >> Module weights saved in models/OneShot/1/checkpoint-741/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:39,314 >> Configuration saved in models/OneShot/1/checkpoint-741/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:40,837 >> Module weights saved in models/OneShot/1/checkpoint-741/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:40,926 >> Configuration saved in models/OneShot/1/checkpoint-741/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:40,937 >> Module weights saved in models/OneShot/1/checkpoint-741/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:47:40,938 >> Configuration saved in models/OneShot/1/checkpoint-741/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:47:42,361 >> Module weights saved in models/OneShot/1/checkpoint-741/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:47:42,362 >> tokenizer config file saved in models/OneShot/1/checkpoint-741/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:47:42,362 >> Special tokens file saved in models/OneShot/1/checkpoint-741/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:47:42,830 >> Deleting older checkpoint [models/OneShot/1/checkpoint-702] due to args.save_total_limit\n",
            " 80% 779/975 [09:55<02:00,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:48:06,341 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:48:06,343 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:48:06,343 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:48:06,343 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.71it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.04it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.14it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.02it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.05it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 11.96it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.84it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.73it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.81it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.85it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.86it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.73it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.15681791305542, 'eval_accuracy': 0.8498168587684631, 'eval_f1': 0.8458115796288899, 'eval_runtime': 2.9052, 'eval_samples_per_second': 93.968, 'eval_steps_per_second': 12.047, 'epoch': 20.0}\n",
            " 80% 780/975 [09:58<01:59,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:48:09,250 >> Saving model checkpoint to models/OneShot/1/checkpoint-780\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:09,250 >> Configuration saved in models/OneShot/1/checkpoint-780/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:09,340 >> Module weights saved in models/OneShot/1/checkpoint-780/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:09,341 >> Configuration saved in models/OneShot/1/checkpoint-780/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:09,348 >> Module weights saved in models/OneShot/1/checkpoint-780/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:09,349 >> Configuration saved in models/OneShot/1/checkpoint-780/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:09,402 >> Module weights saved in models/OneShot/1/checkpoint-780/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:09,403 >> Configuration saved in models/OneShot/1/checkpoint-780/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:09,472 >> Module weights saved in models/OneShot/1/checkpoint-780/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:09,472 >> Configuration saved in models/OneShot/1/checkpoint-780/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:10,991 >> Module weights saved in models/OneShot/1/checkpoint-780/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:11,089 >> Configuration saved in models/OneShot/1/checkpoint-780/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:11,107 >> Module weights saved in models/OneShot/1/checkpoint-780/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:11,107 >> Configuration saved in models/OneShot/1/checkpoint-780/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:12,492 >> Module weights saved in models/OneShot/1/checkpoint-780/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:48:12,493 >> tokenizer config file saved in models/OneShot/1/checkpoint-780/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:48:12,493 >> Special tokens file saved in models/OneShot/1/checkpoint-780/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:48:12,976 >> Deleting older checkpoint [models/OneShot/1/checkpoint-741] due to args.save_total_limit\n",
            " 84% 818/975 [10:25<01:36,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:48:36,483 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:48:36,484 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:48:36,485 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:48:36,485 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.97it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.36it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.34it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.57it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.94it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 11.99it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.04it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.13it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.15it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.91it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.02it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.07it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.95it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1303001642227173, 'eval_accuracy': 0.8424908518791199, 'eval_f1': 0.8382901932693235, 'eval_runtime': 2.874, 'eval_samples_per_second': 94.99, 'eval_steps_per_second': 12.178, 'epoch': 21.0}\n",
            " 84% 819/975 [10:28<01:36,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.33it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:48:39,360 >> Saving model checkpoint to models/OneShot/1/checkpoint-819\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:39,360 >> Configuration saved in models/OneShot/1/checkpoint-819/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:39,456 >> Module weights saved in models/OneShot/1/checkpoint-819/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:39,457 >> Configuration saved in models/OneShot/1/checkpoint-819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:39,465 >> Module weights saved in models/OneShot/1/checkpoint-819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:39,465 >> Configuration saved in models/OneShot/1/checkpoint-819/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:39,523 >> Module weights saved in models/OneShot/1/checkpoint-819/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:39,524 >> Configuration saved in models/OneShot/1/checkpoint-819/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:39,593 >> Module weights saved in models/OneShot/1/checkpoint-819/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:39,593 >> Configuration saved in models/OneShot/1/checkpoint-819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:41,065 >> Module weights saved in models/OneShot/1/checkpoint-819/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:41,193 >> Configuration saved in models/OneShot/1/checkpoint-819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:41,215 >> Module weights saved in models/OneShot/1/checkpoint-819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:48:41,215 >> Configuration saved in models/OneShot/1/checkpoint-819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:48:42,668 >> Module weights saved in models/OneShot/1/checkpoint-819/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:48:42,669 >> tokenizer config file saved in models/OneShot/1/checkpoint-819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:48:42,669 >> Special tokens file saved in models/OneShot/1/checkpoint-819/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:48:43,133 >> Deleting older checkpoint [models/OneShot/1/checkpoint-780] due to args.save_total_limit\n",
            " 88% 857/975 [10:55<01:12,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:49:06,597 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:49:06,598 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:49:06,599 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:49:06,599 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.99it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.18it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.62it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.00it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.02it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.82it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.01it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.07it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.16it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.15it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.151671290397644, 'eval_accuracy': 0.8571428656578064, 'eval_f1': 0.8525856697819315, 'eval_runtime': 2.8667, 'eval_samples_per_second': 95.231, 'eval_steps_per_second': 12.209, 'epoch': 22.0}\n",
            " 88% 858/975 [10:58<01:12,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.48it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:49:09,467 >> Saving model checkpoint to models/OneShot/1/checkpoint-858\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:09,467 >> Configuration saved in models/OneShot/1/checkpoint-858/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:09,556 >> Module weights saved in models/OneShot/1/checkpoint-858/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:09,557 >> Configuration saved in models/OneShot/1/checkpoint-858/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:09,564 >> Module weights saved in models/OneShot/1/checkpoint-858/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:09,565 >> Configuration saved in models/OneShot/1/checkpoint-858/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:09,616 >> Module weights saved in models/OneShot/1/checkpoint-858/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:09,617 >> Configuration saved in models/OneShot/1/checkpoint-858/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:09,690 >> Module weights saved in models/OneShot/1/checkpoint-858/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:09,691 >> Configuration saved in models/OneShot/1/checkpoint-858/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:11,193 >> Module weights saved in models/OneShot/1/checkpoint-858/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:11,271 >> Configuration saved in models/OneShot/1/checkpoint-858/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:11,311 >> Module weights saved in models/OneShot/1/checkpoint-858/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:11,314 >> Configuration saved in models/OneShot/1/checkpoint-858/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:12,586 >> Module weights saved in models/OneShot/1/checkpoint-858/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:49:12,586 >> tokenizer config file saved in models/OneShot/1/checkpoint-858/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:49:12,586 >> Special tokens file saved in models/OneShot/1/checkpoint-858/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:49:13,066 >> Deleting older checkpoint [models/OneShot/1/checkpoint-819] due to args.save_total_limit\n",
            " 92% 896/975 [11:25<00:48,  1.63it/s][INFO|trainer.py:623] 2022-08-25 09:49:36,476 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:49:36,477 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:49:36,477 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:49:36,477 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.72it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.08it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.09it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.55it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.11it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.08it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.16it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.24it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.22it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.03it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.96it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.01it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.92it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1875413656234741, 'eval_accuracy': 0.8534798622131348, 'eval_f1': 0.8481815148481815, 'eval_runtime': 2.8731, 'eval_samples_per_second': 95.021, 'eval_steps_per_second': 12.182, 'epoch': 23.0}\n",
            " 92% 897/975 [11:28<00:47,  1.63it/s]\n",
            "100% 35/35 [00:02<00:00, 13.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:49:39,352 >> Saving model checkpoint to models/OneShot/1/checkpoint-897\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:39,352 >> Configuration saved in models/OneShot/1/checkpoint-897/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:39,444 >> Module weights saved in models/OneShot/1/checkpoint-897/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:39,444 >> Configuration saved in models/OneShot/1/checkpoint-897/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:39,452 >> Module weights saved in models/OneShot/1/checkpoint-897/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:39,452 >> Configuration saved in models/OneShot/1/checkpoint-897/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:39,505 >> Module weights saved in models/OneShot/1/checkpoint-897/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:39,506 >> Configuration saved in models/OneShot/1/checkpoint-897/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:39,573 >> Module weights saved in models/OneShot/1/checkpoint-897/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:39,573 >> Configuration saved in models/OneShot/1/checkpoint-897/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:41,067 >> Module weights saved in models/OneShot/1/checkpoint-897/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:41,097 >> Configuration saved in models/OneShot/1/checkpoint-897/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:41,112 >> Module weights saved in models/OneShot/1/checkpoint-897/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:49:41,113 >> Configuration saved in models/OneShot/1/checkpoint-897/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:49:42,649 >> Module weights saved in models/OneShot/1/checkpoint-897/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:49:42,649 >> tokenizer config file saved in models/OneShot/1/checkpoint-897/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:49:42,649 >> Special tokens file saved in models/OneShot/1/checkpoint-897/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:49:43,138 >> Deleting older checkpoint [models/OneShot/1/checkpoint-858] due to args.save_total_limit\n",
            " 96% 935/975 [11:55<00:24,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:50:06,613 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:50:06,615 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:50:06,615 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:50:06,615 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.74it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.04it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 12.96it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.38it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:02, 11.96it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.03it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.06it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.09it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.00it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 11.86it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.72it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.78it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.91it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.79it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1596567630767822, 'eval_accuracy': 0.8571428656578064, 'eval_f1': 0.8521845367837954, 'eval_runtime': 2.9057, 'eval_samples_per_second': 93.955, 'eval_steps_per_second': 12.045, 'epoch': 24.0}\n",
            " 96% 936/975 [11:58<00:24,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:50:09,522 >> Saving model checkpoint to models/OneShot/1/checkpoint-936\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:09,523 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:09,612 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:09,613 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:09,620 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:09,621 >> Configuration saved in models/OneShot/1/checkpoint-936/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:09,673 >> Module weights saved in models/OneShot/1/checkpoint-936/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:09,674 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:09,744 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:09,744 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:11,261 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:11,292 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:11,309 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:11,310 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:12,733 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:50:12,733 >> tokenizer config file saved in models/OneShot/1/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:50:12,733 >> Special tokens file saved in models/OneShot/1/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:50:13,220 >> Deleting older checkpoint [models/OneShot/1/checkpoint-897] due to args.save_total_limit\n",
            "100% 974/975 [12:25<00:00,  1.62it/s][INFO|trainer.py:623] 2022-08-25 09:50:36,707 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:50:36,709 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:50:36,709 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:50:36,709 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 16.74it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.18it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.31it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.60it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.04it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.15it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.16it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.15it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.07it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:01, 11.81it/s]\u001b[A\n",
            " 71% 25/35 [00:02<00:00, 11.84it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 11.89it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 11.88it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 11.90it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 11.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1524090766906738, 'eval_accuracy': 0.8571428656578064, 'eval_f1': 0.8521845367837954, 'eval_runtime': 2.8847, 'eval_samples_per_second': 94.637, 'eval_steps_per_second': 12.133, 'epoch': 25.0}\n",
            "100% 975/975 [12:28<00:00,  1.62it/s]\n",
            "100% 35/35 [00:02<00:00, 13.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 09:50:39,595 >> Saving model checkpoint to models/OneShot/1/checkpoint-975\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:39,596 >> Configuration saved in models/OneShot/1/checkpoint-975/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:39,687 >> Module weights saved in models/OneShot/1/checkpoint-975/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:39,687 >> Configuration saved in models/OneShot/1/checkpoint-975/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:39,695 >> Module weights saved in models/OneShot/1/checkpoint-975/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:39,695 >> Configuration saved in models/OneShot/1/checkpoint-975/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:39,750 >> Module weights saved in models/OneShot/1/checkpoint-975/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:39,751 >> Configuration saved in models/OneShot/1/checkpoint-975/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:39,822 >> Module weights saved in models/OneShot/1/checkpoint-975/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:39,823 >> Configuration saved in models/OneShot/1/checkpoint-975/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:41,314 >> Module weights saved in models/OneShot/1/checkpoint-975/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:41,390 >> Configuration saved in models/OneShot/1/checkpoint-975/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:41,419 >> Module weights saved in models/OneShot/1/checkpoint-975/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:41,419 >> Configuration saved in models/OneShot/1/checkpoint-975/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:42,791 >> Module weights saved in models/OneShot/1/checkpoint-975/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:50:42,792 >> tokenizer config file saved in models/OneShot/1/checkpoint-975/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:50:42,792 >> Special tokens file saved in models/OneShot/1/checkpoint-975/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 09:50:43,295 >> Deleting older checkpoint [models/OneShot/1/checkpoint-936] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 09:50:43,362 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 09:50:43,362 >> Loading best model from models/OneShot/1/checkpoint-546 (score: 0.8596287075124485).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 09:50:43,362 >> Could not locate the best model at models/OneShot/1/checkpoint-546/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 752.324, 'train_samples_per_second': 40.441, 'train_steps_per_second': 1.296, 'train_loss': 0.05798073493517362, 'epoch': 25.0}\n",
            "100% 975/975 [12:32<00:00,  1.62it/s][INFO|trainer.py:238] 2022-08-25 09:50:43,400 >> Loading best adapter(s) from models/OneShot/1/checkpoint-546 (score: 0.8596287075124485).\n",
            "[INFO|loading.py:77] 2022-08-25 09:50:43,400 >> Loading module configuration from models/OneShot/1/checkpoint-546/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 09:50:43,406 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:50:44,056 >> Loading module weights from models/OneShot/1/checkpoint-546/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:50:44,080 >> Loading module configuration from models/OneShot/1/checkpoint-546/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 09:50:44,085 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 09:50:44,094 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 09:50:44,120 >> Loading module weights from models/OneShot/1/checkpoint-546/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:50:44,124 >> Loading module configuration from models/OneShot/1/checkpoint-546/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 09:50:44,125 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:50:44,274 >> Loading module weights from models/OneShot/1/checkpoint-546/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 09:50:44,294 >> No matching prediction head found in 'models/OneShot/1/checkpoint-546/en'\n",
            "[INFO|loading.py:77] 2022-08-25 09:50:44,294 >> Loading module configuration from models/OneShot/1/checkpoint-546/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 09:50:44,295 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 09:50:44,453 >> Loading module weights from models/OneShot/1/checkpoint-546/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 09:50:44,471 >> Loading module configuration from models/OneShot/1/checkpoint-546/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 09:50:44,472 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 09:50:45,657 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 09:50:47,396 >> Loading module weights from models/OneShot/1/checkpoint-546/pt/pytorch_model_head.bin\n",
            "100% 975/975 [12:36<00:00,  1.29it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 09:50:47,486 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:47,487 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:47,613 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:47,613 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:47,621 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:47,621 >> Configuration saved in models/OneShot/1/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:47,675 >> Module weights saved in models/OneShot/1/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:47,676 >> Configuration saved in models/OneShot/1/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:47,805 >> Module weights saved in models/OneShot/1/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:47,805 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:49,278 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:49,281 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:49,299 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 09:50:49,299 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 09:50:51,520 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 09:50:51,521 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 09:50:51,521 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =      0.058\n",
            "  train_runtime            = 0:12:32.32\n",
            "  train_samples            =       1217\n",
            "  train_samples_per_second =     40.441\n",
            "  train_steps_per_second   =      1.296\n",
            "08/25/2022 09:50:51 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:623] 2022-08-25 09:50:51,802 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 09:50:51,804 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 09:50:51,805 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 09:50:51,805 >>   Batch size = 8\n",
            "100% 35/35 [00:02<00:00, 12.70it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.8608\n",
            "  eval_f1                 =     0.8596\n",
            "  eval_loss               =     0.8867\n",
            "  eval_runtime            = 0:00:02.84\n",
            "  eval_samples            =        273\n",
            "  eval_samples_per_second =     96.033\n",
            "  eval_steps_per_second   =     12.312\n",
            "[INFO|modelcard.py:460] 2022-08-25 09:50:54,757 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.860805869102478}, {'name': 'F1', 'type': 'f1', 'value': 0.8596287075124485}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for one-shot EN-PT idiomatic knowledge transfer"
      ],
      "metadata": {
        "id": "PN3AZU2oEgRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer from English to Portuguese\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/OneShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/OneShot/EN/train.csv \\\n",
        "  --validation_file Data/OneShot/PT/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f27eD0JWXtn2",
        "outputId": "50cdbb85-c2de-4b64-c209-516b900487cd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 10:57:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 10:57:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_10-57-15_9b17356d7e44,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 10:57:15 - INFO - __main__ - load a local file for train: Data/OneShot/EN/train.csv\n",
            "08/25/2022 10:57:15 - INFO - __main__ - load a local file for validation: Data/OneShot/PT/dev.csv\n",
            "08/25/2022 10:57:15 - WARNING - datasets.builder - Using custom data configuration default-3cb437b042d82236\n",
            "08/25/2022 10:57:15 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-3cb437b042d82236/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-3cb437b042d82236/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 11214.72it/s]\n",
            "08/25/2022 10:57:15 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 10:57:15 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1424.45it/s]\n",
            "08/25/2022 10:57:15 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 10:57:15 - INFO - datasets.builder - Generating train split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 10:57:15 - INFO - datasets.builder - Generating validation split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 10:57:15 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3cb437b042d82236/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 1033.08it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:57:15,555 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:57:15,556 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:57:15,602 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:57:15,603 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:57:15,741 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:57:15,741 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:57:15,741 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:57:15,741 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:57:15,741 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:57:15,763 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:57:15,764 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 10:57:15,957 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 10:57:20,736 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 10:57:20,736 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 10:57:20,746 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 10:57:20,812 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 10:57:21,004 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:327] 2022-08-25 10:57:21,113 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 10:57:21,194 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 10:57:22,000 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 10:57:22,000 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:57:22,258 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 10:57:22,270 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 10:57:22,270 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 10:57:22,288 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 10:57:22,370 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 10:57:25,502 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 10:57:25,502 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:57:25,784 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:57:25,798 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 10:57:25,798 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 10:57:27,004 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:57:28,558 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 10:57:28,626 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with EN language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]08/25/2022 10:57:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-3cb437b042d82236/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-53cb17b7ad2aadec.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  9.61ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 10:57:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-3cb437b042d82236/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-d28caaa089337324.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 24.59ba/s]\n",
            "08/25/2022 10:57:29 - INFO - __main__ - Sample 550 of the training set: {'label': 0, 'sentence1': 'When in a current bull market it can be difficult to ask the question: when will the bull market end?', 'sentence2': 'bull market', 'input_ids': [101, 12242, 10106, 169, 14978, 11499, 11231, 17313, 10271, 10944, 10347, 25232, 10114, 63001, 10105, 20210, 131, 10841, 11337, 10105, 11499, 11231, 17313, 11572, 136, 102, 11499, 11231, 17313, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 10:57:29 - INFO - __main__ - Sample 2331 of the training set: {'label': 1, 'sentence1': 'The Pain Killer consists of light and dark rums, pineapple and orange juices, cream of coconut and nutmeg — it’s blended, making it the ideal drink to sip on the beach at the lakefront resort.', 'sentence2': 'pain killer', 'input_ids': [101, 10117, 44170, 38066, 20963, 10108, 15765, 10111, 25100, 52522, 10107, 117, 102150, 102295, 10284, 10111, 41435, 23005, 39801, 117, 93461, 10108, 11170, 23486, 11159, 10111, 11085, 10123, 10627, 10240, 100, 10271, 100, 187, 10718, 53556, 117, 14293, 10271, 10105, 29580, 69423, 10114, 10294, 10410, 10135, 10105, 45405, 10160, 10105, 24923, 31044, 60637, 119, 102, 38576, 61976, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 10:57:29 - INFO - __main__ - Sample 3286 of the training set: {'label': 1, 'sentence1': 'Renovations are scheduled to begin Monday for the P4 parking lot in Colonial Williamsburg, located behind the Merchants Square bus stop on South Henry Street, which will reopen as paid parking.', 'sentence2': 'parking lot', 'input_ids': [101, 46965, 39784, 10107, 10301, 34081, 10114, 16135, 40714, 10142, 10105, 153, 11011, 23975, 19826, 10106, 50422, 12494, 12248, 117, 11954, 17155, 10105, 91101, 10107, 19465, 19369, 20517, 10135, 11056, 11601, 11962, 117, 10319, 11337, 11639, 47656, 10146, 25938, 23975, 119, 102, 23975, 19826, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 10:57:34,755 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 10:57:34,768 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 10:57:34,768 >>   Num examples = 3414\n",
            "[INFO|trainer.py:1421] 2022-08-25 10:57:34,768 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 10:57:34,768 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 10:57:34,768 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 10:57:34,768 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 10:57:34,768 >>   Total optimization steps = 2675\n",
            "  4% 107/2675 [01:00<22:40,  1.89it/s][INFO|trainer.py:623] 2022-08-25 10:58:34,793 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:58:34,795 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:58:34,795 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:58:34,795 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.65it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.60it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.59it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.24it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.92it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.74it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.74it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.73it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.63it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.64it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.57it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.52it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.46it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.39it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2135735750198364, 'eval_accuracy': 0.6483516693115234, 'eval_f1': 0.6228413539028321, 'eval_runtime': 2.7626, 'eval_samples_per_second': 98.821, 'eval_steps_per_second': 12.669, 'epoch': 1.0}\n",
            "  4% 107/2675 [01:02<22:40,  1.89it/s]\n",
            "100% 35/35 [00:02<00:00, 13.66it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:58:37,559 >> Saving model checkpoint to models/OneShot/1/checkpoint-107\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:37,559 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:37,727 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:37,728 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:37,739 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:37,740 >> Configuration saved in models/OneShot/1/checkpoint-107/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:37,854 >> Module weights saved in models/OneShot/1/checkpoint-107/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:37,855 >> Configuration saved in models/OneShot/1/checkpoint-107/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:37,962 >> Module weights saved in models/OneShot/1/checkpoint-107/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:37,963 >> Configuration saved in models/OneShot/1/checkpoint-107/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:39,811 >> Module weights saved in models/OneShot/1/checkpoint-107/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:39,811 >> Configuration saved in models/OneShot/1/checkpoint-107/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:39,825 >> Module weights saved in models/OneShot/1/checkpoint-107/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:58:39,826 >> Configuration saved in models/OneShot/1/checkpoint-107/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:58:41,080 >> Module weights saved in models/OneShot/1/checkpoint-107/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:58:41,081 >> tokenizer config file saved in models/OneShot/1/checkpoint-107/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:58:41,081 >> Special tokens file saved in models/OneShot/1/checkpoint-107/special_tokens_map.json\n",
            "  8% 214/2675 [02:11<22:35,  1.82it/s][INFO|trainer.py:623] 2022-08-25 10:59:45,876 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:59:45,878 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:59:45,878 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:59:45,878 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.31it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.75it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.60it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.09it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.62it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.50it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.47it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.46it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.45it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.41it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.34it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.39it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.40it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.42it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.43it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.2107548713684082, 'eval_accuracy': 0.6520146727561951, 'eval_f1': 0.6519959745051995, 'eval_runtime': 2.7882, 'eval_samples_per_second': 97.914, 'eval_steps_per_second': 12.553, 'epoch': 2.0}\n",
            "  8% 214/2675 [02:13<22:35,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.63it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:59:48,668 >> Saving model checkpoint to models/OneShot/1/checkpoint-214\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:48,669 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:48,765 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:48,766 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:48,774 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:48,775 >> Configuration saved in models/OneShot/1/checkpoint-214/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:48,833 >> Module weights saved in models/OneShot/1/checkpoint-214/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:48,834 >> Configuration saved in models/OneShot/1/checkpoint-214/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:48,894 >> Module weights saved in models/OneShot/1/checkpoint-214/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:48,895 >> Configuration saved in models/OneShot/1/checkpoint-214/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:50,231 >> Module weights saved in models/OneShot/1/checkpoint-214/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:50,262 >> Configuration saved in models/OneShot/1/checkpoint-214/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:50,432 >> Module weights saved in models/OneShot/1/checkpoint-214/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:59:50,432 >> Configuration saved in models/OneShot/1/checkpoint-214/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:59:51,816 >> Module weights saved in models/OneShot/1/checkpoint-214/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:59:51,817 >> tokenizer config file saved in models/OneShot/1/checkpoint-214/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:59:51,817 >> Special tokens file saved in models/OneShot/1/checkpoint-214/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:59:52,307 >> Deleting older checkpoint [models/OneShot/1/checkpoint-107] due to args.save_total_limit\n",
            " 12% 321/2675 [03:21<21:40,  1.81it/s][INFO|trainer.py:623] 2022-08-25 11:00:56,256 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:00:56,258 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:00:56,258 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:00:56,258 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.82it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.45it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.80it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.37it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.39it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.34it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.33it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.35it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.31it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.30it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.29it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.4944775104522705, 'eval_accuracy': 0.6556776762008667, 'eval_f1': 0.6306988256965231, 'eval_runtime': 2.8085, 'eval_samples_per_second': 97.205, 'eval_steps_per_second': 12.462, 'epoch': 3.0}\n",
            " 12% 321/2675 [03:24<21:40,  1.81it/s]\n",
            "100% 35/35 [00:02<00:00, 13.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:00:59,068 >> Saving model checkpoint to models/OneShot/1/checkpoint-321\n",
            "[INFO|loading.py:60] 2022-08-25 11:00:59,068 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:00:59,161 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:00:59,161 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:00:59,169 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:00:59,169 >> Configuration saved in models/OneShot/1/checkpoint-321/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:00:59,223 >> Module weights saved in models/OneShot/1/checkpoint-321/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:00:59,223 >> Configuration saved in models/OneShot/1/checkpoint-321/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:00:59,286 >> Module weights saved in models/OneShot/1/checkpoint-321/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:00:59,287 >> Configuration saved in models/OneShot/1/checkpoint-321/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:01:00,613 >> Module weights saved in models/OneShot/1/checkpoint-321/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:01:00,643 >> Configuration saved in models/OneShot/1/checkpoint-321/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:01:00,689 >> Module weights saved in models/OneShot/1/checkpoint-321/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:01:00,690 >> Configuration saved in models/OneShot/1/checkpoint-321/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:01:02,000 >> Module weights saved in models/OneShot/1/checkpoint-321/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:01:02,001 >> tokenizer config file saved in models/OneShot/1/checkpoint-321/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:01:02,001 >> Special tokens file saved in models/OneShot/1/checkpoint-321/special_tokens_map.json\n",
            " 16% 428/2675 [04:31<20:34,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:02:06,478 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:02:06,480 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:02:06,480 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:02:06,480 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.04it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.58it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.52it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.10it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.71it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.52it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.32it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.16it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.26it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.30it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.28it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.5157266855239868, 'eval_accuracy': 0.6263736486434937, 'eval_f1': 0.5922270384254921, 'eval_runtime': 2.8037, 'eval_samples_per_second': 97.37, 'eval_steps_per_second': 12.483, 'epoch': 4.0}\n",
            " 16% 428/2675 [04:34<20:34,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.86it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:02:09,285 >> Saving model checkpoint to models/OneShot/1/checkpoint-428\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:09,286 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:09,380 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:09,381 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:09,388 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:09,388 >> Configuration saved in models/OneShot/1/checkpoint-428/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:09,443 >> Module weights saved in models/OneShot/1/checkpoint-428/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:09,444 >> Configuration saved in models/OneShot/1/checkpoint-428/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:09,505 >> Module weights saved in models/OneShot/1/checkpoint-428/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:09,506 >> Configuration saved in models/OneShot/1/checkpoint-428/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:10,857 >> Module weights saved in models/OneShot/1/checkpoint-428/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:10,988 >> Configuration saved in models/OneShot/1/checkpoint-428/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:11,006 >> Module weights saved in models/OneShot/1/checkpoint-428/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:02:11,007 >> Configuration saved in models/OneShot/1/checkpoint-428/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:02:12,434 >> Module weights saved in models/OneShot/1/checkpoint-428/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:02:12,435 >> tokenizer config file saved in models/OneShot/1/checkpoint-428/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:02:12,435 >> Special tokens file saved in models/OneShot/1/checkpoint-428/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:02:12,929 >> Deleting older checkpoint [models/OneShot/1/checkpoint-321] due to args.save_total_limit\n",
            "{'loss': 0.1414, 'learning_rate': 8.130841121495327e-05, 'epoch': 4.67}\n",
            " 20% 535/2675 [05:42<19:34,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:03:17,053 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:03:17,055 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:03:17,055 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:03:17,055 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.14it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.40it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.45it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.01it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.63it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.44it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.37it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.16it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.16it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.21it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.0246922969818115, 'eval_accuracy': 0.6153846383094788, 'eval_f1': 0.5661091349428593, 'eval_runtime': 2.8056, 'eval_samples_per_second': 97.306, 'eval_steps_per_second': 12.475, 'epoch': 5.0}\n",
            " 20% 535/2675 [05:45<19:34,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:03:19,862 >> Saving model checkpoint to models/OneShot/1/checkpoint-535\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:19,862 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:19,954 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:19,954 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:19,963 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:19,963 >> Configuration saved in models/OneShot/1/checkpoint-535/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:20,016 >> Module weights saved in models/OneShot/1/checkpoint-535/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:20,017 >> Configuration saved in models/OneShot/1/checkpoint-535/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:20,088 >> Module weights saved in models/OneShot/1/checkpoint-535/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:20,088 >> Configuration saved in models/OneShot/1/checkpoint-535/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:21,435 >> Module weights saved in models/OneShot/1/checkpoint-535/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:21,443 >> Configuration saved in models/OneShot/1/checkpoint-535/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:21,457 >> Module weights saved in models/OneShot/1/checkpoint-535/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:03:21,457 >> Configuration saved in models/OneShot/1/checkpoint-535/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:03:22,957 >> Module weights saved in models/OneShot/1/checkpoint-535/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:03:22,957 >> tokenizer config file saved in models/OneShot/1/checkpoint-535/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:03:22,958 >> Special tokens file saved in models/OneShot/1/checkpoint-535/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:03:23,480 >> Deleting older checkpoint [models/OneShot/1/checkpoint-428] due to args.save_total_limit\n",
            " 24% 642/2675 [06:52<18:39,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:04:27,598 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:04:27,600 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:04:27,600 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:04:27,600 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.99it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.49it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.46it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.03it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.69it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.61it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.54it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.48it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.45it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.37it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.37it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.2804298400878906, 'eval_accuracy': 0.6300366520881653, 'eval_f1': 0.5933485731140772, 'eval_runtime': 2.7895, 'eval_samples_per_second': 97.867, 'eval_steps_per_second': 12.547, 'epoch': 6.0}\n",
            " 24% 642/2675 [06:55<18:39,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:04:30,391 >> Saving model checkpoint to models/OneShot/1/checkpoint-642\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:30,392 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:30,486 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:30,487 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:30,494 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:30,495 >> Configuration saved in models/OneShot/1/checkpoint-642/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:30,553 >> Module weights saved in models/OneShot/1/checkpoint-642/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:30,553 >> Configuration saved in models/OneShot/1/checkpoint-642/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:30,617 >> Module weights saved in models/OneShot/1/checkpoint-642/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:30,617 >> Configuration saved in models/OneShot/1/checkpoint-642/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:31,823 >> Module weights saved in models/OneShot/1/checkpoint-642/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:31,876 >> Configuration saved in models/OneShot/1/checkpoint-642/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:32,026 >> Module weights saved in models/OneShot/1/checkpoint-642/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:04:32,027 >> Configuration saved in models/OneShot/1/checkpoint-642/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:04:33,347 >> Module weights saved in models/OneShot/1/checkpoint-642/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:04:33,347 >> tokenizer config file saved in models/OneShot/1/checkpoint-642/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:04:33,348 >> Special tokens file saved in models/OneShot/1/checkpoint-642/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:04:33,875 >> Deleting older checkpoint [models/OneShot/1/checkpoint-535] due to args.save_total_limit\n",
            " 28% 749/2675 [08:03<17:37,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:05:37,918 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:05:37,920 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:05:37,920 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:05:37,920 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.73it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.16it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.91it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.64it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.58it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.53it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.45it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.35it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.31it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.31it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.1400105953216553, 'eval_accuracy': 0.66300368309021, 'eval_f1': 0.6508174841508174, 'eval_runtime': 2.7968, 'eval_samples_per_second': 97.61, 'eval_steps_per_second': 12.514, 'epoch': 7.0}\n",
            " 28% 749/2675 [08:05<17:37,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.83it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:05:40,718 >> Saving model checkpoint to models/OneShot/1/checkpoint-749\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:40,719 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:40,810 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:40,811 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:40,818 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:40,819 >> Configuration saved in models/OneShot/1/checkpoint-749/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:40,872 >> Module weights saved in models/OneShot/1/checkpoint-749/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:40,873 >> Configuration saved in models/OneShot/1/checkpoint-749/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:40,943 >> Module weights saved in models/OneShot/1/checkpoint-749/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:40,944 >> Configuration saved in models/OneShot/1/checkpoint-749/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:41,929 >> Module weights saved in models/OneShot/1/checkpoint-749/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:41,972 >> Configuration saved in models/OneShot/1/checkpoint-749/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:42,139 >> Module weights saved in models/OneShot/1/checkpoint-749/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:05:42,139 >> Configuration saved in models/OneShot/1/checkpoint-749/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:05:43,609 >> Module weights saved in models/OneShot/1/checkpoint-749/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:05:43,610 >> tokenizer config file saved in models/OneShot/1/checkpoint-749/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:05:43,610 >> Special tokens file saved in models/OneShot/1/checkpoint-749/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:05:44,110 >> Deleting older checkpoint [models/OneShot/1/checkpoint-642] due to args.save_total_limit\n",
            " 32% 856/2675 [09:13<16:42,  1.81it/s][INFO|trainer.py:623] 2022-08-25 11:06:48,196 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:06:48,198 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:06:48,198 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:06:48,198 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.21it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.68it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.50it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.07it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.69it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.49it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.37it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.19it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.25it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.26it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.1200740337371826, 'eval_accuracy': 0.6959707140922546, 'eval_f1': 0.6826159427394842, 'eval_runtime': 2.8067, 'eval_samples_per_second': 97.269, 'eval_steps_per_second': 12.47, 'epoch': 8.0}\n",
            " 32% 856/2675 [09:16<16:42,  1.81it/s]\n",
            "100% 35/35 [00:02<00:00, 13.75it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:06:51,006 >> Saving model checkpoint to models/OneShot/1/checkpoint-856\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:51,007 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:51,100 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:51,100 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:51,108 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:51,108 >> Configuration saved in models/OneShot/1/checkpoint-856/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:51,163 >> Module weights saved in models/OneShot/1/checkpoint-856/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:51,164 >> Configuration saved in models/OneShot/1/checkpoint-856/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:51,233 >> Module weights saved in models/OneShot/1/checkpoint-856/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:51,233 >> Configuration saved in models/OneShot/1/checkpoint-856/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:52,643 >> Module weights saved in models/OneShot/1/checkpoint-856/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:52,768 >> Configuration saved in models/OneShot/1/checkpoint-856/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:52,783 >> Module weights saved in models/OneShot/1/checkpoint-856/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:06:52,785 >> Configuration saved in models/OneShot/1/checkpoint-856/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:06:54,234 >> Module weights saved in models/OneShot/1/checkpoint-856/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:06:54,234 >> tokenizer config file saved in models/OneShot/1/checkpoint-856/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:06:54,234 >> Special tokens file saved in models/OneShot/1/checkpoint-856/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:06:54,744 >> Deleting older checkpoint [models/OneShot/1/checkpoint-214] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:06:54,780 >> Deleting older checkpoint [models/OneShot/1/checkpoint-749] due to args.save_total_limit\n",
            " 36% 963/2675 [10:24<15:40,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:07:58,947 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:07:58,949 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:07:58,949 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:07:58,949 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.11it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.64it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.58it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.08it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.61it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.40it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.26it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.20it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.20it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.25it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.3972110748291016, 'eval_accuracy': 0.6410256624221802, 'eval_f1': 0.6082181349578256, 'eval_runtime': 2.8013, 'eval_samples_per_second': 97.456, 'eval_steps_per_second': 12.494, 'epoch': 9.0}\n",
            " 36% 963/2675 [10:26<15:40,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.90it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:08:01,751 >> Saving model checkpoint to models/OneShot/1/checkpoint-963\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:01,752 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:01,848 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:01,848 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:01,856 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:01,857 >> Configuration saved in models/OneShot/1/checkpoint-963/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:01,911 >> Module weights saved in models/OneShot/1/checkpoint-963/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:01,912 >> Configuration saved in models/OneShot/1/checkpoint-963/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:01,977 >> Module weights saved in models/OneShot/1/checkpoint-963/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:01,978 >> Configuration saved in models/OneShot/1/checkpoint-963/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:03,380 >> Module weights saved in models/OneShot/1/checkpoint-963/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:03,546 >> Configuration saved in models/OneShot/1/checkpoint-963/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:03,566 >> Module weights saved in models/OneShot/1/checkpoint-963/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:08:03,567 >> Configuration saved in models/OneShot/1/checkpoint-963/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:08:04,992 >> Module weights saved in models/OneShot/1/checkpoint-963/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:08:04,993 >> tokenizer config file saved in models/OneShot/1/checkpoint-963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:08:04,993 >> Special tokens file saved in models/OneShot/1/checkpoint-963/special_tokens_map.json\n",
            "{'loss': 0.0225, 'learning_rate': 6.261682242990654e-05, 'epoch': 9.35}\n",
            " 40% 1070/2675 [11:34<14:43,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:09:09,523 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:09:09,525 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:09:09,525 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:09:09,525 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.93it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.43it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.46it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.01it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.72it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.64it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.56it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.48it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.42it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.32it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.30it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.294675588607788, 'eval_accuracy': 0.6849817037582397, 'eval_f1': 0.6660502958579881, 'eval_runtime': 2.7902, 'eval_samples_per_second': 97.843, 'eval_steps_per_second': 12.544, 'epoch': 10.0}\n",
            " 40% 1070/2675 [11:37<14:43,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.78it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:09:12,316 >> Saving model checkpoint to models/OneShot/1/checkpoint-1070\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:12,317 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:12,411 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:12,411 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:12,420 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:12,420 >> Configuration saved in models/OneShot/1/checkpoint-1070/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:12,475 >> Module weights saved in models/OneShot/1/checkpoint-1070/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:12,475 >> Configuration saved in models/OneShot/1/checkpoint-1070/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:12,570 >> Module weights saved in models/OneShot/1/checkpoint-1070/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:12,570 >> Configuration saved in models/OneShot/1/checkpoint-1070/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:13,573 >> Module weights saved in models/OneShot/1/checkpoint-1070/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:13,646 >> Configuration saved in models/OneShot/1/checkpoint-1070/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:13,660 >> Module weights saved in models/OneShot/1/checkpoint-1070/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:09:13,661 >> Configuration saved in models/OneShot/1/checkpoint-1070/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:09:15,276 >> Module weights saved in models/OneShot/1/checkpoint-1070/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:09:15,277 >> tokenizer config file saved in models/OneShot/1/checkpoint-1070/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:09:15,277 >> Special tokens file saved in models/OneShot/1/checkpoint-1070/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:09:15,751 >> Deleting older checkpoint [models/OneShot/1/checkpoint-963] due to args.save_total_limit\n",
            " 44% 1177/2675 [12:45<13:41,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:10:19,830 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:10:19,832 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:10:19,832 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:10:19,832 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.02it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.38it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.12it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.82it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.47it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.47it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.45it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.40it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.31it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.33it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.29it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.208594799041748, 'eval_accuracy': 0.6117216348648071, 'eval_f1': 0.5582254518808012, 'eval_runtime': 2.8024, 'eval_samples_per_second': 97.418, 'eval_steps_per_second': 12.49, 'epoch': 11.0}\n",
            " 44% 1177/2675 [12:47<13:41,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.86it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:10:22,636 >> Saving model checkpoint to models/OneShot/1/checkpoint-1177\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:22,637 >> Configuration saved in models/OneShot/1/checkpoint-1177/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:22,730 >> Module weights saved in models/OneShot/1/checkpoint-1177/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:22,730 >> Configuration saved in models/OneShot/1/checkpoint-1177/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:22,738 >> Module weights saved in models/OneShot/1/checkpoint-1177/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:22,738 >> Configuration saved in models/OneShot/1/checkpoint-1177/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:22,793 >> Module weights saved in models/OneShot/1/checkpoint-1177/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:22,794 >> Configuration saved in models/OneShot/1/checkpoint-1177/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:22,872 >> Module weights saved in models/OneShot/1/checkpoint-1177/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:22,873 >> Configuration saved in models/OneShot/1/checkpoint-1177/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:24,165 >> Module weights saved in models/OneShot/1/checkpoint-1177/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:24,191 >> Configuration saved in models/OneShot/1/checkpoint-1177/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:24,205 >> Module weights saved in models/OneShot/1/checkpoint-1177/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:10:24,205 >> Configuration saved in models/OneShot/1/checkpoint-1177/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:10:25,800 >> Module weights saved in models/OneShot/1/checkpoint-1177/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:10:25,800 >> tokenizer config file saved in models/OneShot/1/checkpoint-1177/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:10:25,801 >> Special tokens file saved in models/OneShot/1/checkpoint-1177/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:10:26,297 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1070] due to args.save_total_limit\n",
            " 48% 1284/2675 [13:55<12:45,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:11:30,425 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:11:30,427 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:11:30,427 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:11:30,427 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.95it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.40it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.46it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.04it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.74it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.65it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.57it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.46it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.39it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.34it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.36it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.39it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.37it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.29it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.24it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7887446880340576, 'eval_accuracy': 0.6300366520881653, 'eval_f1': 0.6006980145685198, 'eval_runtime': 2.7938, 'eval_samples_per_second': 97.717, 'eval_steps_per_second': 12.528, 'epoch': 12.0}\n",
            " 48% 1284/2675 [13:58<12:45,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.69it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:11:33,223 >> Saving model checkpoint to models/OneShot/1/checkpoint-1284\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:33,223 >> Configuration saved in models/OneShot/1/checkpoint-1284/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:33,316 >> Module weights saved in models/OneShot/1/checkpoint-1284/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:33,317 >> Configuration saved in models/OneShot/1/checkpoint-1284/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:33,324 >> Module weights saved in models/OneShot/1/checkpoint-1284/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:33,325 >> Configuration saved in models/OneShot/1/checkpoint-1284/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:33,378 >> Module weights saved in models/OneShot/1/checkpoint-1284/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:33,379 >> Configuration saved in models/OneShot/1/checkpoint-1284/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:33,452 >> Module weights saved in models/OneShot/1/checkpoint-1284/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:33,452 >> Configuration saved in models/OneShot/1/checkpoint-1284/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:34,861 >> Module weights saved in models/OneShot/1/checkpoint-1284/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:34,930 >> Configuration saved in models/OneShot/1/checkpoint-1284/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:34,949 >> Module weights saved in models/OneShot/1/checkpoint-1284/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:11:34,950 >> Configuration saved in models/OneShot/1/checkpoint-1284/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:11:36,455 >> Module weights saved in models/OneShot/1/checkpoint-1284/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:11:36,456 >> tokenizer config file saved in models/OneShot/1/checkpoint-1284/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:11:36,456 >> Special tokens file saved in models/OneShot/1/checkpoint-1284/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:11:36,932 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1177] due to args.save_total_limit\n",
            " 52% 1391/2675 [15:06<11:43,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:12:41,185 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:12:41,187 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:12:41,187 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:12:41,188 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.14it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.69it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.59it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.16it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.76it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.57it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.34it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.14it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.16it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.22it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.24it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.31it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.28it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.26it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.911010265350342, 'eval_accuracy': 0.6336996555328369, 'eval_f1': 0.6020408163265306, 'eval_runtime': 2.7993, 'eval_samples_per_second': 97.525, 'eval_steps_per_second': 12.503, 'epoch': 13.0}\n",
            " 52% 1391/2675 [15:09<11:43,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:12:43,988 >> Saving model checkpoint to models/OneShot/1/checkpoint-1391\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:43,989 >> Configuration saved in models/OneShot/1/checkpoint-1391/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:44,085 >> Module weights saved in models/OneShot/1/checkpoint-1391/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:44,086 >> Configuration saved in models/OneShot/1/checkpoint-1391/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:44,093 >> Module weights saved in models/OneShot/1/checkpoint-1391/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:44,094 >> Configuration saved in models/OneShot/1/checkpoint-1391/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:44,150 >> Module weights saved in models/OneShot/1/checkpoint-1391/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:44,151 >> Configuration saved in models/OneShot/1/checkpoint-1391/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:44,222 >> Module weights saved in models/OneShot/1/checkpoint-1391/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:44,223 >> Configuration saved in models/OneShot/1/checkpoint-1391/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:45,718 >> Module weights saved in models/OneShot/1/checkpoint-1391/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:45,776 >> Configuration saved in models/OneShot/1/checkpoint-1391/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:45,792 >> Module weights saved in models/OneShot/1/checkpoint-1391/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:12:45,792 >> Configuration saved in models/OneShot/1/checkpoint-1391/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:12:47,298 >> Module weights saved in models/OneShot/1/checkpoint-1391/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:12:47,299 >> tokenizer config file saved in models/OneShot/1/checkpoint-1391/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:12:47,300 >> Special tokens file saved in models/OneShot/1/checkpoint-1391/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:12:47,805 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1284] due to args.save_total_limit\n",
            " 56% 1498/2675 [16:17<10:48,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:13:51,938 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:13:51,940 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:13:51,940 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:13:51,940 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.56it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:01, 15.01it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.74it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.13it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.62it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.38it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.31it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.25it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.27it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.21it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.19it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.23it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.28it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7992494106292725, 'eval_accuracy': 0.6813187003135681, 'eval_f1': 0.663988116290585, 'eval_runtime': 2.8016, 'eval_samples_per_second': 97.444, 'eval_steps_per_second': 12.493, 'epoch': 14.0}\n",
            " 56% 1498/2675 [16:19<10:48,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.74it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:13:54,743 >> Saving model checkpoint to models/OneShot/1/checkpoint-1498\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:54,744 >> Configuration saved in models/OneShot/1/checkpoint-1498/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:54,842 >> Module weights saved in models/OneShot/1/checkpoint-1498/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:54,842 >> Configuration saved in models/OneShot/1/checkpoint-1498/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:54,850 >> Module weights saved in models/OneShot/1/checkpoint-1498/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:54,851 >> Configuration saved in models/OneShot/1/checkpoint-1498/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:54,907 >> Module weights saved in models/OneShot/1/checkpoint-1498/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:54,908 >> Configuration saved in models/OneShot/1/checkpoint-1498/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:54,983 >> Module weights saved in models/OneShot/1/checkpoint-1498/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:54,984 >> Configuration saved in models/OneShot/1/checkpoint-1498/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:56,128 >> Module weights saved in models/OneShot/1/checkpoint-1498/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:56,173 >> Configuration saved in models/OneShot/1/checkpoint-1498/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:56,187 >> Module weights saved in models/OneShot/1/checkpoint-1498/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:13:56,188 >> Configuration saved in models/OneShot/1/checkpoint-1498/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:13:57,762 >> Module weights saved in models/OneShot/1/checkpoint-1498/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:13:57,763 >> tokenizer config file saved in models/OneShot/1/checkpoint-1498/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:13:57,763 >> Special tokens file saved in models/OneShot/1/checkpoint-1498/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:13:58,258 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1391] due to args.save_total_limit\n",
            "{'loss': 0.005, 'learning_rate': 4.392523364485982e-05, 'epoch': 14.02}\n",
            " 60% 1605/2675 [17:27<09:47,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:15:02,383 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:15:02,385 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:15:02,385 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:15:02,385 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.06it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.56it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.54it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.11it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.74it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.61it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.56it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.46it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.42it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.41it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.33it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.29it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9673357009887695, 'eval_accuracy': 0.6849817037582397, 'eval_f1': 0.6634846921224631, 'eval_runtime': 2.7831, 'eval_samples_per_second': 98.09, 'eval_steps_per_second': 12.576, 'epoch': 15.0}\n",
            " 60% 1605/2675 [17:30<09:47,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.93it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:15:05,170 >> Saving model checkpoint to models/OneShot/1/checkpoint-1605\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:05,170 >> Configuration saved in models/OneShot/1/checkpoint-1605/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:05,266 >> Module weights saved in models/OneShot/1/checkpoint-1605/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:05,266 >> Configuration saved in models/OneShot/1/checkpoint-1605/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:05,274 >> Module weights saved in models/OneShot/1/checkpoint-1605/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:05,274 >> Configuration saved in models/OneShot/1/checkpoint-1605/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:05,328 >> Module weights saved in models/OneShot/1/checkpoint-1605/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:05,329 >> Configuration saved in models/OneShot/1/checkpoint-1605/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:05,415 >> Module weights saved in models/OneShot/1/checkpoint-1605/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:05,416 >> Configuration saved in models/OneShot/1/checkpoint-1605/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:06,817 >> Module weights saved in models/OneShot/1/checkpoint-1605/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:06,941 >> Configuration saved in models/OneShot/1/checkpoint-1605/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:06,965 >> Module weights saved in models/OneShot/1/checkpoint-1605/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:15:06,966 >> Configuration saved in models/OneShot/1/checkpoint-1605/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:15:08,356 >> Module weights saved in models/OneShot/1/checkpoint-1605/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:15:08,357 >> tokenizer config file saved in models/OneShot/1/checkpoint-1605/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:15:08,357 >> Special tokens file saved in models/OneShot/1/checkpoint-1605/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:15:08,845 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1498] due to args.save_total_limit\n",
            " 64% 1712/2675 [18:38<08:50,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:16:12,925 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:16:12,927 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:16:12,927 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:16:12,927 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.29it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.77it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.65it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.15it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.71it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.57it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.51it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.47it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.44it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.42it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.40it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.34it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.8122572898864746, 'eval_accuracy': 0.6813187003135681, 'eval_f1': 0.6651440172566934, 'eval_runtime': 2.7862, 'eval_samples_per_second': 97.984, 'eval_steps_per_second': 12.562, 'epoch': 16.0}\n",
            " 64% 1712/2675 [18:40<08:50,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.75it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:16:15,715 >> Saving model checkpoint to models/OneShot/1/checkpoint-1712\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:15,716 >> Configuration saved in models/OneShot/1/checkpoint-1712/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:15,807 >> Module weights saved in models/OneShot/1/checkpoint-1712/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:15,808 >> Configuration saved in models/OneShot/1/checkpoint-1712/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:15,815 >> Module weights saved in models/OneShot/1/checkpoint-1712/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:15,816 >> Configuration saved in models/OneShot/1/checkpoint-1712/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:15,868 >> Module weights saved in models/OneShot/1/checkpoint-1712/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:15,869 >> Configuration saved in models/OneShot/1/checkpoint-1712/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:15,947 >> Module weights saved in models/OneShot/1/checkpoint-1712/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:15,947 >> Configuration saved in models/OneShot/1/checkpoint-1712/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:17,418 >> Module weights saved in models/OneShot/1/checkpoint-1712/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:17,419 >> Configuration saved in models/OneShot/1/checkpoint-1712/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:17,430 >> Module weights saved in models/OneShot/1/checkpoint-1712/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:16:17,430 >> Configuration saved in models/OneShot/1/checkpoint-1712/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:16:18,828 >> Module weights saved in models/OneShot/1/checkpoint-1712/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:16:18,829 >> tokenizer config file saved in models/OneShot/1/checkpoint-1712/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:16:18,829 >> Special tokens file saved in models/OneShot/1/checkpoint-1712/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:16:19,351 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1605] due to args.save_total_limit\n",
            " 68% 1819/2675 [19:48<07:48,  1.83it/s][INFO|trainer.py:623] 2022-08-25 11:17:23,490 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:17:23,492 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:17:23,492 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:17:23,492 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.58it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.21it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.27it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.82it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.36it/s]\u001b[A\n",
            " 37% 13/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.33it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.24it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.30it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.33it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.30it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.24it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.29it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.045073986053467, 'eval_accuracy': 0.66300368309021, 'eval_f1': 0.6385562974902141, 'eval_runtime': 2.8152, 'eval_samples_per_second': 96.972, 'eval_steps_per_second': 12.432, 'epoch': 17.0}\n",
            " 68% 1819/2675 [19:51<07:48,  1.83it/s]\n",
            "100% 35/35 [00:02<00:00, 13.79it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:17:26,309 >> Saving model checkpoint to models/OneShot/1/checkpoint-1819\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:26,310 >> Configuration saved in models/OneShot/1/checkpoint-1819/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:26,408 >> Module weights saved in models/OneShot/1/checkpoint-1819/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:26,408 >> Configuration saved in models/OneShot/1/checkpoint-1819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:26,417 >> Module weights saved in models/OneShot/1/checkpoint-1819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:26,418 >> Configuration saved in models/OneShot/1/checkpoint-1819/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:26,476 >> Module weights saved in models/OneShot/1/checkpoint-1819/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:26,476 >> Configuration saved in models/OneShot/1/checkpoint-1819/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:26,546 >> Module weights saved in models/OneShot/1/checkpoint-1819/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:26,546 >> Configuration saved in models/OneShot/1/checkpoint-1819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:27,640 >> Module weights saved in models/OneShot/1/checkpoint-1819/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:27,671 >> Configuration saved in models/OneShot/1/checkpoint-1819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:27,687 >> Module weights saved in models/OneShot/1/checkpoint-1819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:17:27,687 >> Configuration saved in models/OneShot/1/checkpoint-1819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:17:29,152 >> Module weights saved in models/OneShot/1/checkpoint-1819/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:17:29,153 >> tokenizer config file saved in models/OneShot/1/checkpoint-1819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:17:29,153 >> Special tokens file saved in models/OneShot/1/checkpoint-1819/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:17:29,656 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1712] due to args.save_total_limit\n",
            " 72% 1926/2675 [20:58<06:52,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:18:33,730 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:18:33,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:18:33,732 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:18:33,732 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.92it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.36it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.38it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.04it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.72it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.58it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.51it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.46it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.40it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.41it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.39it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.37it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.34it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.8320581912994385, 'eval_accuracy': 0.6849817037582397, 'eval_f1': 0.6672619047619048, 'eval_runtime': 2.7975, 'eval_samples_per_second': 97.588, 'eval_steps_per_second': 12.511, 'epoch': 18.0}\n",
            " 72% 1926/2675 [21:01<06:52,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.63it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:18:36,531 >> Saving model checkpoint to models/OneShot/1/checkpoint-1926\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:36,532 >> Configuration saved in models/OneShot/1/checkpoint-1926/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:36,622 >> Module weights saved in models/OneShot/1/checkpoint-1926/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:36,623 >> Configuration saved in models/OneShot/1/checkpoint-1926/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:36,630 >> Module weights saved in models/OneShot/1/checkpoint-1926/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:36,631 >> Configuration saved in models/OneShot/1/checkpoint-1926/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:36,700 >> Module weights saved in models/OneShot/1/checkpoint-1926/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:36,701 >> Configuration saved in models/OneShot/1/checkpoint-1926/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:36,771 >> Module weights saved in models/OneShot/1/checkpoint-1926/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:36,771 >> Configuration saved in models/OneShot/1/checkpoint-1926/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:38,250 >> Module weights saved in models/OneShot/1/checkpoint-1926/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:38,304 >> Configuration saved in models/OneShot/1/checkpoint-1926/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:38,318 >> Module weights saved in models/OneShot/1/checkpoint-1926/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:18:38,318 >> Configuration saved in models/OneShot/1/checkpoint-1926/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:18:39,725 >> Module weights saved in models/OneShot/1/checkpoint-1926/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:18:39,725 >> tokenizer config file saved in models/OneShot/1/checkpoint-1926/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:18:39,725 >> Special tokens file saved in models/OneShot/1/checkpoint-1926/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:18:40,233 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1819] due to args.save_total_limit\n",
            "{'loss': 0.002, 'learning_rate': 2.5233644859813084e-05, 'epoch': 18.69}\n",
            " 76% 2033/2675 [22:09<05:53,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:19:44,296 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:19:44,298 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:19:44,298 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:19:44,298 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.23it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.70it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.45it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.88it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.35it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.38it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.41it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.35it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.35it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.20it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.15it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.13it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.05it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.03it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.11it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.6855101585388184, 'eval_accuracy': 0.7032967209815979, 'eval_f1': 0.6938317757009346, 'eval_runtime': 2.8232, 'eval_samples_per_second': 96.697, 'eval_steps_per_second': 12.397, 'epoch': 19.0}\n",
            " 76% 2033/2675 [22:12<05:53,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:19:47,123 >> Saving model checkpoint to models/OneShot/1/checkpoint-2033\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:47,123 >> Configuration saved in models/OneShot/1/checkpoint-2033/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:47,217 >> Module weights saved in models/OneShot/1/checkpoint-2033/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:47,218 >> Configuration saved in models/OneShot/1/checkpoint-2033/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:47,226 >> Module weights saved in models/OneShot/1/checkpoint-2033/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:47,226 >> Configuration saved in models/OneShot/1/checkpoint-2033/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:47,284 >> Module weights saved in models/OneShot/1/checkpoint-2033/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:47,285 >> Configuration saved in models/OneShot/1/checkpoint-2033/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:47,356 >> Module weights saved in models/OneShot/1/checkpoint-2033/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:47,357 >> Configuration saved in models/OneShot/1/checkpoint-2033/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:48,818 >> Module weights saved in models/OneShot/1/checkpoint-2033/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:48,849 >> Configuration saved in models/OneShot/1/checkpoint-2033/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:48,867 >> Module weights saved in models/OneShot/1/checkpoint-2033/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:19:48,867 >> Configuration saved in models/OneShot/1/checkpoint-2033/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:19:50,411 >> Module weights saved in models/OneShot/1/checkpoint-2033/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:19:50,411 >> tokenizer config file saved in models/OneShot/1/checkpoint-2033/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:19:50,411 >> Special tokens file saved in models/OneShot/1/checkpoint-2033/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:19:50,888 >> Deleting older checkpoint [models/OneShot/1/checkpoint-856] due to args.save_total_limit\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:19:50,900 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1926] due to args.save_total_limit\n",
            " 80% 2140/2675 [23:20<04:54,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:20:54,949 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:20:54,950 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:20:54,951 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:20:54,951 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.13it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.61it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.58it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.15it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.74it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.58it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.53it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.43it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.39it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.41it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.29it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.29it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7248387336730957, 'eval_accuracy': 0.7032967209815979, 'eval_f1': 0.6921266968325792, 'eval_runtime': 2.7854, 'eval_samples_per_second': 98.011, 'eval_steps_per_second': 12.565, 'epoch': 20.0}\n",
            " 80% 2140/2675 [23:22<04:54,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.88it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:20:57,742 >> Saving model checkpoint to models/OneShot/1/checkpoint-2140\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:57,743 >> Configuration saved in models/OneShot/1/checkpoint-2140/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:20:57,841 >> Module weights saved in models/OneShot/1/checkpoint-2140/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:57,842 >> Configuration saved in models/OneShot/1/checkpoint-2140/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:20:57,850 >> Module weights saved in models/OneShot/1/checkpoint-2140/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:57,850 >> Configuration saved in models/OneShot/1/checkpoint-2140/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:20:57,904 >> Module weights saved in models/OneShot/1/checkpoint-2140/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:57,904 >> Configuration saved in models/OneShot/1/checkpoint-2140/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:20:57,974 >> Module weights saved in models/OneShot/1/checkpoint-2140/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:57,975 >> Configuration saved in models/OneShot/1/checkpoint-2140/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:20:59,127 >> Module weights saved in models/OneShot/1/checkpoint-2140/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:59,239 >> Configuration saved in models/OneShot/1/checkpoint-2140/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:20:59,261 >> Module weights saved in models/OneShot/1/checkpoint-2140/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:20:59,262 >> Configuration saved in models/OneShot/1/checkpoint-2140/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:21:00,734 >> Module weights saved in models/OneShot/1/checkpoint-2140/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:21:00,735 >> tokenizer config file saved in models/OneShot/1/checkpoint-2140/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:21:00,735 >> Special tokens file saved in models/OneShot/1/checkpoint-2140/special_tokens_map.json\n",
            " 84% 2247/2675 [24:30<03:54,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:22:05,235 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:22:05,237 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:22:05,237 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:22:05,237 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.19it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.72it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.55it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.14it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.83it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.63it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.57it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.49it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.42it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.33it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.32it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.31it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.28it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7888147830963135, 'eval_accuracy': 0.6959707140922546, 'eval_f1': 0.6835909681203134, 'eval_runtime': 2.7882, 'eval_samples_per_second': 97.912, 'eval_steps_per_second': 12.553, 'epoch': 21.0}\n",
            " 84% 2247/2675 [24:33<03:54,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.75it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:22:08,028 >> Saving model checkpoint to models/OneShot/1/checkpoint-2247\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:08,028 >> Configuration saved in models/OneShot/1/checkpoint-2247/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:08,126 >> Module weights saved in models/OneShot/1/checkpoint-2247/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:08,127 >> Configuration saved in models/OneShot/1/checkpoint-2247/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:08,134 >> Module weights saved in models/OneShot/1/checkpoint-2247/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:08,135 >> Configuration saved in models/OneShot/1/checkpoint-2247/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:08,190 >> Module weights saved in models/OneShot/1/checkpoint-2247/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:08,191 >> Configuration saved in models/OneShot/1/checkpoint-2247/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:08,298 >> Module weights saved in models/OneShot/1/checkpoint-2247/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:08,299 >> Configuration saved in models/OneShot/1/checkpoint-2247/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:09,772 >> Module weights saved in models/OneShot/1/checkpoint-2247/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:09,797 >> Configuration saved in models/OneShot/1/checkpoint-2247/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:09,810 >> Module weights saved in models/OneShot/1/checkpoint-2247/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:22:09,811 >> Configuration saved in models/OneShot/1/checkpoint-2247/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:22:11,294 >> Module weights saved in models/OneShot/1/checkpoint-2247/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:22:11,294 >> tokenizer config file saved in models/OneShot/1/checkpoint-2247/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:22:11,294 >> Special tokens file saved in models/OneShot/1/checkpoint-2247/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:22:11,760 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2140] due to args.save_total_limit\n",
            " 88% 2354/2675 [25:41<02:56,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:23:15,872 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:23:15,873 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:23:15,874 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:23:15,874 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.13it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.62it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.58it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.11it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.72it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.62it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.55it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.45it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.35it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.30it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.33it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.36it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.7931246757507324, 'eval_accuracy': 0.6959707140922546, 'eval_f1': 0.6815991006815147, 'eval_runtime': 2.7912, 'eval_samples_per_second': 97.806, 'eval_steps_per_second': 12.539, 'epoch': 22.0}\n",
            " 88% 2354/2675 [25:43<02:56,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.73it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:23:18,666 >> Saving model checkpoint to models/OneShot/1/checkpoint-2354\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:18,667 >> Configuration saved in models/OneShot/1/checkpoint-2354/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:18,767 >> Module weights saved in models/OneShot/1/checkpoint-2354/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:18,767 >> Configuration saved in models/OneShot/1/checkpoint-2354/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:18,779 >> Module weights saved in models/OneShot/1/checkpoint-2354/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:18,780 >> Configuration saved in models/OneShot/1/checkpoint-2354/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:18,840 >> Module weights saved in models/OneShot/1/checkpoint-2354/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:18,841 >> Configuration saved in models/OneShot/1/checkpoint-2354/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:18,910 >> Module weights saved in models/OneShot/1/checkpoint-2354/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:18,911 >> Configuration saved in models/OneShot/1/checkpoint-2354/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:20,347 >> Module weights saved in models/OneShot/1/checkpoint-2354/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:20,425 >> Configuration saved in models/OneShot/1/checkpoint-2354/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:20,514 >> Module weights saved in models/OneShot/1/checkpoint-2354/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:23:20,514 >> Configuration saved in models/OneShot/1/checkpoint-2354/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:23:21,917 >> Module weights saved in models/OneShot/1/checkpoint-2354/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:23:21,920 >> tokenizer config file saved in models/OneShot/1/checkpoint-2354/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:23:21,921 >> Special tokens file saved in models/OneShot/1/checkpoint-2354/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:23:22,406 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2247] due to args.save_total_limit\n",
            " 92% 2461/2675 [26:51<01:57,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:24:26,426 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:24:26,428 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:24:26,428 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:24:26,428 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.54it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.50it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.26it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:02, 12.86it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.47it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.42it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.43it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.39it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.36it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.26it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.23it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.29it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.824950695037842, 'eval_accuracy': 0.6959707140922546, 'eval_f1': 0.6805396946242016, 'eval_runtime': 2.8003, 'eval_samples_per_second': 97.49, 'eval_steps_per_second': 12.499, 'epoch': 23.0}\n",
            " 92% 2461/2675 [26:54<01:57,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.89it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:24:29,230 >> Saving model checkpoint to models/OneShot/1/checkpoint-2461\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:29,230 >> Configuration saved in models/OneShot/1/checkpoint-2461/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:29,325 >> Module weights saved in models/OneShot/1/checkpoint-2461/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:29,325 >> Configuration saved in models/OneShot/1/checkpoint-2461/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:29,333 >> Module weights saved in models/OneShot/1/checkpoint-2461/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:29,334 >> Configuration saved in models/OneShot/1/checkpoint-2461/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:29,388 >> Module weights saved in models/OneShot/1/checkpoint-2461/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:29,389 >> Configuration saved in models/OneShot/1/checkpoint-2461/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:29,475 >> Module weights saved in models/OneShot/1/checkpoint-2461/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:29,476 >> Configuration saved in models/OneShot/1/checkpoint-2461/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:30,577 >> Module weights saved in models/OneShot/1/checkpoint-2461/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:30,629 >> Configuration saved in models/OneShot/1/checkpoint-2461/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:30,764 >> Module weights saved in models/OneShot/1/checkpoint-2461/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:24:30,765 >> Configuration saved in models/OneShot/1/checkpoint-2461/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:24:32,168 >> Module weights saved in models/OneShot/1/checkpoint-2461/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:24:32,169 >> tokenizer config file saved in models/OneShot/1/checkpoint-2461/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:24:32,169 >> Special tokens file saved in models/OneShot/1/checkpoint-2461/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:24:32,676 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2354] due to args.save_total_limit\n",
            "{'loss': 0.0008, 'learning_rate': 6.542056074766355e-06, 'epoch': 23.36}\n",
            " 96% 2568/2675 [28:01<00:58,  1.81it/s][INFO|trainer.py:623] 2022-08-25 11:25:36,740 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:25:36,742 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:25:36,742 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:25:36,742 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 18.51it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.81it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.58it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.12it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.58it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.33it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.30it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.24it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.17it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.18it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.12it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.19it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.27it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.25it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9280941486358643, 'eval_accuracy': 0.692307710647583, 'eval_f1': 0.675, 'eval_runtime': 2.8114, 'eval_samples_per_second': 97.105, 'eval_steps_per_second': 12.449, 'epoch': 24.0}\n",
            " 96% 2568/2675 [28:04<00:58,  1.81it/s]\n",
            "100% 35/35 [00:02<00:00, 13.71it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:25:39,555 >> Saving model checkpoint to models/OneShot/1/checkpoint-2568\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:39,556 >> Configuration saved in models/OneShot/1/checkpoint-2568/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:39,647 >> Module weights saved in models/OneShot/1/checkpoint-2568/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:39,648 >> Configuration saved in models/OneShot/1/checkpoint-2568/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:39,655 >> Module weights saved in models/OneShot/1/checkpoint-2568/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:39,656 >> Configuration saved in models/OneShot/1/checkpoint-2568/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:39,710 >> Module weights saved in models/OneShot/1/checkpoint-2568/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:39,711 >> Configuration saved in models/OneShot/1/checkpoint-2568/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:39,780 >> Module weights saved in models/OneShot/1/checkpoint-2568/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:39,781 >> Configuration saved in models/OneShot/1/checkpoint-2568/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:41,208 >> Module weights saved in models/OneShot/1/checkpoint-2568/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:41,280 >> Configuration saved in models/OneShot/1/checkpoint-2568/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:41,295 >> Module weights saved in models/OneShot/1/checkpoint-2568/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:25:41,295 >> Configuration saved in models/OneShot/1/checkpoint-2568/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:25:42,740 >> Module weights saved in models/OneShot/1/checkpoint-2568/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:25:42,741 >> tokenizer config file saved in models/OneShot/1/checkpoint-2568/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:25:42,741 >> Special tokens file saved in models/OneShot/1/checkpoint-2568/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:25:43,225 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2461] due to args.save_total_limit\n",
            "100% 2675/2675 [29:12<00:00,  1.82it/s][INFO|trainer.py:623] 2022-08-25 11:26:47,324 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:26:47,326 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:26:47,326 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:26:47,326 >>   Batch size = 8\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 3/35 [00:00<00:01, 17.78it/s]\u001b[A\n",
            " 14% 5/35 [00:00<00:02, 14.43it/s]\u001b[A\n",
            " 20% 7/35 [00:00<00:02, 13.34it/s]\u001b[A\n",
            " 26% 9/35 [00:00<00:01, 13.01it/s]\u001b[A\n",
            " 31% 11/35 [00:00<00:01, 12.66it/s]\u001b[A\n",
            " 37% 13/35 [00:00<00:01, 12.53it/s]\u001b[A\n",
            " 43% 15/35 [00:01<00:01, 12.40it/s]\u001b[A\n",
            " 49% 17/35 [00:01<00:01, 12.44it/s]\u001b[A\n",
            " 54% 19/35 [00:01<00:01, 12.43it/s]\u001b[A\n",
            " 60% 21/35 [00:01<00:01, 12.43it/s]\u001b[A\n",
            " 66% 23/35 [00:01<00:00, 12.37it/s]\u001b[A\n",
            " 71% 25/35 [00:01<00:00, 12.31it/s]\u001b[A\n",
            " 77% 27/35 [00:02<00:00, 12.30it/s]\u001b[A\n",
            " 83% 29/35 [00:02<00:00, 12.32it/s]\u001b[A\n",
            " 89% 31/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            " 94% 33/35 [00:02<00:00, 12.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.931305408477783, 'eval_accuracy': 0.692307710647583, 'eval_f1': 0.675, 'eval_runtime': 2.7942, 'eval_samples_per_second': 97.701, 'eval_steps_per_second': 12.526, 'epoch': 25.0}\n",
            "100% 2675/2675 [29:15<00:00,  1.82it/s]\n",
            "100% 35/35 [00:02<00:00, 13.81it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 11:26:50,122 >> Saving model checkpoint to models/OneShot/1/checkpoint-2675\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:50,123 >> Configuration saved in models/OneShot/1/checkpoint-2675/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:50,216 >> Module weights saved in models/OneShot/1/checkpoint-2675/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:50,216 >> Configuration saved in models/OneShot/1/checkpoint-2675/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:50,225 >> Module weights saved in models/OneShot/1/checkpoint-2675/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:50,225 >> Configuration saved in models/OneShot/1/checkpoint-2675/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:50,278 >> Module weights saved in models/OneShot/1/checkpoint-2675/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:50,279 >> Configuration saved in models/OneShot/1/checkpoint-2675/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:50,354 >> Module weights saved in models/OneShot/1/checkpoint-2675/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:50,355 >> Configuration saved in models/OneShot/1/checkpoint-2675/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:51,799 >> Module weights saved in models/OneShot/1/checkpoint-2675/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:51,915 >> Configuration saved in models/OneShot/1/checkpoint-2675/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:51,942 >> Module weights saved in models/OneShot/1/checkpoint-2675/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:51,943 >> Configuration saved in models/OneShot/1/checkpoint-2675/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:53,371 >> Module weights saved in models/OneShot/1/checkpoint-2675/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:26:53,372 >> tokenizer config file saved in models/OneShot/1/checkpoint-2675/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:26:53,372 >> Special tokens file saved in models/OneShot/1/checkpoint-2675/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 11:26:53,867 >> Deleting older checkpoint [models/OneShot/1/checkpoint-2568] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 11:26:53,927 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 11:26:53,927 >> Loading best model from models/OneShot/1/checkpoint-2033 (score: 0.6938317757009346).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 11:26:53,927 >> Could not locate the best model at models/OneShot/1/checkpoint-2033/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 1759.1593, 'train_samples_per_second': 48.517, 'train_steps_per_second': 1.521, 'train_loss': 0.03210307888695361, 'epoch': 25.0}\n",
            "100% 2675/2675 [29:19<00:00,  1.82it/s][INFO|trainer.py:238] 2022-08-25 11:26:53,929 >> Loading best adapter(s) from models/OneShot/1/checkpoint-2033 (score: 0.6938317757009346).\n",
            "[INFO|loading.py:77] 2022-08-25 11:26:53,930 >> Loading module configuration from models/OneShot/1/checkpoint-2033/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 11:26:53,931 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 11:26:54,594 >> Loading module weights from models/OneShot/1/checkpoint-2033/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 11:26:54,617 >> Loading module configuration from models/OneShot/1/checkpoint-2033/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 11:26:54,617 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 11:26:54,626 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 11:26:54,629 >> Loading module weights from models/OneShot/1/checkpoint-2033/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 11:26:54,632 >> Loading module configuration from models/OneShot/1/checkpoint-2033/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 11:26:54,633 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 11:26:54,787 >> Loading module weights from models/OneShot/1/checkpoint-2033/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 11:26:54,805 >> No matching prediction head found in 'models/OneShot/1/checkpoint-2033/en'\n",
            "[INFO|loading.py:77] 2022-08-25 11:26:54,806 >> Loading module configuration from models/OneShot/1/checkpoint-2033/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 11:26:54,807 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 11:26:54,966 >> Loading module weights from models/OneShot/1/checkpoint-2033/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 11:26:54,983 >> Loading module configuration from models/OneShot/1/checkpoint-2033/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 11:26:54,984 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 11:26:56,168 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 11:26:56,500 >> Loading module weights from models/OneShot/1/checkpoint-2033/pt/pytorch_model_head.bin\n",
            "100% 2675/2675 [29:21<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 11:26:56,592 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:56,593 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:56,743 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:56,743 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:56,755 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:56,756 >> Configuration saved in models/OneShot/1/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:56,809 >> Module weights saved in models/OneShot/1/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:56,810 >> Configuration saved in models/OneShot/1/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:56,932 >> Module weights saved in models/OneShot/1/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:56,934 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:58,353 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:58,480 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:58,502 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 11:26:58,503 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 11:26:59,912 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 11:26:59,913 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 11:26:59,913 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =     0.0321\n",
            "  train_runtime            = 0:29:19.15\n",
            "  train_samples            =       3414\n",
            "  train_samples_per_second =     48.517\n",
            "  train_steps_per_second   =      1.521\n",
            "08/25/2022 11:27:00 - INFO - __main__ - *** Evaluate ***\n",
            "\n",
            "\n",
            "Changing the language adapter to PT during evaluation..\n",
            "\n",
            "\n",
            "[INFO|trainer.py:623] 2022-08-25 11:27:00,157 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 11:27:00,159 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 11:27:00,159 >>   Num examples = 273\n",
            "[INFO|trainer.py:2595] 2022-08-25 11:27:00,159 >>   Batch size = 8\n",
            "100% 35/35 [00:02<00:00, 12.91it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =      0.652\n",
            "  eval_f1                 =     0.6436\n",
            "  eval_loss               =     2.8365\n",
            "  eval_runtime            = 0:00:02.82\n",
            "  eval_samples            =        273\n",
            "  eval_samples_per_second =     96.692\n",
            "  eval_steps_per_second   =     12.396\n",
            "[INFO|modelcard.py:460] 2022-08-25 11:27:03,022 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6520146727561951}, {'name': 'F1', 'type': 'f1', 'value': 0.6435786435786437}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task+Lang adapter-based BERT for one-shot PT-EN idiomatic knowledge transfer"
      ],
      "metadata": {
        "id": "kr9c5EdAFNR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer from Portueguese to English\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro_adapters.py \\\n",
        "  --model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 25 \\\n",
        "  --evaluation_strategy \"epoch\" \\\n",
        "  --output_dir models/OneShot/1/ \\\n",
        "  --seed 1 \\\n",
        "  --train_file      Data/OneShot/PT/train.csv \\\n",
        "  --validation_file Data/OneShot/EN/dev.csv \\\n",
        "  --overwrite_output_dir \\\n",
        "\t--save_strategy \"epoch\"  \\\n",
        "\t--load_best_model_at_end \\\n",
        "\t--metric_for_best_model \"f1\" \\\n",
        "\t--save_total_limit 1 \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfplaDr241DS",
        "outputId": "654dde90-5a23-4270-f642-595cc4814367"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/25/2022 10:39:01 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/25/2022 10:39:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Aug25_10-39-01_9b17356d7e44,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=25.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "08/25/2022 10:39:01 - INFO - __main__ - load a local file for train: Data/OneShot/PT/train.csv\n",
            "08/25/2022 10:39:01 - INFO - __main__ - load a local file for validation: Data/OneShot/EN/dev.csv\n",
            "08/25/2022 10:39:01 - WARNING - datasets.builder - Using custom data configuration default-5b6adca5a6fbecff\n",
            "08/25/2022 10:39:01 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-5b6adca5a6fbecff/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-5b6adca5a6fbecff/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 10394.81it/s]\n",
            "08/25/2022 10:39:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "08/25/2022 10:39:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1327.31it/s]\n",
            "08/25/2022 10:39:01 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/25/2022 10:39:01 - INFO - datasets.builder - Generating train split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 10:39:01 - INFO - datasets.builder - Generating validation split\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r08/25/2022 10:39:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-5b6adca5a6fbecff/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 1027.76it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:39:01,315 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:39:01,316 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:39:01,364 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:39:01,364 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:39:01,504 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:39:01,504 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:39:01,504 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:39:01,504 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-08-25 10:39:01,505 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:659] 2022-08-25 10:39:01,541 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:708] 2022-08-25 10:39:01,541 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1953] 2022-08-25 10:39:01,734 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:2255] 2022-08-25 10:39:06,489 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertAdapterModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:2272] 2022-08-25 10:39:06,490 >> All the weights of BertAdapterModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertAdapterModel for predictions without further training.\n",
            "[INFO|base.py:688] 2022-08-25 10:39:06,500 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {0: 0, 1: 1}, 'use_pooler': False, 'bias': True}.\n",
            "\n",
            "\n",
            "Adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=1, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|configuration.py:657] 2022-08-25 10:39:06,532 >> Adding adapter 'glue'.\n",
            "\n",
            "\n",
            "Language adapter config :  PfeifferConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity='relu', original_ln_before=True, original_ln_after=True, ln_before=False, ln_after=False, init_weights='bert', is_parallel=False, scaling=1.0, residual_before_ln=True, adapter_residual_before_ln=False, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[], phm_layer=False, phm_dim=4, factorized_phm_W=True, shared_W_phm=False, shared_phm_rule=True, factorized_phm_rule=False, phm_c_init='normal', phm_init_range=0.0001, learn_phm=True, hypercomplex_nonlinearity='glorot-uniform', phm_rank=1, phm_bias=True)\n",
            "\n",
            "\n",
            "[INFO|utils.py:487] 2022-08-25 10:39:06,723 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:327] 2022-08-25 10:39:06,818 >> Found matching adapter at: adapters/ukp/bert-base-multilingual-cased-en-wiki_pfeiffer_relu.json\n",
            "[INFO|utils.py:412] 2022-08-25 10:39:06,903 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 10:39:07,591 >> Loading module configuration from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 10:39:07,593 >> Adding adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:39:07,826 >> Loading module weights from ~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 10:39:07,838 >> No matching prediction head found in '~/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'\n",
            "[INFO|utils.py:487] 2022-08-25 10:39:07,838 >> Attempting to load adapter from source 'ah'...\n",
            "[INFO|utils.py:332] 2022-08-25 10:39:07,857 >> No exactly matching adapter config found for this specifier, falling back to default.\n",
            "[INFO|utils.py:412] 2022-08-25 10:39:07,942 >> Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/pt/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_pt_pt_pfeiffer.zip.\n",
            "[INFO|loading.py:77] 2022-08-25 10:39:10,515 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/adapter_config.json\n",
            "[INFO|configuration.py:657] 2022-08-25 10:39:10,516 >> Adding adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:39:10,777 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:39:10,797 >> Loading module configuration from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/head_config.json\n",
            "[WARNING|head_utils.py:418] 2022-08-25 10:39:10,798 >> No valid map of labels in label2id. Falling back to default (num_labels=2). This may cause errors during loading!\n",
            "[INFO|base.py:688] 2022-08-25 10:39:12,005 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:39:13,598 >> Loading module weights from ~/.cache/torch/adapters/4924e01fe99a0caebf1981f06377a5104fb3796cf7ec3b246e6315cfbefd695e-babbbc708158370b57817d59165808788458aebba22ae543528550fc8eeb099c-extracted/pytorch_model_head.bin\n",
            "[INFO|loading.py:171] 2022-08-25 10:39:13,666 >> Some weights of the state_dict could not be loaded into model: cls.predictions.bias\n",
            "\n",
            "\n",
            "Loaded EN and PT language adapters..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Initially setting up training with PT language adapter..\n",
            "\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]08/25/2022 10:39:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-5b6adca5a6fbecff/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b37df32e56d99906.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 12.20ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]08/25/2022 10:39:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-5b6adca5a6fbecff/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-6c2a52d74d8cfa72.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 18.46ba/s]\n",
            "08/25/2022 10:39:14 - INFO - __main__ - Sample 275 of the training set: {'label': 0, 'sentence1': 'Já o Boeing 787-9 que a Lufthansa encomendou cerca de 20 aeronaves tem programada entrega para 2022, coincidindo com a previsão da Lufthansa em lançar a nova classe executiva.', 'sentence2': 'classe executiva', 'input_ids': [101, 48625, 183, 20172, 53172, 118, 130, 10121, 169, 101529, 10110, 61643, 10605, 10138, 13698, 10104, 10197, 82895, 12900, 13815, 10229, 47937, 10220, 75632, 117, 71770, 43275, 26104, 10212, 169, 12229, 100167, 10143, 101529, 10266, 110543, 10129, 169, 15656, 15702, 11419, 97038, 17403, 119, 102, 15702, 11419, 97038, 17403, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 10:39:14 - INFO - __main__ - Sample 1165 of the training set: {'label': 0, 'sentence1': 'Assim, Bibi continuará ao lado marido trambiqueiro, sem saber que seu verdadeiro príncipe encantado é o advogado.', 'sentence2': 'príncipe encantado', 'input_ids': [101, 43479, 117, 31156, 11645, 36055, 10661, 10610, 15776, 40407, 51127, 101349, 12772, 14213, 117, 11531, 33335, 10121, 10617, 58695, 14213, 32039, 10110, 62745, 11272, 263, 183, 10840, 95926, 11272, 119, 102, 32039, 10110, 62745, 11272, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/25/2022 10:39:14 - INFO - __main__ - Sample 129 of the training set: {'label': 0, 'sentence1': 'O amigo secreto parece nunca sair de moda entre os brasileiros.', 'sentence2': 'amigo secreto', 'input_ids': [101, 152, 27989, 65234, 30395, 19096, 13410, 10129, 10104, 38231, 10402, 10427, 88442, 119, 102, 27989, 65234, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:623] 2022-08-25 10:39:19,180 >> The following columns in the training set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1419] 2022-08-25 10:39:19,194 >> ***** Running training *****\n",
            "[INFO|trainer.py:1420] 2022-08-25 10:39:19,194 >>   Num examples = 1217\n",
            "[INFO|trainer.py:1421] 2022-08-25 10:39:19,194 >>   Num Epochs = 25\n",
            "[INFO|trainer.py:1422] 2022-08-25 10:39:19,194 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1423] 2022-08-25 10:39:19,194 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1424] 2022-08-25 10:39:19,194 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1425] 2022-08-25 10:39:19,194 >>   Total optimization steps = 975\n",
            "  4% 38/975 [00:23<09:39,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:39:42,946 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:39:42,947 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:39:42,948 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:39:42,948 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.95it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.98it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.82it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.30it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.71it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.82it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.57it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.60it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.61it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.69it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.83it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.63it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.59it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.69it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.75it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.73it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.75it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.61it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.60it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.59it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.69it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.70it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.63it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8467865586280823, 'eval_accuracy': 0.5600858330726624, 'eval_f1': 0.5508000771158665, 'eval_runtime': 4.9935, 'eval_samples_per_second': 93.321, 'eval_steps_per_second': 11.815, 'epoch': 1.0}\n",
            "  4% 39/975 [00:28<09:38,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 12.87it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:39:47,942 >> Saving model checkpoint to models/OneShot/1/checkpoint-39\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:47,943 >> Configuration saved in models/OneShot/1/checkpoint-39/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:48,092 >> Module weights saved in models/OneShot/1/checkpoint-39/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:48,093 >> Configuration saved in models/OneShot/1/checkpoint-39/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:48,103 >> Module weights saved in models/OneShot/1/checkpoint-39/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:48,104 >> Configuration saved in models/OneShot/1/checkpoint-39/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:48,215 >> Module weights saved in models/OneShot/1/checkpoint-39/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:48,216 >> Configuration saved in models/OneShot/1/checkpoint-39/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:48,340 >> Module weights saved in models/OneShot/1/checkpoint-39/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:48,341 >> Configuration saved in models/OneShot/1/checkpoint-39/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:49,669 >> Module weights saved in models/OneShot/1/checkpoint-39/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:49,670 >> Configuration saved in models/OneShot/1/checkpoint-39/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:49,682 >> Module weights saved in models/OneShot/1/checkpoint-39/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:39:49,683 >> Configuration saved in models/OneShot/1/checkpoint-39/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:39:51,207 >> Module weights saved in models/OneShot/1/checkpoint-39/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:39:51,207 >> tokenizer config file saved in models/OneShot/1/checkpoint-39/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:39:51,207 >> Special tokens file saved in models/OneShot/1/checkpoint-39/special_tokens_map.json\n",
            "  8% 77/975 [00:56<09:18,  1.61it/s][INFO|trainer.py:623] 2022-08-25 10:40:15,563 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:40:15,564 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:40:15,564 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:40:15,564 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.73it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.20it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.27it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.99it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.10it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.79it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.74it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.08it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.74it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.84it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.85it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8140078186988831, 'eval_accuracy': 0.5708154439926147, 'eval_f1': 0.5700234364908008, 'eval_runtime': 4.9119, 'eval_samples_per_second': 94.872, 'eval_steps_per_second': 12.012, 'epoch': 2.0}\n",
            "  8% 78/975 [01:01<09:17,  1.61it/s]\n",
            "100% 59/59 [00:04<00:00, 13.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:40:20,478 >> Saving model checkpoint to models/OneShot/1/checkpoint-78\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:20,478 >> Configuration saved in models/OneShot/1/checkpoint-78/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:20,570 >> Module weights saved in models/OneShot/1/checkpoint-78/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:20,570 >> Configuration saved in models/OneShot/1/checkpoint-78/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:20,578 >> Module weights saved in models/OneShot/1/checkpoint-78/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:20,579 >> Configuration saved in models/OneShot/1/checkpoint-78/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:20,634 >> Module weights saved in models/OneShot/1/checkpoint-78/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:20,634 >> Configuration saved in models/OneShot/1/checkpoint-78/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:20,709 >> Module weights saved in models/OneShot/1/checkpoint-78/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:20,710 >> Configuration saved in models/OneShot/1/checkpoint-78/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:22,036 >> Module weights saved in models/OneShot/1/checkpoint-78/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:22,139 >> Configuration saved in models/OneShot/1/checkpoint-78/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:22,152 >> Module weights saved in models/OneShot/1/checkpoint-78/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:22,153 >> Configuration saved in models/OneShot/1/checkpoint-78/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:23,563 >> Module weights saved in models/OneShot/1/checkpoint-78/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:40:23,564 >> tokenizer config file saved in models/OneShot/1/checkpoint-78/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:40:23,564 >> Special tokens file saved in models/OneShot/1/checkpoint-78/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:40:24,064 >> Deleting older checkpoint [models/OneShot/1/checkpoint-39] due to args.save_total_limit\n",
            " 12% 116/975 [01:28<08:44,  1.64it/s][INFO|trainer.py:623] 2022-08-25 10:40:47,278 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:40:47,280 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:40:47,280 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:40:47,280 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.77it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.50it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.34it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.68it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.25it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.23it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.24it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.28it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.26it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.03it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.02it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.12it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.15it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.03it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.09it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.18it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.14it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.20it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.23it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.24it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.30it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.31it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.31it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.429753065109253, 'eval_accuracy': 0.6008583903312683, 'eval_f1': 0.5999667700487372, 'eval_runtime': 4.812, 'eval_samples_per_second': 96.842, 'eval_steps_per_second': 12.261, 'epoch': 3.0}\n",
            " 12% 117/975 [01:32<08:43,  1.64it/s]\n",
            "100% 59/59 [00:04<00:00, 13.68it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:40:52,093 >> Saving model checkpoint to models/OneShot/1/checkpoint-117\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:52,094 >> Configuration saved in models/OneShot/1/checkpoint-117/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:52,184 >> Module weights saved in models/OneShot/1/checkpoint-117/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:52,184 >> Configuration saved in models/OneShot/1/checkpoint-117/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:52,192 >> Module weights saved in models/OneShot/1/checkpoint-117/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:52,193 >> Configuration saved in models/OneShot/1/checkpoint-117/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:52,245 >> Module weights saved in models/OneShot/1/checkpoint-117/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:52,246 >> Configuration saved in models/OneShot/1/checkpoint-117/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:52,302 >> Module weights saved in models/OneShot/1/checkpoint-117/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:52,302 >> Configuration saved in models/OneShot/1/checkpoint-117/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:53,637 >> Module weights saved in models/OneShot/1/checkpoint-117/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:53,743 >> Configuration saved in models/OneShot/1/checkpoint-117/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:53,763 >> Module weights saved in models/OneShot/1/checkpoint-117/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:40:53,764 >> Configuration saved in models/OneShot/1/checkpoint-117/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:40:55,003 >> Module weights saved in models/OneShot/1/checkpoint-117/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:40:55,004 >> tokenizer config file saved in models/OneShot/1/checkpoint-117/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:40:55,004 >> Special tokens file saved in models/OneShot/1/checkpoint-117/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:40:55,495 >> Deleting older checkpoint [models/OneShot/1/checkpoint-78] due to args.save_total_limit\n",
            " 16% 155/975 [01:59<08:28,  1.61it/s][INFO|trainer.py:623] 2022-08-25 10:41:18,977 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:41:18,978 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:41:18,978 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:41:18,979 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.96it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.95it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.97it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.38it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.92it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.87it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.69it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.70it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.81it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.96it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.80it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.78it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.68it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.78it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.78it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.67it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.63it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.66it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.85it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.2011497020721436, 'eval_accuracy': 0.6330472230911255, 'eval_f1': 0.6318111880680322, 'eval_runtime': 4.9568, 'eval_samples_per_second': 94.013, 'eval_steps_per_second': 11.903, 'epoch': 4.0}\n",
            " 16% 156/975 [02:04<08:28,  1.61it/s]\n",
            "100% 59/59 [00:04<00:00, 13.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:41:23,937 >> Saving model checkpoint to models/OneShot/1/checkpoint-156\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:23,937 >> Configuration saved in models/OneShot/1/checkpoint-156/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:24,029 >> Module weights saved in models/OneShot/1/checkpoint-156/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:24,029 >> Configuration saved in models/OneShot/1/checkpoint-156/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:24,038 >> Module weights saved in models/OneShot/1/checkpoint-156/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:24,038 >> Configuration saved in models/OneShot/1/checkpoint-156/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:24,099 >> Module weights saved in models/OneShot/1/checkpoint-156/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:24,100 >> Configuration saved in models/OneShot/1/checkpoint-156/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:24,158 >> Module weights saved in models/OneShot/1/checkpoint-156/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:24,159 >> Configuration saved in models/OneShot/1/checkpoint-156/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:25,477 >> Module weights saved in models/OneShot/1/checkpoint-156/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:25,623 >> Configuration saved in models/OneShot/1/checkpoint-156/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:25,644 >> Module weights saved in models/OneShot/1/checkpoint-156/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:25,644 >> Configuration saved in models/OneShot/1/checkpoint-156/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:27,663 >> Module weights saved in models/OneShot/1/checkpoint-156/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:41:27,664 >> tokenizer config file saved in models/OneShot/1/checkpoint-156/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:41:27,664 >> Special tokens file saved in models/OneShot/1/checkpoint-156/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:41:28,149 >> Deleting older checkpoint [models/OneShot/1/checkpoint-117] due to args.save_total_limit\n",
            " 20% 194/975 [02:32<08:01,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:41:51,609 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:41:51,611 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:41:51,611 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:41:51,611 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.53it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.44it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.47it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.71it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.08it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.17it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.88it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.08it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.04it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.81it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.75it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.82it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.06it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.15it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.15it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 12.11it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.08it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.525639533996582, 'eval_accuracy': 0.5665236115455627, 'eval_f1': 0.5473117245359238, 'eval_runtime': 4.8605, 'eval_samples_per_second': 95.875, 'eval_steps_per_second': 12.139, 'epoch': 5.0}\n",
            " 20% 195/975 [02:37<08:00,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.51it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:41:56,473 >> Saving model checkpoint to models/OneShot/1/checkpoint-195\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:56,474 >> Configuration saved in models/OneShot/1/checkpoint-195/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:56,565 >> Module weights saved in models/OneShot/1/checkpoint-195/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:56,565 >> Configuration saved in models/OneShot/1/checkpoint-195/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:56,573 >> Module weights saved in models/OneShot/1/checkpoint-195/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:56,574 >> Configuration saved in models/OneShot/1/checkpoint-195/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:56,627 >> Module weights saved in models/OneShot/1/checkpoint-195/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:56,628 >> Configuration saved in models/OneShot/1/checkpoint-195/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:56,686 >> Module weights saved in models/OneShot/1/checkpoint-195/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:56,687 >> Configuration saved in models/OneShot/1/checkpoint-195/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:57,855 >> Module weights saved in models/OneShot/1/checkpoint-195/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:57,973 >> Configuration saved in models/OneShot/1/checkpoint-195/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:57,987 >> Module weights saved in models/OneShot/1/checkpoint-195/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:41:57,987 >> Configuration saved in models/OneShot/1/checkpoint-195/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:41:59,462 >> Module weights saved in models/OneShot/1/checkpoint-195/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:41:59,462 >> tokenizer config file saved in models/OneShot/1/checkpoint-195/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:41:59,462 >> Special tokens file saved in models/OneShot/1/checkpoint-195/special_tokens_map.json\n",
            " 24% 233/975 [03:03<07:34,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:42:23,199 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:42:23,201 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:42:23,201 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:42:23,201 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.41it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.90it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.98it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.53it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.94it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.97it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.85it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.06it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.09it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.10it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.96it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.97it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 12.05it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.99it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.883284568786621, 'eval_accuracy': 0.5944206118583679, 'eval_f1': 0.5847818400396012, 'eval_runtime': 4.8949, 'eval_samples_per_second': 95.201, 'eval_steps_per_second': 12.053, 'epoch': 6.0}\n",
            " 24% 234/975 [03:08<07:33,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.22it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:42:28,097 >> Saving model checkpoint to models/OneShot/1/checkpoint-234\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:28,098 >> Configuration saved in models/OneShot/1/checkpoint-234/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:28,195 >> Module weights saved in models/OneShot/1/checkpoint-234/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:28,195 >> Configuration saved in models/OneShot/1/checkpoint-234/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:28,203 >> Module weights saved in models/OneShot/1/checkpoint-234/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:28,204 >> Configuration saved in models/OneShot/1/checkpoint-234/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:28,257 >> Module weights saved in models/OneShot/1/checkpoint-234/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:28,258 >> Configuration saved in models/OneShot/1/checkpoint-234/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:28,316 >> Module weights saved in models/OneShot/1/checkpoint-234/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:28,317 >> Configuration saved in models/OneShot/1/checkpoint-234/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:29,650 >> Module weights saved in models/OneShot/1/checkpoint-234/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:29,802 >> Configuration saved in models/OneShot/1/checkpoint-234/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:29,825 >> Module weights saved in models/OneShot/1/checkpoint-234/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:42:29,825 >> Configuration saved in models/OneShot/1/checkpoint-234/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:42:31,227 >> Module weights saved in models/OneShot/1/checkpoint-234/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:42:31,228 >> tokenizer config file saved in models/OneShot/1/checkpoint-234/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:42:31,228 >> Special tokens file saved in models/OneShot/1/checkpoint-234/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:42:31,726 >> Deleting older checkpoint [models/OneShot/1/checkpoint-195] due to args.save_total_limit\n",
            " 28% 272/975 [03:35<07:15,  1.61it/s][INFO|trainer.py:623] 2022-08-25 10:42:55,202 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:42:55,203 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:42:55,204 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:42:55,204 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.55it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.08it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.15it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.44it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.98it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.81it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.82it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.76it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.81it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.95it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.78it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.72it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.69it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.75it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.72it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.8299291133880615, 'eval_accuracy': 0.5300429463386536, 'eval_f1': 0.5017745990675422, 'eval_runtime': 4.9391, 'eval_samples_per_second': 94.349, 'eval_steps_per_second': 11.945, 'epoch': 7.0}\n",
            " 28% 273/975 [03:40<07:14,  1.61it/s]\n",
            "100% 59/59 [00:04<00:00, 13.00it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:43:00,144 >> Saving model checkpoint to models/OneShot/1/checkpoint-273\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:00,145 >> Configuration saved in models/OneShot/1/checkpoint-273/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:00,240 >> Module weights saved in models/OneShot/1/checkpoint-273/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:00,241 >> Configuration saved in models/OneShot/1/checkpoint-273/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:00,248 >> Module weights saved in models/OneShot/1/checkpoint-273/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:00,249 >> Configuration saved in models/OneShot/1/checkpoint-273/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:00,303 >> Module weights saved in models/OneShot/1/checkpoint-273/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:00,304 >> Configuration saved in models/OneShot/1/checkpoint-273/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:00,362 >> Module weights saved in models/OneShot/1/checkpoint-273/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:00,362 >> Configuration saved in models/OneShot/1/checkpoint-273/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:01,754 >> Module weights saved in models/OneShot/1/checkpoint-273/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:01,919 >> Configuration saved in models/OneShot/1/checkpoint-273/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:01,950 >> Module weights saved in models/OneShot/1/checkpoint-273/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:01,951 >> Configuration saved in models/OneShot/1/checkpoint-273/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:03,339 >> Module weights saved in models/OneShot/1/checkpoint-273/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:43:03,340 >> tokenizer config file saved in models/OneShot/1/checkpoint-273/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:43:03,340 >> Special tokens file saved in models/OneShot/1/checkpoint-273/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:43:03,830 >> Deleting older checkpoint [models/OneShot/1/checkpoint-234] due to args.save_total_limit\n",
            " 32% 311/975 [04:08<06:47,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:43:27,335 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:43:27,337 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:43:27,337 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:43:27,337 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.46it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.19it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.09it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.41it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.96it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.15it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.83it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.01it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.09it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.16it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.16it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.00it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.06it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.09it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.84it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.06it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.215245246887207, 'eval_accuracy': 0.5364806652069092, 'eval_f1': 0.5141053119388288, 'eval_runtime': 4.8784, 'eval_samples_per_second': 95.523, 'eval_steps_per_second': 12.094, 'epoch': 8.0}\n",
            " 32% 312/975 [04:13<06:46,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:43:32,216 >> Saving model checkpoint to models/OneShot/1/checkpoint-312\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:32,217 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:32,307 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:32,307 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:32,315 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:32,315 >> Configuration saved in models/OneShot/1/checkpoint-312/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:32,369 >> Module weights saved in models/OneShot/1/checkpoint-312/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:32,370 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:32,446 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:32,447 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:33,828 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:33,969 >> Configuration saved in models/OneShot/1/checkpoint-312/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:33,996 >> Module weights saved in models/OneShot/1/checkpoint-312/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:43:33,997 >> Configuration saved in models/OneShot/1/checkpoint-312/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:43:35,453 >> Module weights saved in models/OneShot/1/checkpoint-312/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:43:35,454 >> tokenizer config file saved in models/OneShot/1/checkpoint-312/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:43:35,454 >> Special tokens file saved in models/OneShot/1/checkpoint-312/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:43:35,947 >> Deleting older checkpoint [models/OneShot/1/checkpoint-273] due to args.save_total_limit\n",
            " 36% 350/975 [04:40<06:24,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:43:59,298 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:43:59,300 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:43:59,300 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:43:59,300 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.34it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.21it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.21it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.53it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.90it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.96it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.73it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.04it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.10it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.17it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.19it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.07it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.08it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.08it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.97it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.85it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.683738946914673, 'eval_accuracy': 0.5879828333854675, 'eval_f1': 0.5823749066467513, 'eval_runtime': 4.8791, 'eval_samples_per_second': 95.51, 'eval_steps_per_second': 12.092, 'epoch': 9.0}\n",
            " 36% 351/975 [04:44<06:23,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:44:04,186 >> Saving model checkpoint to models/OneShot/1/checkpoint-351\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:04,186 >> Configuration saved in models/OneShot/1/checkpoint-351/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:04,281 >> Module weights saved in models/OneShot/1/checkpoint-351/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:04,282 >> Configuration saved in models/OneShot/1/checkpoint-351/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:04,292 >> Module weights saved in models/OneShot/1/checkpoint-351/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:04,293 >> Configuration saved in models/OneShot/1/checkpoint-351/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:04,348 >> Module weights saved in models/OneShot/1/checkpoint-351/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:04,348 >> Configuration saved in models/OneShot/1/checkpoint-351/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:04,416 >> Module weights saved in models/OneShot/1/checkpoint-351/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:04,416 >> Configuration saved in models/OneShot/1/checkpoint-351/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:05,860 >> Module weights saved in models/OneShot/1/checkpoint-351/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:06,003 >> Configuration saved in models/OneShot/1/checkpoint-351/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:06,031 >> Module weights saved in models/OneShot/1/checkpoint-351/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:06,032 >> Configuration saved in models/OneShot/1/checkpoint-351/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:07,462 >> Module weights saved in models/OneShot/1/checkpoint-351/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:44:07,463 >> tokenizer config file saved in models/OneShot/1/checkpoint-351/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:44:07,463 >> Special tokens file saved in models/OneShot/1/checkpoint-351/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:44:07,935 >> Deleting older checkpoint [models/OneShot/1/checkpoint-312] due to args.save_total_limit\n",
            " 40% 389/975 [05:12<06:01,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:44:31,413 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:44:31,415 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:44:31,415 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:44:31,415 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.97it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.95it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.18it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.74it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.16it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.17it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.25it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.27it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.23it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.16it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.87it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.12it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.12it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.78it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.83it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.11it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.04it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.91it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.88it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.7764225006103516, 'eval_accuracy': 0.6115880012512207, 'eval_f1': 0.6088482852969137, 'eval_runtime': 4.8802, 'eval_samples_per_second': 95.487, 'eval_steps_per_second': 12.09, 'epoch': 10.0}\n",
            " 40% 390/975 [05:17<06:00,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.26it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:44:36,296 >> Saving model checkpoint to models/OneShot/1/checkpoint-390\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:36,297 >> Configuration saved in models/OneShot/1/checkpoint-390/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:36,396 >> Module weights saved in models/OneShot/1/checkpoint-390/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:36,396 >> Configuration saved in models/OneShot/1/checkpoint-390/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:36,405 >> Module weights saved in models/OneShot/1/checkpoint-390/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:36,405 >> Configuration saved in models/OneShot/1/checkpoint-390/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:36,458 >> Module weights saved in models/OneShot/1/checkpoint-390/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:36,459 >> Configuration saved in models/OneShot/1/checkpoint-390/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:36,531 >> Module weights saved in models/OneShot/1/checkpoint-390/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:36,531 >> Configuration saved in models/OneShot/1/checkpoint-390/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:37,949 >> Module weights saved in models/OneShot/1/checkpoint-390/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:37,950 >> Configuration saved in models/OneShot/1/checkpoint-390/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:37,962 >> Module weights saved in models/OneShot/1/checkpoint-390/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:44:37,962 >> Configuration saved in models/OneShot/1/checkpoint-390/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:44:39,506 >> Module weights saved in models/OneShot/1/checkpoint-390/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:44:39,506 >> tokenizer config file saved in models/OneShot/1/checkpoint-390/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:44:39,506 >> Special tokens file saved in models/OneShot/1/checkpoint-390/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:44:40,000 >> Deleting older checkpoint [models/OneShot/1/checkpoint-351] due to args.save_total_limit\n",
            " 44% 428/975 [05:44<05:34,  1.64it/s][INFO|trainer.py:623] 2022-08-25 10:45:03,432 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:45:03,434 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:45:03,434 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:45:03,434 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.97it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.06it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.14it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.57it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.99it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.88it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.05it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.07it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.11it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.16it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.13it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 12.00it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.96it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.02it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.07it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.4007647037506104, 'eval_accuracy': 0.5793991684913635, 'eval_f1': 0.5679061405998675, 'eval_runtime': 4.8725, 'eval_samples_per_second': 95.638, 'eval_steps_per_second': 12.109, 'epoch': 11.0}\n",
            " 44% 429/975 [05:49<05:33,  1.64it/s]\n",
            "100% 59/59 [00:04<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:45:08,308 >> Saving model checkpoint to models/OneShot/1/checkpoint-429\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:08,309 >> Configuration saved in models/OneShot/1/checkpoint-429/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:08,401 >> Module weights saved in models/OneShot/1/checkpoint-429/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:08,402 >> Configuration saved in models/OneShot/1/checkpoint-429/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:08,411 >> Module weights saved in models/OneShot/1/checkpoint-429/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:08,412 >> Configuration saved in models/OneShot/1/checkpoint-429/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:08,471 >> Module weights saved in models/OneShot/1/checkpoint-429/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:08,472 >> Configuration saved in models/OneShot/1/checkpoint-429/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:08,538 >> Module weights saved in models/OneShot/1/checkpoint-429/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:08,539 >> Configuration saved in models/OneShot/1/checkpoint-429/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:09,976 >> Module weights saved in models/OneShot/1/checkpoint-429/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:09,977 >> Configuration saved in models/OneShot/1/checkpoint-429/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:09,992 >> Module weights saved in models/OneShot/1/checkpoint-429/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:09,993 >> Configuration saved in models/OneShot/1/checkpoint-429/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:11,539 >> Module weights saved in models/OneShot/1/checkpoint-429/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:45:11,540 >> tokenizer config file saved in models/OneShot/1/checkpoint-429/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:45:11,540 >> Special tokens file saved in models/OneShot/1/checkpoint-429/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:45:12,037 >> Deleting older checkpoint [models/OneShot/1/checkpoint-390] due to args.save_total_limit\n",
            " 48% 467/975 [06:16<05:12,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:45:35,509 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:45:35,510 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:45:35,510 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:45:35,510 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.94it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.16it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.24it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.66it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.05it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.95it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.84it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.96it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.84it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.83it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.99it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.78it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.84it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.07it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.6898739337921143, 'eval_accuracy': 0.557939887046814, 'eval_f1': 0.541592649895899, 'eval_runtime': 4.8949, 'eval_samples_per_second': 95.202, 'eval_steps_per_second': 12.053, 'epoch': 12.0}\n",
            " 48% 468/975 [06:21<05:12,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.48it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:45:40,407 >> Saving model checkpoint to models/OneShot/1/checkpoint-468\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:40,407 >> Configuration saved in models/OneShot/1/checkpoint-468/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:40,498 >> Module weights saved in models/OneShot/1/checkpoint-468/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:40,499 >> Configuration saved in models/OneShot/1/checkpoint-468/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:40,508 >> Module weights saved in models/OneShot/1/checkpoint-468/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:40,509 >> Configuration saved in models/OneShot/1/checkpoint-468/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:40,566 >> Module weights saved in models/OneShot/1/checkpoint-468/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:40,568 >> Configuration saved in models/OneShot/1/checkpoint-468/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:40,649 >> Module weights saved in models/OneShot/1/checkpoint-468/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:40,650 >> Configuration saved in models/OneShot/1/checkpoint-468/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:42,121 >> Module weights saved in models/OneShot/1/checkpoint-468/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:42,122 >> Configuration saved in models/OneShot/1/checkpoint-468/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:42,133 >> Module weights saved in models/OneShot/1/checkpoint-468/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:45:42,133 >> Configuration saved in models/OneShot/1/checkpoint-468/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:45:43,567 >> Module weights saved in models/OneShot/1/checkpoint-468/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:45:43,568 >> tokenizer config file saved in models/OneShot/1/checkpoint-468/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:45:43,568 >> Special tokens file saved in models/OneShot/1/checkpoint-468/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:45:44,068 >> Deleting older checkpoint [models/OneShot/1/checkpoint-429] due to args.save_total_limit\n",
            "{'loss': 0.112, 'learning_rate': 4.871794871794872e-05, 'epoch': 12.82}\n",
            " 52% 506/975 [06:48<04:49,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:46:07,524 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:46:07,526 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:46:07,526 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:46:07,526 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.87it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.12it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.95it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.43it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.75it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.89it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.97it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.05it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.02it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.74it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.82it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.08it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.12it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.15it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.84it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.88it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.81it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.96it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.05it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.07it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.813967227935791, 'eval_accuracy': 0.5515021681785583, 'eval_f1': 0.5328981760804195, 'eval_runtime': 4.8999, 'eval_samples_per_second': 95.103, 'eval_steps_per_second': 12.041, 'epoch': 13.0}\n",
            " 52% 507/975 [06:53<04:48,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.35it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:46:12,427 >> Saving model checkpoint to models/OneShot/1/checkpoint-507\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:12,428 >> Configuration saved in models/OneShot/1/checkpoint-507/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:12,523 >> Module weights saved in models/OneShot/1/checkpoint-507/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:12,523 >> Configuration saved in models/OneShot/1/checkpoint-507/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:12,535 >> Module weights saved in models/OneShot/1/checkpoint-507/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:12,536 >> Configuration saved in models/OneShot/1/checkpoint-507/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:12,595 >> Module weights saved in models/OneShot/1/checkpoint-507/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:12,595 >> Configuration saved in models/OneShot/1/checkpoint-507/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:12,669 >> Module weights saved in models/OneShot/1/checkpoint-507/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:12,669 >> Configuration saved in models/OneShot/1/checkpoint-507/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:14,156 >> Module weights saved in models/OneShot/1/checkpoint-507/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:14,253 >> Configuration saved in models/OneShot/1/checkpoint-507/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:14,278 >> Module weights saved in models/OneShot/1/checkpoint-507/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:14,278 >> Configuration saved in models/OneShot/1/checkpoint-507/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:15,684 >> Module weights saved in models/OneShot/1/checkpoint-507/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:46:15,684 >> tokenizer config file saved in models/OneShot/1/checkpoint-507/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:46:15,684 >> Special tokens file saved in models/OneShot/1/checkpoint-507/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:46:16,174 >> Deleting older checkpoint [models/OneShot/1/checkpoint-468] due to args.save_total_limit\n",
            " 56% 545/975 [07:20<04:26,  1.61it/s][INFO|trainer.py:623] 2022-08-25 10:46:39,654 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:46:39,656 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:46:39,656 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:46:39,656 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.86it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.10it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.09it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.69it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.98it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.98it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.90it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.74it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.76it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.79it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.82it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.73it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.76it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.72it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.82it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.77it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.75it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.1833558082580566, 'eval_accuracy': 0.6223176121711731, 'eval_f1': 0.6171769977595221, 'eval_runtime': 4.9501, 'eval_samples_per_second': 94.139, 'eval_steps_per_second': 11.919, 'epoch': 14.0}\n",
            " 56% 546/975 [07:25<04:26,  1.61it/s]\n",
            "100% 59/59 [00:04<00:00, 13.02it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:46:44,607 >> Saving model checkpoint to models/OneShot/1/checkpoint-546\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:44,608 >> Configuration saved in models/OneShot/1/checkpoint-546/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:44,698 >> Module weights saved in models/OneShot/1/checkpoint-546/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:44,698 >> Configuration saved in models/OneShot/1/checkpoint-546/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:44,706 >> Module weights saved in models/OneShot/1/checkpoint-546/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:44,707 >> Configuration saved in models/OneShot/1/checkpoint-546/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:44,759 >> Module weights saved in models/OneShot/1/checkpoint-546/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:44,760 >> Configuration saved in models/OneShot/1/checkpoint-546/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:44,829 >> Module weights saved in models/OneShot/1/checkpoint-546/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:44,830 >> Configuration saved in models/OneShot/1/checkpoint-546/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:46,341 >> Module weights saved in models/OneShot/1/checkpoint-546/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:46,380 >> Configuration saved in models/OneShot/1/checkpoint-546/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:46,393 >> Module weights saved in models/OneShot/1/checkpoint-546/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:46:46,394 >> Configuration saved in models/OneShot/1/checkpoint-546/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:46:47,831 >> Module weights saved in models/OneShot/1/checkpoint-546/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:46:47,831 >> tokenizer config file saved in models/OneShot/1/checkpoint-546/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:46:47,831 >> Special tokens file saved in models/OneShot/1/checkpoint-546/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:46:48,347 >> Deleting older checkpoint [models/OneShot/1/checkpoint-507] due to args.save_total_limit\n",
            " 60% 584/975 [07:52<03:59,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:47:11,849 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:47:11,851 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:47:11,851 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:47:11,851 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.05it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.75it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.05it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.55it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.90it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.95it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.93it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 11.95it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.92it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.75it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.00it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.09it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.16it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.17it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.11it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.01it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.89it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.96it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.99it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.88it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.745563268661499, 'eval_accuracy': 0.5708154439926147, 'eval_f1': 0.5564017134697763, 'eval_runtime': 4.9002, 'eval_samples_per_second': 95.098, 'eval_steps_per_second': 12.04, 'epoch': 15.0}\n",
            " 60% 585/975 [07:57<03:59,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.32it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:47:16,753 >> Saving model checkpoint to models/OneShot/1/checkpoint-585\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:16,754 >> Configuration saved in models/OneShot/1/checkpoint-585/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:16,846 >> Module weights saved in models/OneShot/1/checkpoint-585/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:16,847 >> Configuration saved in models/OneShot/1/checkpoint-585/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:16,855 >> Module weights saved in models/OneShot/1/checkpoint-585/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:16,856 >> Configuration saved in models/OneShot/1/checkpoint-585/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:16,915 >> Module weights saved in models/OneShot/1/checkpoint-585/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:16,916 >> Configuration saved in models/OneShot/1/checkpoint-585/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:16,985 >> Module weights saved in models/OneShot/1/checkpoint-585/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:16,986 >> Configuration saved in models/OneShot/1/checkpoint-585/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:18,482 >> Module weights saved in models/OneShot/1/checkpoint-585/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:18,610 >> Configuration saved in models/OneShot/1/checkpoint-585/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:18,634 >> Module weights saved in models/OneShot/1/checkpoint-585/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:18,635 >> Configuration saved in models/OneShot/1/checkpoint-585/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:20,002 >> Module weights saved in models/OneShot/1/checkpoint-585/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:47:20,002 >> tokenizer config file saved in models/OneShot/1/checkpoint-585/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:47:20,002 >> Special tokens file saved in models/OneShot/1/checkpoint-585/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:47:20,496 >> Deleting older checkpoint [models/OneShot/1/checkpoint-546] due to args.save_total_limit\n",
            " 64% 623/975 [08:24<03:34,  1.64it/s][INFO|trainer.py:623] 2022-08-25 10:47:43,855 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:47:43,857 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:47:43,857 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:47:43,857 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.96it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.98it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.16it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.64it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.17it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.19it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.18it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.21it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.20it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.06it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.97it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.15it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.16it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.19it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.03it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.13it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.16it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.23it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.25it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.12it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.02it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.4986870288848877, 'eval_accuracy': 0.6072961091995239, 'eval_f1': 0.6004497856490265, 'eval_runtime': 4.8511, 'eval_samples_per_second': 96.061, 'eval_steps_per_second': 12.162, 'epoch': 16.0}\n",
            " 64% 624/975 [08:29<03:34,  1.64it/s]\n",
            "100% 59/59 [00:04<00:00, 13.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:47:48,710 >> Saving model checkpoint to models/OneShot/1/checkpoint-624\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:48,711 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:48,813 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:48,814 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:48,822 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:48,823 >> Configuration saved in models/OneShot/1/checkpoint-624/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:48,878 >> Module weights saved in models/OneShot/1/checkpoint-624/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:48,879 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:48,952 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:48,952 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:50,480 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:50,481 >> Configuration saved in models/OneShot/1/checkpoint-624/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:50,491 >> Module weights saved in models/OneShot/1/checkpoint-624/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:47:50,491 >> Configuration saved in models/OneShot/1/checkpoint-624/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:47:52,037 >> Module weights saved in models/OneShot/1/checkpoint-624/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:47:52,038 >> tokenizer config file saved in models/OneShot/1/checkpoint-624/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:47:52,038 >> Special tokens file saved in models/OneShot/1/checkpoint-624/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:47:52,530 >> Deleting older checkpoint [models/OneShot/1/checkpoint-585] due to args.save_total_limit\n",
            " 68% 662/975 [08:56<03:12,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:48:16,012 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:48:16,014 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:48:16,014 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:48:16,014 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.54it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.00it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:04, 12.98it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.37it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.84it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.93it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.92it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.74it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.01it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.08it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.18it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.18it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.07it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.00it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.96it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.89it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.99it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.004510402679443, 'eval_accuracy': 0.5364806652069092, 'eval_f1': 0.5131558582262807, 'eval_runtime': 4.9026, 'eval_samples_per_second': 95.051, 'eval_steps_per_second': 12.034, 'epoch': 17.0}\n",
            " 68% 663/975 [09:01<03:11,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:48:20,918 >> Saving model checkpoint to models/OneShot/1/checkpoint-663\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:20,919 >> Configuration saved in models/OneShot/1/checkpoint-663/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:21,019 >> Module weights saved in models/OneShot/1/checkpoint-663/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:21,019 >> Configuration saved in models/OneShot/1/checkpoint-663/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:21,027 >> Module weights saved in models/OneShot/1/checkpoint-663/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:21,027 >> Configuration saved in models/OneShot/1/checkpoint-663/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:21,080 >> Module weights saved in models/OneShot/1/checkpoint-663/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:21,081 >> Configuration saved in models/OneShot/1/checkpoint-663/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:21,160 >> Module weights saved in models/OneShot/1/checkpoint-663/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:21,161 >> Configuration saved in models/OneShot/1/checkpoint-663/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:22,662 >> Module weights saved in models/OneShot/1/checkpoint-663/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:22,686 >> Configuration saved in models/OneShot/1/checkpoint-663/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:22,721 >> Module weights saved in models/OneShot/1/checkpoint-663/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:22,743 >> Configuration saved in models/OneShot/1/checkpoint-663/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:23,966 >> Module weights saved in models/OneShot/1/checkpoint-663/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:48:23,966 >> tokenizer config file saved in models/OneShot/1/checkpoint-663/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:48:23,966 >> Special tokens file saved in models/OneShot/1/checkpoint-663/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:48:24,409 >> Deleting older checkpoint [models/OneShot/1/checkpoint-624] due to args.save_total_limit\n",
            " 72% 701/975 [09:28<02:47,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:48:47,856 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:48:47,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:48:47,858 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:48:47,858 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.08it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.30it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.36it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.63it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.07it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.12it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.14it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.15it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.21it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.13it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.86it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.96it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.07it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.10it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.99it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.82it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.95it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.04it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.14it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.18it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 12.23it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.18it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.03it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.03it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.9974753856658936, 'eval_accuracy': 0.545064389705658, 'eval_f1': 0.5240132593277829, 'eval_runtime': 4.8611, 'eval_samples_per_second': 95.862, 'eval_steps_per_second': 12.137, 'epoch': 18.0}\n",
            " 72% 702/975 [09:33<02:47,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:48:52,721 >> Saving model checkpoint to models/OneShot/1/checkpoint-702\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:52,722 >> Configuration saved in models/OneShot/1/checkpoint-702/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:52,813 >> Module weights saved in models/OneShot/1/checkpoint-702/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:52,814 >> Configuration saved in models/OneShot/1/checkpoint-702/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:52,822 >> Module weights saved in models/OneShot/1/checkpoint-702/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:52,823 >> Configuration saved in models/OneShot/1/checkpoint-702/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:52,881 >> Module weights saved in models/OneShot/1/checkpoint-702/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:52,882 >> Configuration saved in models/OneShot/1/checkpoint-702/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:52,950 >> Module weights saved in models/OneShot/1/checkpoint-702/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:52,951 >> Configuration saved in models/OneShot/1/checkpoint-702/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:54,398 >> Module weights saved in models/OneShot/1/checkpoint-702/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:54,514 >> Configuration saved in models/OneShot/1/checkpoint-702/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:54,540 >> Module weights saved in models/OneShot/1/checkpoint-702/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:48:54,541 >> Configuration saved in models/OneShot/1/checkpoint-702/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:48:55,961 >> Module weights saved in models/OneShot/1/checkpoint-702/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:48:55,962 >> tokenizer config file saved in models/OneShot/1/checkpoint-702/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:48:55,962 >> Special tokens file saved in models/OneShot/1/checkpoint-702/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:48:56,440 >> Deleting older checkpoint [models/OneShot/1/checkpoint-663] due to args.save_total_limit\n",
            " 76% 740/975 [10:00<02:24,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:49:19,921 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:49:19,922 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:49:19,923 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:49:19,923 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.39it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 13.94it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.12it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.69it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.98it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.97it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.00it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.10it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.15it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:02, 12.03it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.83it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:01, 12.00it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 12.04it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.94it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.92it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.83it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.88it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.86it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.95it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.960491418838501, 'eval_accuracy': 0.5643776655197144, 'eval_f1': 0.5478863478863478, 'eval_runtime': 4.9128, 'eval_samples_per_second': 94.855, 'eval_steps_per_second': 12.009, 'epoch': 19.0}\n",
            " 76% 741/975 [10:05<02:24,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.14it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:49:24,837 >> Saving model checkpoint to models/OneShot/1/checkpoint-741\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:24,838 >> Configuration saved in models/OneShot/1/checkpoint-741/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:24,935 >> Module weights saved in models/OneShot/1/checkpoint-741/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:24,935 >> Configuration saved in models/OneShot/1/checkpoint-741/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:24,943 >> Module weights saved in models/OneShot/1/checkpoint-741/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:24,943 >> Configuration saved in models/OneShot/1/checkpoint-741/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:25,004 >> Module weights saved in models/OneShot/1/checkpoint-741/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:25,004 >> Configuration saved in models/OneShot/1/checkpoint-741/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:25,083 >> Module weights saved in models/OneShot/1/checkpoint-741/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:25,084 >> Configuration saved in models/OneShot/1/checkpoint-741/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:26,576 >> Module weights saved in models/OneShot/1/checkpoint-741/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:26,596 >> Configuration saved in models/OneShot/1/checkpoint-741/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:26,629 >> Module weights saved in models/OneShot/1/checkpoint-741/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:26,639 >> Configuration saved in models/OneShot/1/checkpoint-741/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:27,939 >> Module weights saved in models/OneShot/1/checkpoint-741/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:49:27,939 >> tokenizer config file saved in models/OneShot/1/checkpoint-741/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:49:27,939 >> Special tokens file saved in models/OneShot/1/checkpoint-741/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:49:28,453 >> Deleting older checkpoint [models/OneShot/1/checkpoint-702] due to args.save_total_limit\n",
            " 80% 779/975 [10:32<02:01,  1.61it/s][INFO|trainer.py:623] 2022-08-25 10:49:51,935 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:49:51,937 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:49:51,937 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:49:51,937 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.23it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.36it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.32it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.57it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.05it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.11it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.15it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.16it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.10it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.85it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.84it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.73it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.91it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.06it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.12it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.06it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.87it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 12.00it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.831899642944336, 'eval_accuracy': 0.5751072764396667, 'eval_f1': 0.5615305656934306, 'eval_runtime': 4.8954, 'eval_samples_per_second': 95.191, 'eval_steps_per_second': 12.052, 'epoch': 20.0}\n",
            " 80% 780/975 [10:37<02:00,  1.61it/s]\n",
            "100% 59/59 [00:04<00:00, 13.24it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:49:56,834 >> Saving model checkpoint to models/OneShot/1/checkpoint-780\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:56,835 >> Configuration saved in models/OneShot/1/checkpoint-780/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:56,926 >> Module weights saved in models/OneShot/1/checkpoint-780/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:56,926 >> Configuration saved in models/OneShot/1/checkpoint-780/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:56,934 >> Module weights saved in models/OneShot/1/checkpoint-780/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:56,935 >> Configuration saved in models/OneShot/1/checkpoint-780/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:56,989 >> Module weights saved in models/OneShot/1/checkpoint-780/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:56,990 >> Configuration saved in models/OneShot/1/checkpoint-780/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:57,060 >> Module weights saved in models/OneShot/1/checkpoint-780/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:57,061 >> Configuration saved in models/OneShot/1/checkpoint-780/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:58,567 >> Module weights saved in models/OneShot/1/checkpoint-780/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:58,572 >> Configuration saved in models/OneShot/1/checkpoint-780/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:49:58,718 >> Module weights saved in models/OneShot/1/checkpoint-780/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:49:58,719 >> Configuration saved in models/OneShot/1/checkpoint-780/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:00,099 >> Module weights saved in models/OneShot/1/checkpoint-780/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:50:00,099 >> tokenizer config file saved in models/OneShot/1/checkpoint-780/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:50:00,100 >> Special tokens file saved in models/OneShot/1/checkpoint-780/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:50:00,594 >> Deleting older checkpoint [models/OneShot/1/checkpoint-741] due to args.save_total_limit\n",
            " 84% 818/975 [11:04<01:36,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:50:23,972 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:50:23,973 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:50:23,973 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:50:23,973 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.71it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.31it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.10it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.51it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.03it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.78it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.97it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.91it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.88it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.74it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.98it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.08it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.14it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.15it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.99it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.97it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.03it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.99it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 3.8349545001983643, 'eval_accuracy': 0.5708154439926147, 'eval_f1': 0.5571015815085159, 'eval_runtime': 4.8955, 'eval_samples_per_second': 95.19, 'eval_steps_per_second': 12.052, 'epoch': 21.0}\n",
            " 84% 819/975 [11:09<01:35,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:50:28,871 >> Saving model checkpoint to models/OneShot/1/checkpoint-819\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:28,871 >> Configuration saved in models/OneShot/1/checkpoint-819/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:28,966 >> Module weights saved in models/OneShot/1/checkpoint-819/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:28,966 >> Configuration saved in models/OneShot/1/checkpoint-819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:28,974 >> Module weights saved in models/OneShot/1/checkpoint-819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:28,975 >> Configuration saved in models/OneShot/1/checkpoint-819/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:29,029 >> Module weights saved in models/OneShot/1/checkpoint-819/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:29,030 >> Configuration saved in models/OneShot/1/checkpoint-819/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:29,121 >> Module weights saved in models/OneShot/1/checkpoint-819/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:29,121 >> Configuration saved in models/OneShot/1/checkpoint-819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:30,569 >> Module weights saved in models/OneShot/1/checkpoint-819/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:30,687 >> Configuration saved in models/OneShot/1/checkpoint-819/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:30,710 >> Module weights saved in models/OneShot/1/checkpoint-819/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:50:30,712 >> Configuration saved in models/OneShot/1/checkpoint-819/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:50:32,067 >> Module weights saved in models/OneShot/1/checkpoint-819/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:50:32,067 >> tokenizer config file saved in models/OneShot/1/checkpoint-819/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:50:32,068 >> Special tokens file saved in models/OneShot/1/checkpoint-819/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:50:32,571 >> Deleting older checkpoint [models/OneShot/1/checkpoint-780] due to args.save_total_limit\n",
            " 88% 857/975 [11:36<01:12,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:50:56,032 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:50:56,033 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:50:56,033 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:50:56,033 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 17.18it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.17it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.13it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.54it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.02it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.01it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.06it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.10it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.90it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.94it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.01it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.05it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.01it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.86it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.81it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.85it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.74it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.80it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.00it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.94it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.015086650848389, 'eval_accuracy': 0.5600858330726624, 'eval_f1': 0.5418379239066313, 'eval_runtime': 4.9067, 'eval_samples_per_second': 94.972, 'eval_steps_per_second': 12.024, 'epoch': 22.0}\n",
            " 88% 858/975 [11:41<01:12,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:51:00,942 >> Saving model checkpoint to models/OneShot/1/checkpoint-858\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:00,943 >> Configuration saved in models/OneShot/1/checkpoint-858/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:01,037 >> Module weights saved in models/OneShot/1/checkpoint-858/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:01,038 >> Configuration saved in models/OneShot/1/checkpoint-858/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:01,048 >> Module weights saved in models/OneShot/1/checkpoint-858/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:01,049 >> Configuration saved in models/OneShot/1/checkpoint-858/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:01,108 >> Module weights saved in models/OneShot/1/checkpoint-858/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:01,109 >> Configuration saved in models/OneShot/1/checkpoint-858/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:01,181 >> Module weights saved in models/OneShot/1/checkpoint-858/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:01,182 >> Configuration saved in models/OneShot/1/checkpoint-858/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:02,645 >> Module weights saved in models/OneShot/1/checkpoint-858/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:02,763 >> Configuration saved in models/OneShot/1/checkpoint-858/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:02,779 >> Module weights saved in models/OneShot/1/checkpoint-858/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:02,780 >> Configuration saved in models/OneShot/1/checkpoint-858/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:04,198 >> Module weights saved in models/OneShot/1/checkpoint-858/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:51:04,199 >> tokenizer config file saved in models/OneShot/1/checkpoint-858/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:51:04,199 >> Special tokens file saved in models/OneShot/1/checkpoint-858/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:51:04,681 >> Deleting older checkpoint [models/OneShot/1/checkpoint-819] due to args.save_total_limit\n",
            " 92% 896/975 [12:08<00:48,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:51:28,167 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:51:28,169 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:51:28,169 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:51:28,169 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.85it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.20it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.31it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.64it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 12.00it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.03it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.08it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.87it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.03it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.04it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.09it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 12.00it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.85it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.77it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.84it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.93it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.05it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.04it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.93it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.92it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.99it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.90it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.213359832763672, 'eval_accuracy': 0.5515021681785583, 'eval_f1': 0.5303031033734417, 'eval_runtime': 4.8878, 'eval_samples_per_second': 95.34, 'eval_steps_per_second': 12.071, 'epoch': 23.0}\n",
            " 92% 897/975 [12:13<00:47,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:51:33,058 >> Saving model checkpoint to models/OneShot/1/checkpoint-897\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:33,059 >> Configuration saved in models/OneShot/1/checkpoint-897/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:33,154 >> Module weights saved in models/OneShot/1/checkpoint-897/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:33,154 >> Configuration saved in models/OneShot/1/checkpoint-897/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:33,162 >> Module weights saved in models/OneShot/1/checkpoint-897/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:33,163 >> Configuration saved in models/OneShot/1/checkpoint-897/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:33,217 >> Module weights saved in models/OneShot/1/checkpoint-897/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:33,218 >> Configuration saved in models/OneShot/1/checkpoint-897/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:33,289 >> Module weights saved in models/OneShot/1/checkpoint-897/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:33,289 >> Configuration saved in models/OneShot/1/checkpoint-897/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:34,803 >> Module weights saved in models/OneShot/1/checkpoint-897/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:34,804 >> Configuration saved in models/OneShot/1/checkpoint-897/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:34,927 >> Module weights saved in models/OneShot/1/checkpoint-897/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:51:34,928 >> Configuration saved in models/OneShot/1/checkpoint-897/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:51:36,329 >> Module weights saved in models/OneShot/1/checkpoint-897/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:51:36,330 >> tokenizer config file saved in models/OneShot/1/checkpoint-897/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:51:36,330 >> Special tokens file saved in models/OneShot/1/checkpoint-897/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:51:36,829 >> Deleting older checkpoint [models/OneShot/1/checkpoint-858] due to args.save_total_limit\n",
            " 96% 935/975 [12:40<00:24,  1.63it/s][INFO|trainer.py:623] 2022-08-25 10:52:00,235 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:52:00,237 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:52:00,237 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:52:00,237 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.70it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.09it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.29it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:03, 12.89it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:03, 12.42it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 12.29it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.30it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.25it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.18it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.16it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.94it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.93it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 12.01it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 12.11it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 12.01it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.98it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.92it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.79it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 12.02it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 12.05it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 12.09it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:00, 12.15it/s]\u001b[A\n",
            " 83% 49/59 [00:03<00:00, 12.18it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 12.18it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 12.14it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 12.01it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.98it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.184279918670654, 'eval_accuracy': 0.5472102761268616, 'eval_f1': 0.5258083962286899, 'eval_runtime': 4.8501, 'eval_samples_per_second': 96.08, 'eval_steps_per_second': 12.165, 'epoch': 24.0}\n",
            " 96% 936/975 [12:45<00:23,  1.63it/s]\n",
            "100% 59/59 [00:04<00:00, 13.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:52:05,089 >> Saving model checkpoint to models/OneShot/1/checkpoint-936\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:05,090 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:05,189 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:05,190 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:05,198 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:05,198 >> Configuration saved in models/OneShot/1/checkpoint-936/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:05,251 >> Module weights saved in models/OneShot/1/checkpoint-936/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:05,252 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:05,320 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:05,321 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:06,826 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:06,946 >> Configuration saved in models/OneShot/1/checkpoint-936/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:06,969 >> Module weights saved in models/OneShot/1/checkpoint-936/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:06,970 >> Configuration saved in models/OneShot/1/checkpoint-936/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:08,338 >> Module weights saved in models/OneShot/1/checkpoint-936/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:52:08,338 >> tokenizer config file saved in models/OneShot/1/checkpoint-936/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:52:08,338 >> Special tokens file saved in models/OneShot/1/checkpoint-936/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:52:08,829 >> Deleting older checkpoint [models/OneShot/1/checkpoint-897] due to args.save_total_limit\n",
            "100% 974/975 [13:13<00:00,  1.62it/s][INFO|trainer.py:623] 2022-08-25 10:52:32,308 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:52:32,310 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:52:32,310 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:52:32,310 >>   Batch size = 8\n",
            "\n",
            "  0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/59 [00:00<00:03, 16.99it/s]\u001b[A\n",
            "  8% 5/59 [00:00<00:03, 14.16it/s]\u001b[A\n",
            " 12% 7/59 [00:00<00:03, 13.15it/s]\u001b[A\n",
            " 15% 9/59 [00:00<00:04, 12.48it/s]\u001b[A\n",
            " 19% 11/59 [00:00<00:04, 11.97it/s]\u001b[A\n",
            " 22% 13/59 [00:01<00:03, 11.99it/s]\u001b[A\n",
            " 25% 15/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 29% 17/59 [00:01<00:03, 12.07it/s]\u001b[A\n",
            " 32% 19/59 [00:01<00:03, 12.09it/s]\u001b[A\n",
            " 36% 21/59 [00:01<00:03, 12.04it/s]\u001b[A\n",
            " 39% 23/59 [00:01<00:03, 11.79it/s]\u001b[A\n",
            " 42% 25/59 [00:02<00:02, 11.89it/s]\u001b[A\n",
            " 46% 27/59 [00:02<00:02, 11.88it/s]\u001b[A\n",
            " 49% 29/59 [00:02<00:02, 11.84it/s]\u001b[A\n",
            " 53% 31/59 [00:02<00:02, 11.90it/s]\u001b[A\n",
            " 56% 33/59 [00:02<00:02, 11.87it/s]\u001b[A\n",
            " 59% 35/59 [00:02<00:02, 11.74it/s]\u001b[A\n",
            " 63% 37/59 [00:03<00:01, 11.87it/s]\u001b[A\n",
            " 66% 39/59 [00:03<00:01, 11.90it/s]\u001b[A\n",
            " 69% 41/59 [00:03<00:01, 11.83it/s]\u001b[A\n",
            " 73% 43/59 [00:03<00:01, 11.86it/s]\u001b[A\n",
            " 76% 45/59 [00:03<00:01, 11.80it/s]\u001b[A\n",
            " 80% 47/59 [00:03<00:01, 11.73it/s]\u001b[A\n",
            " 83% 49/59 [00:04<00:00, 11.84it/s]\u001b[A\n",
            " 86% 51/59 [00:04<00:00, 11.82it/s]\u001b[A\n",
            " 90% 53/59 [00:04<00:00, 11.77it/s]\u001b[A\n",
            " 93% 55/59 [00:04<00:00, 11.79it/s]\u001b[A\n",
            " 97% 57/59 [00:04<00:00, 11.70it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.168864727020264, 'eval_accuracy': 0.5472102761268616, 'eval_f1': 0.5258083962286899, 'eval_runtime': 4.9345, 'eval_samples_per_second': 94.438, 'eval_steps_per_second': 11.957, 'epoch': 25.0}\n",
            "100% 975/975 [13:18<00:00,  1.62it/s]\n",
            "100% 59/59 [00:04<00:00, 13.05it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:141] 2022-08-25 10:52:37,246 >> Saving model checkpoint to models/OneShot/1/checkpoint-975\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:37,247 >> Configuration saved in models/OneShot/1/checkpoint-975/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:37,341 >> Module weights saved in models/OneShot/1/checkpoint-975/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:37,342 >> Configuration saved in models/OneShot/1/checkpoint-975/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:37,350 >> Module weights saved in models/OneShot/1/checkpoint-975/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:37,351 >> Configuration saved in models/OneShot/1/checkpoint-975/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:37,409 >> Module weights saved in models/OneShot/1/checkpoint-975/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:37,410 >> Configuration saved in models/OneShot/1/checkpoint-975/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:37,481 >> Module weights saved in models/OneShot/1/checkpoint-975/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:37,481 >> Configuration saved in models/OneShot/1/checkpoint-975/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:38,958 >> Module weights saved in models/OneShot/1/checkpoint-975/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:39,054 >> Configuration saved in models/OneShot/1/checkpoint-975/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:39,079 >> Module weights saved in models/OneShot/1/checkpoint-975/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:39,080 >> Configuration saved in models/OneShot/1/checkpoint-975/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:40,486 >> Module weights saved in models/OneShot/1/checkpoint-975/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:52:40,486 >> tokenizer config file saved in models/OneShot/1/checkpoint-975/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:52:40,486 >> Special tokens file saved in models/OneShot/1/checkpoint-975/special_tokens_map.json\n",
            "[INFO|trainer.py:2418] 2022-08-25 10:52:40,968 >> Deleting older checkpoint [models/OneShot/1/checkpoint-936] due to args.save_total_limit\n",
            "[INFO|trainer.py:1662] 2022-08-25 10:52:41,028 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1727] 2022-08-25 10:52:41,029 >> Loading best model from models/OneShot/1/checkpoint-156 (score: 0.6318111880680322).\n",
            "[WARNING|trainer.py:1754] 2022-08-25 10:52:41,029 >> Could not locate the best model at models/OneShot/1/checkpoint-156/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n",
            "{'train_runtime': 801.8349, 'train_samples_per_second': 37.944, 'train_steps_per_second': 1.216, 'train_loss': 0.05798073493517362, 'epoch': 25.0}\n",
            "100% 975/975 [13:21<00:00,  1.62it/s][INFO|trainer.py:238] 2022-08-25 10:52:41,031 >> Loading best adapter(s) from models/OneShot/1/checkpoint-156 (score: 0.6318111880680322).\n",
            "[INFO|loading.py:77] 2022-08-25 10:52:41,031 >> Loading module configuration from models/OneShot/1/checkpoint-156/glue/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 10:52:41,033 >> Overwriting existing adapter 'glue'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:52:41,761 >> Loading module weights from models/OneShot/1/checkpoint-156/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:52:41,784 >> Loading module configuration from models/OneShot/1/checkpoint-156/glue/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 10:52:41,784 >> Overwriting existing head 'glue'\n",
            "[INFO|base.py:688] 2022-08-25 10:52:41,793 >> Adding head 'glue' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'0': 0, '1': 1}, 'use_pooler': False, 'bias': True}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:52:41,808 >> Loading module weights from models/OneShot/1/checkpoint-156/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:52:41,811 >> Loading module configuration from models/OneShot/1/checkpoint-156/en/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 10:52:41,812 >> Overwriting existing adapter 'en'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:52:41,977 >> Loading module weights from models/OneShot/1/checkpoint-156/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:672] 2022-08-25 10:52:41,995 >> No matching prediction head found in 'models/OneShot/1/checkpoint-156/en'\n",
            "[INFO|loading.py:77] 2022-08-25 10:52:41,995 >> Loading module configuration from models/OneShot/1/checkpoint-156/pt/adapter_config.json\n",
            "[WARNING|loading.py:452] 2022-08-25 10:52:41,996 >> Overwriting existing adapter 'pt'.\n",
            "[INFO|loading.py:146] 2022-08-25 10:52:42,168 >> Loading module weights from models/OneShot/1/checkpoint-156/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:77] 2022-08-25 10:52:42,186 >> Loading module configuration from models/OneShot/1/checkpoint-156/pt/head_config.json\n",
            "[WARNING|loading.py:730] 2022-08-25 10:52:42,187 >> Overwriting existing head 'pt'\n",
            "[INFO|base.py:688] 2022-08-25 10:52:43,369 >> Adding head 'pt' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
            "[INFO|loading.py:146] 2022-08-25 10:52:45,033 >> Loading module weights from models/OneShot/1/checkpoint-156/pt/pytorch_model_head.bin\n",
            "100% 975/975 [13:25<00:00,  1.21it/s]\n",
            "[INFO|trainer.py:141] 2022-08-25 10:52:45,121 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:45,122 >> Configuration saved in models/OneShot/1/glue/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:45,245 >> Module weights saved in models/OneShot/1/glue/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:45,246 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:45,254 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:45,255 >> Configuration saved in models/OneShot/1/en/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:45,309 >> Module weights saved in models/OneShot/1/en/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:45,310 >> Configuration saved in models/OneShot/1/pt/adapter_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:45,387 >> Module weights saved in models/OneShot/1/pt/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:45,388 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:46,854 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:46,973 >> Configuration saved in models/OneShot/1/glue/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:46,994 >> Module weights saved in models/OneShot/1/glue/pytorch_model_head.bin\n",
            "[INFO|loading.py:60] 2022-08-25 10:52:46,995 >> Configuration saved in models/OneShot/1/pt/head_config.json\n",
            "[INFO|loading.py:73] 2022-08-25 10:52:48,336 >> Module weights saved in models/OneShot/1/pt/pytorch_model_head.bin\n",
            "[INFO|tokenization_utils_base.py:2108] 2022-08-25 10:52:48,337 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2114] 2022-08-25 10:52:48,337 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       25.0\n",
            "  train_loss               =      0.058\n",
            "  train_runtime            = 0:13:21.83\n",
            "  train_samples            =       1217\n",
            "  train_samples_per_second =     37.944\n",
            "  train_steps_per_second   =      1.216\n",
            "08/25/2022 10:52:48 - INFO - __main__ - *** Evaluate ***\n",
            "\n",
            "\n",
            "Changing the language adapter to EN during evaluation..\n",
            "\n",
            "\n",
            "[INFO|trainer.py:623] 2022-08-25 10:52:48,581 >> The following columns in the evaluation set don't have a corresponding argument in `BertAdapterModel.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertAdapterModel.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2590] 2022-08-25 10:52:48,583 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2592] 2022-08-25 10:52:48,583 >>   Num examples = 466\n",
            "[INFO|trainer.py:2595] 2022-08-25 10:52:48,583 >>   Batch size = 8\n",
            "100% 59/59 [00:04<00:00, 12.64it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =     0.6631\n",
            "  eval_f1                 =     0.6629\n",
            "  eval_loss               =     0.6963\n",
            "  eval_runtime            = 0:00:04.77\n",
            "  eval_samples            =        466\n",
            "  eval_samples_per_second =     97.512\n",
            "  eval_steps_per_second   =     12.346\n",
            "[INFO|modelcard.py:460] 2022-08-25 10:52:53,402 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6630901098251343}, {'name': 'F1', 'type': 'f1', 'value': 0.6629022968645609}]}\n"
          ]
        }
      ]
    }
  ]
}